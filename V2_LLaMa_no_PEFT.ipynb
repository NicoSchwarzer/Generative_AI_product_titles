{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using the plain LLaMa model (no PEFT)to generate titles.\n",
        "\n",
        "* LLaMa model: 7B parameter version of: https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/.\n"
      ],
      "metadata": {
        "id": "L9ikghCnCZ_g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R5ECGqrU2mBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface\n",
        "!pip install huggingface_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8imoAwTwLle",
        "outputId": "d3efad8f-9fe3-4e64-f493-9f1aae24826d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: huggingface\n",
            "Successfully installed huggingface-0.0.1\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9yoA8uk1T1f",
        "outputId": "c5ae2390-fcec-4c12-ed07-827147de1721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "83fc548240ca44bf8b5b1ff9de50127a",
            "5fb98220578645d4bca46aba29a660d4",
            "3525d588b3f649f5b36e57b155ba0d40",
            "cd2f62aa3bf34e23b2c6703e5efd64af",
            "4e49e007dc804e2d929ecf044cc3ccf5",
            "f2dce3f5c3a1476c8807f37cdce2c706",
            "5cedba9733294d73ade6fca9556515fb",
            "1cebef149e5044ecbf9c198c964b394a",
            "854d94a285a14f8fba532f3ac8b93040",
            "b8c2e8173ceb423aab0ab3b7f65ef1c1",
            "ccc3cdae02f5449381d8074bef11b699",
            "ef68b5d2eb774415b34c019ca7da2654",
            "00f4cee8a9d543ae934b515390915f99",
            "decfda2a5fd44fcabe6c751e3c12b4ab",
            "a25425c6de4b4bb1ad912817d9c2b5dd",
            "6ae4ea4bdd88420b87ce40b54ae27cb9",
            "bef5eef2d32a44a9b5ef1c926ca063ea",
            "f310d898ed724ceeb1501b23306c6789",
            "8f5b497066e84e9f984219676a526d04",
            "ea49883d4044433aa642cb7c8593c7d5",
            "6a78197da02142a8b107a05e003a5107",
            "fb3b1a33a08c40e6af3f9ad2ea278e57",
            "7f47d095d6614a1cbb4958f552e019d8",
            "fa5e03b1644f4afb980f0d085290678a",
            "803477098d9142ab969b387c3c75a55e",
            "9b91da96afe2449897bcd79f2c472d65",
            "f6634e30825149c684c71cd7978a3099",
            "a04b471def24414daeec913f9ff6a26b",
            "71b9dc16239d48b2bd869dfde812e026",
            "23eb8284eb6044e8a71b2cc0f5af823b",
            "731ca9981dbc4e8a9eab36388efebb92",
            "09375db72c1b483d82929981ffd2e2ab"
          ]
        },
        "id": "oVBGelWw4ci5",
        "outputId": "20dba16b-20de-4bcb-d77b-34d335a09b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83fc548240ca44bf8b5b1ff9de50127a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Lightning-AI/lit-llama/blob/main/howto/finetune_adapter.md"
      ],
      "metadata": {
        "id": "ZRO_EbRN0_VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lightning-AI/lit-llama ## this first ?!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267ac00a-1ed9-428b-914c-fdc7271b7bd7",
        "id": "1Y1lAOa10_VJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lit-llama'...\n",
            "remote: Enumerating objects: 1845, done.\u001b[K\n",
            "remote: Counting objects: 100% (230/230), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 1845 (delta 134), reused 167 (delta 95), pack-reused 1615\u001b[K\n",
            "Receiving objects: 100% (1845/1845), 1.65 MiB | 25.95 MiB/s, done.\n",
            "Resolving deltas: 100% (1133/1133), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/meta-llama/Llama-2-7b-hf checkpoints/open-llama/7B\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8un_onwriGO",
        "outputId": "bafd2f2c-cee6-4864-b050-254172c01107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'checkpoints/open-llama/7B'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 27 (delta 3), reused 0 (delta 0), pack-reused 16\u001b[K\n",
            "Unpacking objects: 100% (27/27), 495.01 KiB | 1.03 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r lit-llama/requirements.txt\n",
        "# might take some time dpeneding on internet connectivity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j5qDaAITASD",
        "outputId": "d9dc2c86-bd27-4cf9-aa3b-e5fe3acb7f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lightning@ git+https://github.com/Lightning-AI/lightning@master (from -r lit-llama/requirements.txt (line 2))\n",
            "  Cloning https://github.com/Lightning-AI/lightning (to revision master) to /tmp/pip-install-tpy7f_xe/lightning_7d74df5663ad4c899a59ad5744661560\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-tpy7f_xe/lightning_7d74df5663ad4c899a59ad5744661560\n",
            "  Resolved https://github.com/Lightning-AI/lightning to commit f04089d10d3d8f94251e420870d51e4da66b32b8\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Encountered 31 file(s) that should have been pointers, but weren't:\n",
            "        .notebooks/course_UvA-DL/01-introduction-to-pytorch.ipynb\n",
            "        .notebooks/course_UvA-DL/02-activation-functions.ipynb\n",
            "        .notebooks/course_UvA-DL/03-initialization-and-optimization.ipynb\n",
            "        .notebooks/course_UvA-DL/04-inception-resnet-densenet.ipynb\n",
            "        .notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb\n",
            "        .notebooks/course_UvA-DL/06-graph-neural-networks.ipynb\n",
            "        .notebooks/course_UvA-DL/07-deep-energy-based-generative-models.ipynb\n",
            "        .notebooks/course_UvA-DL/08-deep-autoencoders.ipynb\n",
            "        .notebooks/course_UvA-DL/09-normalizing-flows.ipynb\n",
            "        .notebooks/course_UvA-DL/10-autoregressive-image-modeling.ipynb\n",
            "        .notebooks/course_UvA-DL/11-vision-transformer.ipynb\n",
            "        .notebooks/course_UvA-DL/12-meta-learning.ipynb\n",
            "        .notebooks/course_UvA-DL/13-contrastive-learning.ipynb\n",
            "        .notebooks/flash_tutorials/electricity_forecasting.ipynb\n",
            "        .notebooks/flash_tutorials/image_classification.ipynb\n",
            "        .notebooks/flash_tutorials/tabular_classification.ipynb\n",
            "        .notebooks/flash_tutorials/text_classification.ipynb\n",
            "        .notebooks/lightning_examples/augmentation_kornia.ipynb\n",
            "        .notebooks/lightning_examples/barlow-twins.ipynb\n",
            "        .notebooks/lightning_examples/basic-gan.ipynb\n",
            "        .notebooks/lightning_examples/cifar10-baseline.ipynb\n",
            "        .notebooks/lightning_examples/datamodules.ipynb\n",
            "        .notebooks/lightning_examples/finetuning-scheduler.ipynb\n",
            "        .notebooks/lightning_examples/mnist-hello-world.ipynb\n",
            "        .notebooks/lightning_examples/mnist-tpu-training.ipynb\n",
            "        .notebooks/lightning_examples/reinforce-learning-DQN.ipynb\n",
            "        .notebooks/lightning_examples/text-transformers.ipynb\n",
            "        .notebooks/lightning_examples/warp-drive.ipynb\n",
            "        .notebooks/templates/img-classify.ipynb\n",
            "        .notebooks/templates/simple.ipynb\n",
            "        .notebooks/templates/titanic.ipynb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting sentencepiece (from -r lit-llama/requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 4)) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 5)) (1.22.4)\n",
            "Collecting jsonargparse[signatures] (from -r lit-llama/requirements.txt (line 6))\n",
            "  Downloading jsonargparse-4.22.0-py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from -r lit-llama/requirements.txt (line 7))\n",
            "  Downloading bitsandbytes-0.39.1-py3-none-any.whl (97.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from -r lit-llama/requirements.txt (line 8))\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from -r lit-llama/requirements.txt (line 9))\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (16.0.6)\n",
            "Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (6.0)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (8.1.3)\n",
            "Collecting croniter<1.4.0,>=1.3.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading croniter-1.3.15-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading deepdiff-6.3.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<2.0,>=0.92.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2024.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2023.6.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.37 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading lightning_cloud-0.5.37-py3-none-any.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.8.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<4.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.10.9)\n",
            "Collecting python-multipart<2.0,>=0.0.5 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (13.4.2)\n",
            "Collecting starlette (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading starlette-0.28.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting torchmetrics<2.0,>=0.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: urllib3<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.26.16)\n",
            "Collecting uvicorn<2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.6.0)\n",
            "Collecting websockets<12.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading pytorch_lightning-2.0.4-py3-none-any.whl (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.2/721.2 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6))\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6))\n",
            "  Downloading typeshed_client-2.3.0-py3-none-any.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.6/581.6 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r lit-llama/requirements.txt (line 8)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r lit-llama/requirements.txt (line 8)) (1.5.3)\n",
            "Collecting xxhash (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2022.7.1)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (2.1.3)\n",
            "Collecting pyjwt (from lightning-cloud>=0.5.37->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.37->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6)) (5.12.0)\n",
            "Collecting h11>=0.8 (from uvicorn<2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (0.2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (67.7.2)\n",
            "Building wheels for collected packages: lightning\n",
            "  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightning: filename=lightning-2.1.0.dev0-py3-none-any.whl size=1863037 sha256=6ca9caf9c7572f39d85e0c84d7ec1225a50341130047e0732a3333a6c6651107\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-55artwom/wheels/b9/92/07/634ec381ab7d682d3afdcf943d0a6604881441ff2d6c409103\n",
            "Successfully built lightning\n",
            "Installing collected packages: sentencepiece, python-editor, bitsandbytes, zstandard, xxhash, websockets, typeshed-client, readchar, python-multipart, pyjwt, ordered-set, multidict, lightning-utilities, jsonargparse, h11, frozenlist, docstring-parser, dill, blessed, async-timeout, yarl, uvicorn, starlette, multiprocess, inquirer, huggingface-hub, deepdiff, dateutils, croniter, arrow, aiosignal, starsessions, fastapi, aiohttp, lightning-cloud, datasets, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 arrow-1.2.3 async-timeout-4.0.2 bitsandbytes-0.39.1 blessed-1.20.0 croniter-1.3.15 datasets-2.13.1 dateutils-0.6.12 deepdiff-6.3.0 dill-0.3.6 docstring-parser-0.15 fastapi-0.98.0 frozenlist-1.3.3 h11-0.14.0 huggingface-hub-0.15.1 inquirer-3.1.3 jsonargparse-4.22.0 lightning-2.1.0.dev0 lightning-cloud-0.5.37 lightning-utilities-0.8.0 multidict-6.0.4 multiprocess-0.70.14 ordered-set-4.1.0 pyjwt-2.7.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.4 readchar-4.0.5 sentencepiece-0.1.99 starlette-0.27.0 starsessions-1.3.0 torchmetrics-0.11.4 typeshed-client-2.3.0 uvicorn-0.22.0 websockets-11.0.3 xxhash-3.2.0 yarl-1.9.2 zstandard-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python lit-llama/scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/open-llama/7B --model_size 7B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z_FR3LicIwq",
        "outputId": "c84f04c0-86ba-4e51-8a2e-b0fb6b5d8267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing lit-llama\n",
            "Saving to disk at checkpoints/lit-llama/7B\n",
            "Processing checkpoints/open-llama/7B/pytorch_model-00001-of-00002.bin\n",
            "Processing checkpoints/open-llama/7B/pytorch_model-00002-of-00002.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connecting to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o8Fh6ZspmqS",
        "outputId": "d2d3959c-ce4f-4d30-b4d8-ce553f45e1f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading in data from drive and setting up own dataloader (.pt) files like the function in the python file prepare_alpacy do."
      ],
      "metadata": {
        "id": "UQ732ZBufCMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "airbnb_london_filtered_images = pd.read_csv(\"/content/gdrive/My Drive/Thesis/London_Data/discriminator_tabular_data_images.csv\")"
      ],
      "metadata": {
        "id": "zikMD2xup37L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "frac_train_size = 0.7\n",
        "\n",
        "train_eval_airbnb_london_filtered_images = airbnb_london_filtered_images[airbnb_london_filtered_images.in_top_third == 1]\n",
        "\n",
        "train_airbnb_london_filtered_advanced = train_eval_airbnb_london_filtered_images.sample(n = int(np.ceil(frac_train_size*train_eval_airbnb_london_filtered_images.shape[0])), random_state = 100)\n",
        "eval_airbnb_london_filtered_advanced = train_eval_airbnb_london_filtered_images.drop(train_airbnb_london_filtered_advanced.index, axis = 0)\n",
        "\n",
        "train_airbnb_london_filtered_advanced.index = list(range(train_airbnb_london_filtered_advanced.shape[0]))\n",
        "eval_airbnb_london_filtered_advanced.index = list(range(eval_airbnb_london_filtered_advanced.shape[0]))\n"
      ],
      "metadata": {
        "id": "wPoGSTBRqMjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "import torch\n",
        "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
        "import sys\n",
        "import torch\n",
        "import requests\n",
        "import json\n",
        "from torch.utils.data import random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Tokenizer for LLaMA.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: Path) -> None:\n",
        "        self.processor = SentencePieceProcessor(model_file=str(model_path))\n",
        "        self.bos_id = self.processor.bos_id()\n",
        "        self.eos_id = self.processor.eos_id()\n",
        "        self.pad_id = self.processor.pad_id()\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return self.processor.vocab_size()\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        string: str,\n",
        "        bos: bool = True,\n",
        "        eos: bool = False,\n",
        "        max_length: int = -1,\n",
        "        pad: bool = False,\n",
        "        device: Optional[torch.device] = None\n",
        "    ) -> torch.Tensor:\n",
        "        tokens = self.processor.encode(string)\n",
        "        if bos:\n",
        "            tokens = [self.bos_id] + tokens\n",
        "        if eos:\n",
        "            tokens = tokens + [self.eos_id]\n",
        "        if max_length > 0:\n",
        "            tokens = tokens[:max_length]\n",
        "        if pad and len(tokens) < max_length:\n",
        "            tokens += [self.pad_id] * (max_length - len(tokens))\n",
        "\n",
        "        return torch.tensor(tokens, dtype=torch.int, device=device)\n",
        "\n",
        "    def decode(self, tokens: torch.Tensor) -> str:\n",
        "        return self.processor.decode(tokens.tolist())\n",
        "\n",
        "    @staticmethod\n",
        "    def train(input: str, destination: str, vocab_size=32000) -> None:\n",
        "        model_prefix = os.path.join(destination, \"tokenizer\")\n",
        "        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)\n",
        "\n",
        "\n",
        "## general function\n",
        "def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n",
        "    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n",
        "\n",
        "## general function\n",
        "\n",
        "def generate_prompt(example):\n",
        "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
        "    'response' field.\"\"\"\n",
        "\n",
        "    if example[\"input\"]:\n",
        "        return (\n",
        "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\"\n",
        "        )\n",
        "    return (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_sample(example: dict, tokenizer: Tokenizer, max_length: int, mask_inputs: bool = True):\n",
        "    \"\"\"Processes a single sample.\n",
        "\n",
        "    Each sample in the dataset consists of:\n",
        "    - instruction: A string describing the task\n",
        "    - input: A string holding a special input value for the instruction.\n",
        "        This only applies to some samples, and in others this is empty.\n",
        "    - output: The response string\n",
        "\n",
        "    This function processes this data to produce a prompt text and a label for\n",
        "    supervised training. The input text is formed as a single message including all\n",
        "    the instruction, the input (optional) and the response.\n",
        "    The label/target is the same message but can optionally have the instruction + input text\n",
        "    masked out (mask_inputs=True).\n",
        "\n",
        "    Finally, both the prompt and the label get tokenized. If desired, all tokens\n",
        "    in the label that correspond to the original input prompt get masked out (default).\n",
        "    \"\"\"\n",
        "\n",
        "    full_prompt = generate_prompt(example)\n",
        "    full_prompt_and_response = full_prompt + example[\"output\"]\n",
        "    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=max_length, eos=False)\n",
        "    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=max_length)\n",
        "\n",
        "    # The labels are the full prompt with response, but with the prompt masked out\n",
        "    labels = encoded_full_prompt_and_response.clone()\n",
        "    if mask_inputs:\n",
        "        labels[:len(encoded_full_prompt)]  = -1 ## corresponds to \"ignore index\" inprepare_alpaca.py file\n",
        "\n",
        "\n",
        "    return {**example, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### prepare function is not required!"
      ],
      "metadata": {
        "id": "lj9xr8XqoPMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(\"/content/checkpoints/lit-llama/tokenizer.model\")"
      ],
      "metadata": {
        "id": "5hXIcoNftMnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reocding the prepare function:"
      ],
      "metadata": {
        "id": "QDtto7ccfk2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## setting up new dict\n",
        "\n",
        "data_dict_train  = []\n",
        "data_dict_eval  = []"
      ],
      "metadata": {
        "id": "6iumtx1ge-C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(train_airbnb_london_filtered_advanced.shape[0]):\n",
        "\n",
        "  data_dict_train.append({\n",
        "      \"instruction\": \"Summarize the following description into a short title for an AirBnB listing.\",\n",
        "      \"input\":   train_airbnb_london_filtered_advanced.description[i],\n",
        "      \"output\": train_airbnb_london_filtered_advanced.name[i]\n",
        "  })\n",
        "\n",
        "for i in range(eval_airbnb_london_filtered_advanced.shape[0]):\n",
        "\n",
        "  data_dict_eval.append({\n",
        "      \"instruction\": \"Summarize the following description into a short title for an AirBnB listing.\",\n",
        "      \"input\":   eval_airbnb_london_filtered_advanced.description[i],\n",
        "      \"output\": eval_airbnb_london_filtered_advanced.name[i]\n",
        "  })\n"
      ],
      "metadata": {
        "id": "sLkxLmIne-DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new folder called \"data\""
      ],
      "metadata": {
        "id": "UozUHNijpy3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "Nhw66XaGqFLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing train split ...\")\n",
        "max_seq_length = 256\n",
        "mask_inputs = True\n",
        "train_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in data_dict_train]\n",
        "torch.save(train_set, \"/content/data/train.pt\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22413762-f437-4030-890c-7b149ca8b268",
        "id": "U9aLarO4e-DG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train split ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing test split ...\")\n",
        "max_seq_length = 256\n",
        "mask_inputs = True\n",
        "test_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in data_dict_eval]\n",
        "torch.save(train_set, \"/content/data/test.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edffa95f-b3ef-4675-9de2-b7e3018a4d59",
        "id": "YWy_l6xhe-DL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test split ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating and generating titles after the Fine-Tuning\n",
        "\n",
        "Firstly: computation of ROUGE Scores"
      ],
      "metadata": {
        "id": "80Hh9JxsXQPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def postprocess_text(text_sequence):   ##, labels):\n",
        "\n",
        "    \"\"\"\n",
        "    Post-processing to prepare inputs to the ROGUE functions\n",
        "    \"\"\"\n",
        "\n",
        "    text_sequence = [a.strip() for a in text_sequence]\n",
        "\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    text_sequence = [\"\\n\".join(nltk.sent_tokenize(a)) for a in text_sequence]\n",
        "\n",
        "    return text_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3nk59yvBlPs",
        "outputId": "be89b257-94f2-431b-ad25-8d8d50e3d3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"/content/lit-llama\")\n",
        "\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "from generate import generate\n",
        "from lit_llama import Tokenizer\n",
        "from lit_llama.adapter import LLaMA\n",
        "from lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\n",
        "from scripts.prepare_alpaca import generate_prompt"
      ],
      "metadata": {
        "id": "m1Lh_DNB8VGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adapter_path =\"/content/out/lit-llama-adapter-finetuned.pth\"\n",
        "pretrained_path = \"/content/checkpoints/lit-llama/7B/lit-llama.pth\"\n",
        "tokenizer_path = \"/content/checkpoints/lit-llama/tokenizer.model\"\n",
        "\n",
        "\n",
        "fabric = L.Fabric(devices=1)\n",
        "dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n"
      ],
      "metadata": {
        "id": "0iDfLRpU4A_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function runs slowly, but sadly is needed to call for every generation ...."
      ],
      "metadata": {
        "id": "V3txi9vMMyM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_title(pretrained_path, sample, max_new_tokens, top_k, temperature):\n",
        "\n",
        "  \"\"\" Expects sample as set up above / like in the .pt data \"\"\"\n",
        "\n",
        "  with lazy_load(pretrained_path) as pretrained_checkpoint:\n",
        "      name = llama_model_lookup(pretrained_checkpoint)\n",
        "\n",
        "      with EmptyInitOnDevice(\n",
        "              device=fabric.device, dtype=dtype, quantization_mode = \"llm.int8\"\n",
        "      ):\n",
        "      #   quantization\n",
        "          model = LLaMA.from_name(name)\n",
        "\n",
        "\n",
        "      # 1. Load the pretrained weights\n",
        "      model.load_state_dict(pretrained_checkpoint, strict=False)\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  model = fabric.setup_module(model)\n",
        "\n",
        "  tokenizer = Tokenizer(tokenizer_path)\n",
        "\n",
        "  prompt = generate_prompt(sample)\n",
        "  encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n",
        "  prompt_length = encoded.size(0)\n",
        "\n",
        "  y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n",
        "\n",
        "  output = tokenizer.decode(y)\n",
        "  output = output.split(\"### Response:\")[1].strip()\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "s1WQqwMy6_MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## as else it takes far too long -> rouge with 1000 samples\n",
        "\n",
        "test_set_ = test_set[:1000]"
      ],
      "metadata": {
        "id": "MKbnW_OGLO4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## looping through eval/test samples\n",
        "\n",
        "gen_titles_eval = []\n",
        "\n",
        "for i in range(len(test_set_)):\n",
        "  sample_ = test_set_[i]\n",
        "  gen_title_new = gen_title(pretrained_path, sample_, 30, 200, 0.2)\n",
        "  gen_titles_eval.append(gen_title_new)\n",
        "\n"
      ],
      "metadata": {
        "id": "grNgND_08xCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles_eval_true = [a[\"output\"] for a in test_set_]"
      ],
      "metadata": {
        "id": "W4aXRdwu8kCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h2158ZxYWlp"
      },
      "outputs": [],
      "source": [
        "processed_preds = postprocess_text(gen_titles_eval)\n",
        "processed_labels = postprocess_text(titles_eval_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "AuleVdVKHxmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387d70a6-82ed-4291-f554-716e1756bd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.65.0)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=04abdd9c0f77f508cf2e6f19d16a4c88d1e60aa36f1f035eaed198708d528555\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBzpnVWqZ5kV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "55a0edaf0f8e4a7c980dcb62c4a4dda0",
            "400cab11aa2a46e3b6459a4bc4add75e",
            "cc2e405b03ec466fb674a648c8dfca7f",
            "70121987f3ca45f59a908f1340b66b09",
            "c0dfc46c79334098ab6c3dfadf067906",
            "aa0e6266bac64762b6caabf977c35330",
            "959ba813a8364be7a2afa07bb0f8b634",
            "a125ec9aa7644db496897d36134ac53e",
            "4a9b7627788747479b4d3cdd2d86aed7",
            "3d6898939b344b4abd78eb3a65ff03ef",
            "6b2df51a588041219a0412446c99e9f4"
          ]
        },
        "outputId": "04201c37-f2d6-4a0d-c3ff-83df59495e94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55a0edaf0f8e4a7c980dcb62c4a4dda0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import evaluate\n",
        "rouge_score = evaluate.load(\"rouge\")\n",
        "\n",
        "rouge_score.add_batch(predictions=processed_preds, references=processed_labels)\n",
        "\n",
        "result = rouge_score.compute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The ROUGE metrics on the evaluation data PRIOIR to fine-tuning are:\")\n",
        "print(f'     F1 - ROUGE-1: 0.0579.')\n",
        "print(f'     F1 - ROUGE-2: 0.004.')\n",
        "print(f'     F1 - ROUGE-L: 0.0551.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dXBpA5-S9bn",
        "outputId": "24f72d08-6351-40d9-e332-1793d1a1dc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ROUGE metrics on the evaluation data PRIOIR to fine-tuning are:\n",
            "     F1 - ROUGE-1: 0.0579.\n",
            "     F1 - ROUGE-2: 0.004.\n",
            "     F1 - ROUGE-L: 0.0551.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTQuJ35-L5M1"
      },
      "source": [
        "### Saving generated titles\n",
        "\n",
        "Because this process is extremely slow (even with large A 100 GPU) I shall only generate titles for a subsample - i.e. the same subsample as was used for the Adapter Tuned and LORA-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sub_df = pd.read_csv(\"/content/gdrive/My Drive/Thesis/loss_data/gen_titles_llama_adapter.csv\")\n"
      ],
      "metadata": {
        "id": "NyXbIvooXQnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b68vCsPA_mUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### raus\n",
        "\n",
        "sub_df = pd.merge(sub_df, airbnb_london_filtered_images[[\"id\", \"description\", \"name\"]], how = \"inner\", on = \"id\")"
      ],
      "metadata": {
        "id": "cmDmSay2X1cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df[\"gen_title\"] = \"\""
      ],
      "metadata": {
        "id": "j8gVDXX7zckD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df.index = list(range(sub_df.shape[0]))"
      ],
      "metadata": {
        "id": "WGNOdcOpIh4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## setting up new dict\n",
        "\n",
        "data_dict_sub_df = []\n",
        "\n",
        "for i in range(sub_df.shape[0]):\n",
        "\n",
        "  data_dict_sub_df.append({\n",
        "      \"instruction\": \"Summarize the following description into a short, appealing title for an AirBnB listing.\",\n",
        "      \"input\":   sub_df.description[i],\n",
        "      \"output\": sub_df.name[i]\n",
        "  })\n",
        "\n",
        "\n",
        "max_seq_length = 256\n",
        "mask_inputs = True\n",
        "gen_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in data_dict_sub_df]\n"
      ],
      "metadata": {
        "id": "y2FgkoQ2XsxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_ids_ = list(sub_df.id)"
      ],
      "metadata": {
        "id": "TcV7kr3xn7CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gen_titles = []\n",
        "list_ids = []\n",
        "\n",
        "for i in range(len(gen_set)):\n",
        "  sample_ = gen_set[i]\n",
        "  print(i/len(gen_set))\n",
        "  gen_title_new = gen_title(pretrained_path, sample_, 30, 200, 0.2)\n",
        "  gen_titles.append(gen_title_new)\n",
        "  list_ids.append(list_ids_[i])\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    sub_df = pd.DataFrame({\"id\": list_ids, \"gen_title\":gen_titles})\n",
        "    sub_df.to_csv(\"/content/gdrive/My Drive/Thesis/loss_data/gen_titles_llama_no_peft_v2.csv\", index = False)\n",
        "\n",
        "sub_df = pd.DataFrame({\"id\": list_ids, \"gen_title\":gen_titles})\n",
        "sub_df.to_csv(\"/content/gdrive/My Drive/Thesis/loss_data/gen_titles_llama_no_peft_v2.csv\", index = False)\n"
      ],
      "metadata": {
        "id": "ouArq4DzZXF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # since this code in running overnight in Colab and no resources shall be wasted:\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "g_cvfJg5leDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}