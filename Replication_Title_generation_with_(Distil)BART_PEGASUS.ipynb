{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcuF6Bdp3all"
      },
      "source": [
        "## Easily generate titles using fine-tuned BART, DistilBART and PEGASUS models\n",
        "\n",
        "Using this file, you can easily use these fine-tuned models to generate titles for a listing description of your choice.\n",
        "\n",
        "\n",
        "Note that this requires no GPU and can thus be run on consumer hardware! However, this slows the title generation function down\n",
        "\n",
        "You merely need to define a few variables (see below):\n",
        "\n",
        "* read_in_from_drive: a boolean, set to True if you need to read in the models from gdrive\n",
        "\n",
        "* model_path_bart: the path to the tuned BART model\n",
        "* #model_path_distilbart: the path to the tuned DistilBART model\n",
        "* model_path_pegasus: the path to the tuned PEGASUS model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading in Libraries and defining functions"
      ],
      "metadata": {
        "id": "R1tb5GN8RH3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gv5W_hT33XaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71afc007-e8b1-4a10-8047-b6eda4909143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.28.1 in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (2023.7.22)\n",
            "Requirement already satisfied: huggingface==0.0.1 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: accelerate==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.18.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.18.0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.18.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.18.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.18.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.18.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.18.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.18.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.18.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.18.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.18.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.18.0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.18.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->accelerate==0.18.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate==0.18.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.28.1\n",
        "!pip install huggingface==0.0.1\n",
        "!pip install accelerate==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iQ5Mtq_d35QH"
      },
      "outputs": [],
      "source": [
        "## important all needed packages\n",
        "import transformers\n",
        "import huggingface\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from google.colab import drive\n",
        "from transformers import pipeline\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "import nltk\n",
        "from accelerate import Accelerator\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### functions ####\n",
        "\n",
        "## Code for all functions to be called in later\n",
        "\n",
        "def preprocess_function(input):\n",
        "\n",
        "    \"\"\"\n",
        "    This function applies the tokenizer to both\n",
        "    the descriptons and the names.\n",
        "    max_input_length: Maximum length of tokens in input (description ) - to be defined globally\n",
        "    max_target_length: Maximum number of tokens in output (title) - to be defined globally\n",
        "    \"\"\"\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        input[\"description\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        input[\"name\"], max_length=max_target_length, truncation=True   ## truncate to maximum length\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## function to process data so that it can be used in the finetuning\n",
        "\n",
        "\n",
        "def prepare_data(airbnb_london_filtered_advanced, frac_train_size, batch_size):\n",
        "\n",
        "  \"\"\"\n",
        "  This function prepares the data for the subsequent fine-tuning.\n",
        "  First Input: Dataset (as loaded from Drive)\n",
        "  frac_train_size: fraction to be used in training\n",
        "  batch_size: Batch size to be used in preparing the DataLoaders\n",
        "  \"\"\"\n",
        "\n",
        "  train_eval_airbnb_london_filtered_advanced = airbnb_london_filtered_advanced[airbnb_london_filtered_advanced.in_top_third == 1]\n",
        "\n",
        "  train_airbnb_london_filtered_advanced = train_eval_airbnb_london_filtered_advanced.sample(n = int(np.ceil(frac_train_size*train_eval_airbnb_london_filtered_advanced.shape[0])), random_state = 100)\n",
        "  eval_airbnb_london_filtered_advanced = train_eval_airbnb_london_filtered_advanced.drop(train_airbnb_london_filtered_advanced.index, axis = 0)\n",
        "\n",
        "\n",
        "  ## re-setting the index, else looping through dataloaders results in erros: https://discuss.pytorch.org/t/keyerror-when-enumerating-over-dataloader/54210/8\n",
        "  train_airbnb_london_filtered_advanced.index = list(range(train_airbnb_london_filtered_advanced.shape[0]))\n",
        "  eval_airbnb_london_filtered_advanced.index = list(range(eval_airbnb_london_filtered_advanced.shape[0]))\n",
        "\n",
        "\n",
        "  # tokenized_datasets\n",
        "  tokenized_datasets  = train_airbnb_london_filtered_advanced.apply(preprocess_function, axis = 1)\n",
        "\n",
        "  # eval_tokenized_datasets\n",
        "  eval_tokenized_datasets  = eval_airbnb_london_filtered_advanced.apply(preprocess_function, axis = 1)\n",
        "\n",
        "  ## calling data loader\n",
        "  #batch_size = 8\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "      tokenized_datasets,\n",
        "      shuffle=True,\n",
        "      collate_fn=data_collator,\n",
        "      batch_size=batch_size,\n",
        "  )\n",
        "\n",
        "  eval_dataloader = DataLoader(\n",
        "      eval_tokenized_datasets,\n",
        "  #   shuffle=True,\n",
        "      collate_fn=data_collator,\n",
        "      batch_size=batch_size\n",
        "  )\n",
        "\n",
        "  return train_dataloader, eval_dataloader\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "\n",
        "    \"\"\"\n",
        "    Post-processing to prepare inputs to the ROGUE functions\n",
        "    \"\"\"\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "\n",
        "def generate_title(model_type, description, min_length=5, max_length=25):\n",
        "\n",
        "  \"\"\" This function can be called to easily let a model of your choice generate a title.\n",
        "  - Model_type: Either 'bart', 'distilbart' or 'pegasus'\n",
        "  - Description is obvious\n",
        "  - min_length and max_length denote the token length restrictions on the title to be generated\n",
        "  \"\"\"\n",
        "\n",
        "  if model_type == \"bart\":\n",
        "    output_pipeline = summarizer_bart(description, min_length=min_length, max_length=max_length)\n",
        "  elif model_type == \"distilbart\":\n",
        "    output_pipeline = summarizer_distilbart(description, min_length=min_length, max_length=max_length)\n",
        "  elif model_type == \"pegasus\":\n",
        "    output_pipeline = summarizer_pegasus(description, min_length=min_length, max_length=max_length)\n",
        "  else:\n",
        "    raise Exception(\"model_type must be either 'bart', 'distilbart' or 'pegasus'.\")\n",
        "\n",
        "  return output_pipeline[0]['summary_text']\n",
        "\n",
        "\n",
        "## also reading in associated tokenization functions\n",
        "\n",
        "tokenizer_bart = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
        "tokenizer_distilbart = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-xsum-12-3\")\n",
        "tokenizer_pegasus = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n"
      ],
      "metadata": {
        "id": "0NZTNh_XFGnQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is assumed that you will loadin the models from drive, hence connecting to drive is essential.\n",
        "\n",
        "If you store the models elswhere, ignore his cell or set read_in_from_drive to False"
      ],
      "metadata": {
        "id": "mDD58vm0-dlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "read_in_from_drive = True\n",
        "\n",
        "if read_in_from_drive:\n",
        "\n",
        "  # connecting to drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "else:\n",
        "  pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o8Fh6ZspmqS",
        "outputId": "2d77ba98-6afc-4bed-90d3-6710fb5a2c4a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next, read in the fine-tuned models.\n",
        "\n"
      ],
      "metadata": {
        "id": "ql4ad54VRQ9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ktzao5_QaS3G"
      },
      "outputs": [],
      "source": [
        "# adjust these path-variables correspondingly\n",
        "\n",
        "model_path_bart = \"...\"\n",
        "# model_path_bart = \"/content/gdrive/My Drive/Thesis/Models/bart_finetuned_1_alt.pth\"\n",
        "\n",
        "model_path_distilbart = \"...\"\n",
        "# model_path_distilbart = \"/content/gdrive/My Drive/Thesis/Models/distilbart_finetuned_1_alt.pth\"\n",
        "\n",
        "model_path_pegasus =  \"...\"\n",
        "# model_path_pegasus = \"/content/gdrive/My Drive/Thesis/Models/pegasus_fine_tuned_1.pth\"\n",
        "\n",
        "# models\n",
        "model_bart = torch.load(model_path_bart).to('cpu')\n",
        "model_distilbart = torch.load(model_path_distilbart).to('cpu')\n",
        "model_pegasus = torch.load(model_path_pegasus).to('cpu')\n",
        "\n",
        "# pipelines\n",
        "summarizer_bart = pipeline(\"summarization\", model=model_bart, tokenizer = tokenizer_bart)\n",
        "summarizer_distilbart = pipeline(\"summarization\", model=model_distilbart, tokenizer = tokenizer_distilbart)\n",
        "summarizer_pegasus = pipeline(\"summarization\", model=model_pegasus, tokenizer = tokenizer_pegasus)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a title :)\n",
        "\n",
        "Finally, simply define a description and let the model generate a title!\n",
        "\n",
        "For the BART, pass 'bart' as the first input parameter, for the DistilBART, pass 'distilbart' and for PEGASUS pass 'pegasus'.\n",
        "\n",
        "See the examples provided below:"
      ],
      "metadata": {
        "id": "lGpKxJ2PRjfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = 'The space Bright double bedroom, own living room and own bathroom all on your own floor in our Victorian house in leafy West Hampstead. You will have a mini fridge, toaster, and tea & coffee making facilities in your living room. We also provide you with tea, coffee, cereal, bread & milk & therefore wonâ€™t need to share any spaces with us during this time however, we are always available to advise on places to visit, restaurant, bars etc. As always the space is incredibly clean and we take extra precautions to keep the space safe, strictly following the Airbnb COVID cleansing guidelines. The bedroom has floor to ceiling wardrobes, a chest of drawers, real wood flooring, decorative fireplace, mirror and wireless internet connection. While your own private bathroom is not en-suite it is just a couple of steps away. It is a recently refurbished modern bathroom with power shower and full sized bath. The living room is large bright with bay windows &'\n"
      ],
      "metadata": {
        "id": "8b59EURcN8fq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_title('bart', description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rn0ZJjBCP5Z7",
        "outputId": "f94e1285-7e7e-4d9f-f4b0-ad6ee8e64c3c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bright double bedroom & private bathroom in Hampstead'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_title('distilbart', description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gtS1CZbMQEQw",
        "outputId": "54c4e5df-1c3d-4ae3-d1ef-35269275c48f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bright double bedroom, living room & own bathroom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_title('pegasus', description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t3a_tpYOQGCu",
        "outputId": "ae4c9361-32f0-403d-b447-521a17707693"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bright double bedroom in leafy West Hampstead'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}