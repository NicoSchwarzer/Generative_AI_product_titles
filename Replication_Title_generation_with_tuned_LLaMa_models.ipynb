{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Easily generate titles using fine-tuned LLaMa models\n",
        "\n",
        "\n",
        "Using this file, you can easily use the fine-tuned LLaMa models to generate titles for a listing description of your choice.\n",
        "\n",
        "Importantly, note that this notebook uses the LLaMa-1 generation as LLaMa-2 requires a permit by META AI.\n",
        "\n",
        "Note that this requires a strong GPU!\n",
        "\n",
        "You merely need to define a few variables (see below):\n",
        "\n",
        "* read_in_from_drive: a boolean, set to True if you need to read in the models from gdrive\n",
        "* read_in_file_adapter: the path to the llama-adapter tuned model\n",
        "* read_in_file_lora. the path to the lora tuned model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2J1uKlkQBnR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cloning the environment needed"
      ],
      "metadata": {
        "id": "INqZV08fA67V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nicht\n",
        "\n",
        "!git clone https://github.com/Lightning-AI/lit-llama ## this first ?!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqgY8zzQGqAe",
        "outputId": "d4e5dff4-fdb3-4c92-8d73-2276d065d5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lit-llama'...\n",
            "remote: Enumerating objects: 1865, done.\u001b[K\n",
            "remote: Counting objects: 100% (604/604), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 1865 (delta 532), reused 474 (delta 469), pack-reused 1261\u001b[K\n",
            "Receiving objects: 100% (1865/1865), 1.63 MiB | 3.21 MiB/s, done.\n",
            "Resolving deltas: 100% (1167/1167), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r lit-llama/requirements.txt\n",
        "# might take some time dpeneding on internet connectivity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j5qDaAITASD",
        "outputId": "d3368364-6034-4848-ddcd-dcfef7ba8c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning@ git+https://github.com/Lightning-AI/lightning@master (from -r lit-llama/requirements.txt (line 2))\n",
            "  Cloning https://github.com/Lightning-AI/lightning (to revision master) to /tmp/pip-install-d7ebm41s/lightning_a199327333e445d2852b1d79f7fa3ad0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-d7ebm41s/lightning_a199327333e445d2852b1d79f7fa3ad0\n",
            "  Resolved https://github.com/Lightning-AI/lightning to commit a3218cb0380579d4dccfc0c8b49a8802664291dc\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Encountered 31 file(s) that should have been pointers, but weren't:\n",
            "        .notebooks/course_UvA-DL/01-introduction-to-pytorch.ipynb\n",
            "        .notebooks/course_UvA-DL/02-activation-functions.ipynb\n",
            "        .notebooks/course_UvA-DL/03-initialization-and-optimization.ipynb\n",
            "        .notebooks/course_UvA-DL/04-inception-resnet-densenet.ipynb\n",
            "        .notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb\n",
            "        .notebooks/course_UvA-DL/06-graph-neural-networks.ipynb\n",
            "        .notebooks/course_UvA-DL/07-deep-energy-based-generative-models.ipynb\n",
            "        .notebooks/course_UvA-DL/08-deep-autoencoders.ipynb\n",
            "        .notebooks/course_UvA-DL/09-normalizing-flows.ipynb\n",
            "        .notebooks/course_UvA-DL/10-autoregressive-image-modeling.ipynb\n",
            "        .notebooks/course_UvA-DL/11-vision-transformer.ipynb\n",
            "        .notebooks/course_UvA-DL/12-meta-learning.ipynb\n",
            "        .notebooks/course_UvA-DL/13-contrastive-learning.ipynb\n",
            "        .notebooks/flash_tutorials/electricity_forecasting.ipynb\n",
            "        .notebooks/flash_tutorials/image_classification.ipynb\n",
            "        .notebooks/flash_tutorials/tabular_classification.ipynb\n",
            "        .notebooks/flash_tutorials/text_classification.ipynb\n",
            "        .notebooks/lightning_examples/augmentation_kornia.ipynb\n",
            "        .notebooks/lightning_examples/barlow-twins.ipynb\n",
            "        .notebooks/lightning_examples/basic-gan.ipynb\n",
            "        .notebooks/lightning_examples/cifar10-baseline.ipynb\n",
            "        .notebooks/lightning_examples/datamodules.ipynb\n",
            "        .notebooks/lightning_examples/finetuning-scheduler.ipynb\n",
            "        .notebooks/lightning_examples/mnist-hello-world.ipynb\n",
            "        .notebooks/lightning_examples/mnist-tpu-training.ipynb\n",
            "        .notebooks/lightning_examples/reinforce-learning-DQN.ipynb\n",
            "        .notebooks/lightning_examples/text-transformers.ipynb\n",
            "        .notebooks/lightning_examples/warp-drive.ipynb\n",
            "        .notebooks/templates/img-classify.ipynb\n",
            "        .notebooks/templates/simple.ipynb\n",
            "        .notebooks/templates/titanic.ipynb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting sentencepiece (from -r lit-llama/requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 4)) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 5)) (1.23.5)\n",
            "Collecting jsonargparse[signatures] (from -r lit-llama/requirements.txt (line 6))\n",
            "  Downloading jsonargparse-4.24.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from -r lit-llama/requirements.txt (line 7))\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from -r lit-llama/requirements.txt (line 8))\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from -r lit-llama/requirements.txt (line 9))\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (16.0.6)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (23.1)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading torchmetrics-1.1.1-py3-none-any.whl (763 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.4/763.4 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n",
            "  Downloading pytorch_lightning-2.0.8-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.0/727.0 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6))\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6))\n",
            "  Downloading typeshed_client-2.3.0-py3-none-any.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.6/581.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r lit-llama/requirements.txt (line 8)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r lit-llama/requirements.txt (line 8)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r lit-llama/requirements.txt (line 8)) (2.31.0)\n",
            "Collecting xxhash (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r lit-llama/requirements.txt (line 8)) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets->-r lit-llama/requirements.txt (line 8))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r lit-llama/requirements.txt (line 8)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r lit-llama/requirements.txt (line 8)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r lit-llama/requirements.txt (line 8)) (2023.7.22)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6)) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r lit-llama/requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r lit-llama/requirements.txt (line 8)) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r lit-llama/requirements.txt (line 8)) (1.16.0)\n",
            "Building wheels for collected packages: lightning\n",
            "  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightning: filename=lightning-2.1.0rc0-py3-none-any.whl size=1893961 sha256=cd0a48b9a793bde9d7ab0f772be179d8fb5cc83efceaa017c9ef6576f2a0f0c7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nnc_byuk/wheels/b9/92/07/634ec381ab7d682d3afdcf943d0a6604881441ff2d6c409103\n",
            "Successfully built lightning\n",
            "Installing collected packages: sentencepiece, bitsandbytes, zstandard, xxhash, typeshed-client, lightning-utilities, jsonargparse, docstring-parser, dill, multiprocess, huggingface-hub, datasets, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed bitsandbytes-0.41.1 datasets-2.14.4 dill-0.3.7 docstring-parser-0.15 huggingface-hub-0.16.4 jsonargparse-4.24.0 lightning-2.1.0rc0 lightning-utilities-0.9.0 multiprocess-0.70.15 pytorch-lightning-2.0.8 sentencepiece-0.1.99 torchmetrics-1.1.1 typeshed-client-2.3.0 xxhash-3.3.0 zstandard-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://huggingface.co/openlm-research/open_llama_7b checkpoints/open-llama/7B\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxwew1xBJH5p",
        "outputId": "ce60d7e9-8d34-4dd1-93dc-66b1f0a404b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'checkpoints/open-llama/7B'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 21 (delta 2), reused 0 (delta 0), pack-reused 18\u001b[K\n",
            "Unpacking objects: 100% (21/21), 7.72 KiB | 1.54 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.55 GiB | 4.21 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python lit-llama/scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/open-llama/7B --model_size 7B\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z_FR3LicIwq",
        "outputId": "a886c0b5-1f0e-40fb-dd5b-35d2c21d6161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.utils:Representation` has been removed. We are importing from `pydantic.v1.utils:Representation` instead.See the migration guide for more details: https://docs.pydantic.dev/latest/migration/\n",
            "  warnings.warn(\n",
            "Initializing lit-llama\n",
            "Saving to disk at checkpoints/lit-llama/7B\n",
            "Processing checkpoints/open-llama/7B/pytorch_model-00002-of-00002.bin\n",
            "Processing checkpoints/open-llama/7B/pytorch_model-00001-of-00002.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is assumed that you will loadin the models from drive, hence connecting to drive is essential.\n",
        "\n",
        "If you store the models elswhere, ignore his cell or set read_in_from_drive to False"
      ],
      "metadata": {
        "id": "mDD58vm0-dlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "read_in_from_drive = True\n",
        "\n",
        "if read_in_from_drive:\n",
        "\n",
        "  # connecting to drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "else:\n",
        "  pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o8Fh6ZspmqS",
        "outputId": "8a3df932-0ca9-4274-9026-d5bd9d4caeef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraties and defining some needed functions below."
      ],
      "metadata": {
        "id": "zVUIlUYFAHgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "import torch\n",
        "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
        "import sys\n",
        "import torch\n",
        "import requests\n",
        "import json\n",
        "from torch.utils.data import random_split\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "d3pO8tJU-1PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"/content/lit-llama\")\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "from generate import generate\n",
        "from lit_llama import Tokenizer\n",
        "from lit_llama.adapter import LLaMA\n",
        "from lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\n",
        "from scripts.prepare_alpaca import generate_prompt\n"
      ],
      "metadata": {
        "id": "_VqCC-EF-5Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir out"
      ],
      "metadata": {
        "id": "BnnQim1P__vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"Tokenizer for LLaMA.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: Path) -> None:\n",
        "        self.processor = SentencePieceProcessor(model_file=str(model_path))\n",
        "        self.bos_id = self.processor.bos_id()\n",
        "        self.eos_id = self.processor.eos_id()\n",
        "        self.pad_id = self.processor.pad_id()\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return self.processor.vocab_size()\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        string: str,\n",
        "        bos: bool = True,\n",
        "        eos: bool = False,\n",
        "        max_length: int = -1,\n",
        "        pad: bool = False,\n",
        "        device: Optional[torch.device] = None\n",
        "    ) -> torch.Tensor:\n",
        "        tokens = self.processor.encode(string)\n",
        "        if bos:\n",
        "            tokens = [self.bos_id] + tokens\n",
        "        if eos:\n",
        "            tokens = tokens + [self.eos_id]\n",
        "        if max_length > 0:\n",
        "            tokens = tokens[:max_length]\n",
        "        if pad and len(tokens) < max_length:\n",
        "            tokens += [self.pad_id] * (max_length - len(tokens))\n",
        "\n",
        "        return torch.tensor(tokens, dtype=torch.int, device=device)\n",
        "\n",
        "    def decode(self, tokens: torch.Tensor) -> str:\n",
        "        return self.processor.decode(tokens.tolist())\n",
        "\n",
        "    @staticmethod\n",
        "    def train(input: str, destination: str, vocab_size=32000) -> None:\n",
        "        model_prefix = os.path.join(destination, \"tokenizer\")\n",
        "        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)\n",
        "\n",
        "\n",
        "## general function\n",
        "def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n",
        "    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n",
        "\n",
        "## general function\n",
        "\n",
        "def generate_prompt(example):\n",
        "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
        "    'response' field.\"\"\"\n",
        "\n",
        "    if example[\"input\"]:\n",
        "        return (\n",
        "            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\"\n",
        "        )\n",
        "    return (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_sample(example: dict, tokenizer: Tokenizer, max_length: int, mask_inputs: bool = True):\n",
        "    \"\"\"Processes a single sample.\n",
        "\n",
        "    Each sample in the dataset consists of:\n",
        "    - instruction: A string describing the task\n",
        "    - input: A string holding a special input value for the instruction.\n",
        "        This only applies to some samples, and in others this is empty.\n",
        "    - output: The response string\n",
        "\n",
        "    This function processes this data to produce a prompt text and a label for\n",
        "    supervised training. The input text is formed as a single message including all\n",
        "    the instruction, the input (optional) and the response.\n",
        "    The label/target is the same message but can optionally have the instruction + input text\n",
        "    masked out (mask_inputs=True).\n",
        "\n",
        "    Finally, both the prompt and the label get tokenized. If desired, all tokens\n",
        "    in the label that correspond to the original input prompt get masked out (default).\n",
        "    \"\"\"\n",
        "\n",
        "    full_prompt = generate_prompt(example)\n",
        "    full_prompt_and_response = full_prompt + example[\"output\"]\n",
        "    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=max_length, eos=False)\n",
        "    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=max_length)\n",
        "\n",
        "    # The labels are the full prompt with response, but with the prompt masked out\n",
        "    labels = encoded_full_prompt_and_response.clone()\n",
        "    if mask_inputs:\n",
        "        labels[:len(encoded_full_prompt)]  = -1 ## corresponds to \"ignore index\" inprepare_alpaca.py file\n",
        "\n",
        "\n",
        "    return {**example, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels}\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(\"/content/checkpoints/lit-llama/tokenizer.model\")\n",
        "\n",
        "adapter_path = \"/content/out/lit-llama-adapter-finetuned.pth\"\n",
        "lora_path = \"/content/out/lit-llama-lora-finetuned.pth\"\n",
        "pretrained_path = \"/content/checkpoints/lit-llama/7B/lit-llama.pth\"\n",
        "tokenizer_path = \"/content/checkpoints/lit-llama/tokenizer.model\"\n",
        "\n",
        "\n",
        "def gen_title(pretrained_path, adapter_path, sample, max_new_tokens, top_k, temperature):\n",
        "\n",
        "  \"\"\" Expects sample as set up above / like in the .pt data \"\"\"\n",
        "\n",
        "  fabric = L.Fabric(devices=1)\n",
        "  dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
        "\n",
        "\n",
        "  with lazy_load(pretrained_path) as pretrained_checkpoint, lazy_load(adapter_path) as adapter_checkpoint:\n",
        "      name = llama_model_lookup(pretrained_checkpoint)\n",
        "\n",
        "      with EmptyInitOnDevice(\n",
        "              device=fabric.device, dtype=dtype, quantization_mode = \"llm.int8\"\n",
        "      ):\n",
        "      #   quantization\n",
        "          model = LLaMA.from_name(name)\n",
        "\n",
        "\n",
        "      # 1. Load the pretrained weights\n",
        "      model.load_state_dict(pretrained_checkpoint, strict=False)\n",
        "      # 2. Load the fine-tuned adapter weights\n",
        "      model.load_state_dict(adapter_checkpoint, strict=False)\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  model = fabric.setup_module(model)\n",
        "\n",
        "  tokenizer = Tokenizer(tokenizer_path)\n",
        "\n",
        "  prompt = generate_prompt(sample)\n",
        "  encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n",
        "  prompt_length = encoded.size(0)\n",
        "\n",
        "  y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n",
        "\n",
        "  output = tokenizer.decode(y)\n",
        "  output = output.split(\"### Response:\")[1].strip()\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def tokenize_and_gen_title(type_, prompt, description, max_seq_length = 256):\n",
        "\n",
        "  '''\n",
        "  This function lets users simply provide a prompt and the decription and puts out a title.\n",
        "  The following are the input arguments:\n",
        "   - type:  denotes the type of fine-tuned model, either 'lora' or 'adapter'\n",
        "   - prompt and description are self explanatory\n",
        "   - max_seq_length: maxium length of input sequence, 256 will do for descriptions\n",
        "  '''\n",
        "\n",
        "  print(f\"It may take some time, but soon this function will return a title based on the {type_}-tuned LLaMa model!\")\n",
        "\n",
        "  if type_ == \"lora\":\n",
        "    path = lora_path\n",
        "  elif type_ == 'adapter':\n",
        "    path = adapter_path\n",
        "  else:\n",
        "    raise Exception(\"Type_ must be 'lora' or 'adapter' \")\n",
        "\n",
        "\n",
        "  # creating needed dict object\n",
        "  dict_tokenizer = {'instruction': prompt, 'input':description, 'output': 'no output yet'}\n",
        "\n",
        "\n",
        "  # tokenization\n",
        "  input_sample = prepare_sample(dict_tokenizer, tokenizer, max_seq_length, True)\n",
        "\n",
        "  # generating the title\n",
        "  output = gen_title(pretrained_path, path, input_sample, 30, 200, 0.2)\n",
        "\n",
        "\n",
        "  print(\"The generated title is:\")\n",
        "\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "lj9xr8XqoPMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir out\n"
      ],
      "metadata": {
        "id": "UXPNP2QdyZpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next, read in the fine-tuned models.\n",
        "\n",
        "Define read_in_file_adapter as the path to the llama-adapter tuned model and read_in_file_lora as the path to the lora tuned model."
      ],
      "metadata": {
        "id": "PjTF6P-dARCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change this for you\n",
        "\n",
        "read_in_file_adapter = \"...\"\n",
        "\n",
        "#read_in_file_adapter = /content/gdrive/My Drive/Thesis/Models/lit-llama-adapter-finetuned.pth\"\n",
        "\n",
        "mode_ft = torch.load(read_in_file_adapter)\n",
        "torch.save(mode_ft, \"/content/out/lit-llama-adapter-finetuned.pth\")\n",
        "\n",
        "read_in_file_lora = \"...\"\n",
        "# read_in_file_lora = \"/content/gdrive/My Drive/Thesis/Models/lit-llama-lora-finetuned.pth\"\n",
        "\n",
        "model_ft_lora = torch.load(read_in_file_lora)\n",
        "torch.save(model_ft_lora, \"/content/out/lit-llama-lora-finetuned.pth\")"
      ],
      "metadata": {
        "id": "AyPX0AqKXPuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a title :)\n",
        "\n",
        "Finally, simply define a prompt and a description and let the model generate a title!\n",
        "\n",
        "For the lora-tuned model, pass 'lora' as the first input parameter, for the LLaMa-Adapter model, simply pass 'adapter'.\n",
        "\n",
        "See the examples provided below:"
      ],
      "metadata": {
        "id": "6Ob9uzqZ_Pgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Summarize the following description into a short title for an AirBnB listing.'\n",
        "\n",
        "description = 'The space Bright double bedroom, own living room and own bathroom all on your own floor in our Victorian house in leafy West Hampstead. You will have a mini fridge, toaster, and tea & coffee making facilities in your living room. We also provide you with tea, coffee, cereal, bread & milk & therefore won’t need to share any spaces with us during this time however, we are always available to advise on places to visit, restaurant, bars etc. As always the space is incredibly clean and we take extra precautions to keep the space safe, strictly following the Airbnb COVID cleansing guidelines. The bedroom has floor to ceiling wardrobes, a chest of drawers, real wood flooring, decorative fireplace, mirror and wireless internet connection. While your own private bathroom is not en-suite it is just a couple of steps away. It is a recently refurbished modern bathroom with power shower and full sized bath. The living room is large bright with bay windows &'\n"
      ],
      "metadata": {
        "id": "63xkWl444Iry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_and_gen_title('adapter', prompt, description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "LVkdqFBY8luN",
        "outputId": "849fc90e-f737-4327-8a20-6096b43757aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It may take some time, but soon this function will return a title based on the adapter-tuned LLaMa model!\n",
            "The generated title is:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bright, sunny, double bedroom with own bathroom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_and_gen_title('lora', prompt, description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "mLQoRk1B9-Cy",
        "outputId": "f461df36-5850-4e3b-8ef7-c637ab2e6bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It may take some time, but soon this function will return a title based on the lora-tuned LLaMa model!\n",
            "The generated title is:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bright double with own bathroom in North West London'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}