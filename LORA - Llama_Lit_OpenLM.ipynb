{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zaA8b-Wy3-NILLCvYtexMVqMlak3siq8","timestamp":1686322536128}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOgOlgtzU/qsfo82KZ+7kLt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"55a0edaf0f8e4a7c980dcb62c4a4dda0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_400cab11aa2a46e3b6459a4bc4add75e","IPY_MODEL_cc2e405b03ec466fb674a648c8dfca7f","IPY_MODEL_70121987f3ca45f59a908f1340b66b09"],"layout":"IPY_MODEL_c0dfc46c79334098ab6c3dfadf067906"}},"400cab11aa2a46e3b6459a4bc4add75e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa0e6266bac64762b6caabf977c35330","placeholder":"​","style":"IPY_MODEL_959ba813a8364be7a2afa07bb0f8b634","value":"Downloading builder script: 100%"}},"cc2e405b03ec466fb674a648c8dfca7f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a125ec9aa7644db496897d36134ac53e","max":6270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a9b7627788747479b4d3cdd2d86aed7","value":6270}},"70121987f3ca45f59a908f1340b66b09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d6898939b344b4abd78eb3a65ff03ef","placeholder":"​","style":"IPY_MODEL_6b2df51a588041219a0412446c99e9f4","value":" 6.27k/6.27k [00:00&lt;00:00, 468kB/s]"}},"c0dfc46c79334098ab6c3dfadf067906":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa0e6266bac64762b6caabf977c35330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"959ba813a8364be7a2afa07bb0f8b634":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a125ec9aa7644db496897d36134ac53e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a9b7627788747479b4d3cdd2d86aed7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d6898939b344b4abd78eb3a65ff03ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b2df51a588041219a0412446c99e9f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## Code for LLaMa-LORA Tuning\n","\n","Find the code for tuning the LLaMa-7B mode using the LORA method below. It is an implementation of LLaMA-tuninig which is \"fully open source under the Apache 2.0 license.\""],"metadata":{"id":"L9ikghCnCZ_g"}},{"cell_type":"markdown","source":["https://github.com/Lightning-AI/lit-llama/blob/main/howto/finetune_adapter.md"],"metadata":{"id":"UEQoizoUSwmZ"}},{"cell_type":"code","source":["!git clone https://github.com/Lightning-AI/lit-llama ## this first ?!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwZAoFRmlWmc","executionInfo":{"status":"ok","timestamp":1686669452259,"user_tz":-120,"elapsed":1576,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"49d7188f-9f42-41b9-bac5-fc9c56e79c0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'lit-llama'...\n","remote: Enumerating objects: 1710, done.\u001b[K\n","remote: Counting objects: 100% (95/95), done.\u001b[K\n","remote: Compressing objects: 100% (70/70), done.\u001b[K\n","remote: Total 1710 (delta 46), reused 54 (delta 25), pack-reused 1615\u001b[K\n","Receiving objects: 100% (1710/1710), 670.91 KiB | 6.64 MiB/s, done.\n","Resolving deltas: 100% (1052/1052), done.\n"]}]},{"cell_type":"code","source":["!git clone https://huggingface.co/openlm-research/open_llama_7b_700bt_preview checkpoints/open-llama/7B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWs_bKIslAIJ","executionInfo":{"status":"ok","timestamp":1686669666203,"user_tz":-120,"elapsed":213979,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"e7e5a9ae-3f00-4cdb-e665-7154b7b0a37d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'checkpoints/open-llama/7B'...\n","remote: Enumerating objects: 25, done.\u001b[K\n","remote: Counting objects:   4% (1/25)\u001b[K\rremote: Counting objects:   8% (2/25)\u001b[K\rremote: Counting objects:  12% (3/25)\u001b[K\rremote: Counting objects:  16% (4/25)\u001b[K\rremote: Counting objects:  20% (5/25)\u001b[K\rremote: Counting objects:  24% (6/25)\u001b[K\rremote: Counting objects:  28% (7/25)\u001b[K\rremote: Counting objects:  32% (8/25)\u001b[K\rremote: Counting objects:  36% (9/25)\u001b[K\rremote: Counting objects:  40% (10/25)\u001b[K\rremote: Counting objects:  44% (11/25)\u001b[K\rremote: Counting objects:  48% (12/25)\u001b[K\rremote: Counting objects:  52% (13/25)\u001b[K\rremote: Counting objects:  56% (14/25)\u001b[K\rremote: Counting objects:  60% (15/25)\u001b[K\rremote: Counting objects:  64% (16/25)\u001b[K\rremote: Counting objects:  68% (17/25)\u001b[K\rremote: Counting objects:  72% (18/25)\u001b[K\rremote: Counting objects:  76% (19/25)\u001b[K\rremote: Counting objects:  80% (20/25)\u001b[K\rremote: Counting objects:  84% (21/25)\u001b[K\rremote: Counting objects:  88% (22/25)\u001b[K\rremote: Counting objects:  92% (23/25)\u001b[K\rremote: Counting objects:  96% (24/25)\u001b[K\rremote: Counting objects: 100% (25/25)\u001b[K\rremote: Counting objects: 100% (25/25), done.\u001b[K\n","remote: Compressing objects: 100% (23/23), done.\u001b[K\n","remote: Total 25 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (25/25), 498.91 KiB | 3.39 MiB/s, done.\n","Filtering content: 100% (3/3), 4.55 GiB | 21.80 MiB/s, done.\n","Encountered 1 file(s) that may not have been copied correctly on Windows:\n","\tpytorch_model-00001-of-00002.bin\n","\n","See: `git lfs help smudge` for more details.\n"]}]},{"cell_type":"code","source":["!pip install -r lit-llama/requirements.txt\n","# might take some time dpeneding on internet connectivity\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5j5qDaAITASD","executionInfo":{"status":"ok","timestamp":1686669711641,"user_tz":-120,"elapsed":45512,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"cfe7a40c-d6bf-4e9d-99c7-06e03820b717"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting lightning@ git+https://github.com/Lightning-AI/lightning@master (from -r lit-llama/requirements.txt (line 2))\n","  Cloning https://github.com/Lightning-AI/lightning (to revision master) to /tmp/pip-install-l2p8xyrg/lightning_b593f0223a904f6c9d1c59767d4b4312\n","  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-l2p8xyrg/lightning_b593f0223a904f6c9d1c59767d4b4312\n","  Resolved https://github.com/Lightning-AI/lightning to commit ca30fd7752582201a3966806c92e3acbbaf2a045\n","  Running command git submodule update --init --recursive -q\n","  Encountered 31 file(s) that should have been pointers, but weren't:\n","        .notebooks/course_UvA-DL/01-introduction-to-pytorch.ipynb\n","        .notebooks/course_UvA-DL/02-activation-functions.ipynb\n","        .notebooks/course_UvA-DL/03-initialization-and-optimization.ipynb\n","        .notebooks/course_UvA-DL/04-inception-resnet-densenet.ipynb\n","        .notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb\n","        .notebooks/course_UvA-DL/06-graph-neural-networks.ipynb\n","        .notebooks/course_UvA-DL/07-deep-energy-based-generative-models.ipynb\n","        .notebooks/course_UvA-DL/08-deep-autoencoders.ipynb\n","        .notebooks/course_UvA-DL/09-normalizing-flows.ipynb\n","        .notebooks/course_UvA-DL/10-autoregressive-image-modeling.ipynb\n","        .notebooks/course_UvA-DL/11-vision-transformer.ipynb\n","        .notebooks/course_UvA-DL/12-meta-learning.ipynb\n","        .notebooks/course_UvA-DL/13-contrastive-learning.ipynb\n","        .notebooks/flash_tutorials/electricity_forecasting.ipynb\n","        .notebooks/flash_tutorials/image_classification.ipynb\n","        .notebooks/flash_tutorials/tabular_classification.ipynb\n","        .notebooks/flash_tutorials/text_classification.ipynb\n","        .notebooks/lightning_examples/augmentation_kornia.ipynb\n","        .notebooks/lightning_examples/barlow-twins.ipynb\n","        .notebooks/lightning_examples/basic-gan.ipynb\n","        .notebooks/lightning_examples/cifar10-baseline.ipynb\n","        .notebooks/lightning_examples/datamodules.ipynb\n","        .notebooks/lightning_examples/finetuning-scheduler.ipynb\n","        .notebooks/lightning_examples/mnist-hello-world.ipynb\n","        .notebooks/lightning_examples/mnist-tpu-training.ipynb\n","        .notebooks/lightning_examples/reinforce-learning-DQN.ipynb\n","        .notebooks/lightning_examples/text-transformers.ipynb\n","        .notebooks/lightning_examples/warp-drive.ipynb\n","        .notebooks/templates/img-classify.ipynb\n","        .notebooks/templates/simple.ipynb\n","        .notebooks/templates/titanic.ipynb\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 1)) (2.0.1+cu118)\n","Collecting sentencepiece (from -r lit-llama/requirements.txt (line 3))\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 4)) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r lit-llama/requirements.txt (line 5)) (1.22.4)\n","Collecting jsonargparse[signatures] (from -r lit-llama/requirements.txt (line 6))\n","  Downloading jsonargparse-4.21.2-py3-none-any.whl (192 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes (from -r lit-llama/requirements.txt (line 7))\n","  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from -r lit-llama/requirements.txt (line 8))\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting zstandard (from -r lit-llama/requirements.txt (line 9))\n","  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (16.0.5)\n","Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (6.0)\n","Collecting arrow<3.0,>=1.2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (4.11.2)\n","Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (8.1.3)\n","Collecting croniter<1.4.0,>=1.3.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading croniter-1.3.15-py2.py3-none-any.whl (19 kB)\n","Collecting dateutils<2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n","Collecting deepdiff<8.0,>=5.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading deepdiff-6.3.0-py3-none-any.whl (69 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi<0.89.0,>=0.69.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec<2024.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2023.4.0)\n","Collecting inquirer<5.0,>=2.10.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n","Collecting lightning-cloud>=0.5.34 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading lightning_cloud-0.5.36-py3-none-any.whl (562 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lightning-utilities<2.0,>=0.8.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (23.1)\n","Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (5.9.5)\n","Requirement already satisfied: pydantic<4.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.10.7)\n","Collecting python-multipart<2.0,>=0.0.5 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.27.1)\n","Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (13.3.4)\n","Collecting starlette (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading starlette-0.28.0-py3-none-any.whl (68 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n","Collecting torchmetrics<2.0,>=0.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (5.7.1)\n","Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.26.15)\n","Collecting uvicorn<2.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.5.1)\n","Collecting websockets<12.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading pytorch_lightning-2.0.3-py3-none-any.whl (720 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.6/720.6 kB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6))\n","  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n","Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6))\n","  Downloading typeshed_client-2.3.0-py3-none-any.whl (581 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.6/581.6 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r lit-llama/requirements.txt (line 8)) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r lit-llama/requirements.txt (line 8)) (1.5.3)\n","Collecting xxhash (from datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp (from datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19 (from datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.4.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2022.7.1)\n","Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n","Collecting starlette (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (3.6.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8)) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r lit-llama/requirements.txt (line 8))\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n","Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (2.1.2)\n","Collecting pyjwt (from lightning-cloud>=0.5.34->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.34->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.16.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (3.4)\n","Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.2.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.14.0)\n","Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (2.1.2)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]->-r lit-llama/requirements.txt (line 6)) (5.12.0)\n","Collecting h11>=0.8 (from uvicorn<2.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2))\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->-r lit-llama/requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (0.2.6)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (0.1.2)\n","Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r lit-llama/requirements.txt (line 2)) (67.7.2)\n","Building wheels for collected packages: lightning\n","  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lightning: filename=lightning-2.1.0.dev0-py3-none-any.whl size=1861287 sha256=90c7708c3b6c16044ebdd9943f66f6eaebef8670f4b8a8d755c3abac4acd877e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cekzteyg/wheels/b9/92/07/634ec381ab7d682d3afdcf943d0a6604881441ff2d6c409103\n","Successfully built lightning\n","Installing collected packages: sentencepiece, python-editor, bitsandbytes, zstandard, xxhash, websockets, typeshed-client, readchar, python-multipart, pyjwt, ordered-set, multidict, lightning-utilities, jsonargparse, h11, frozenlist, docstring-parser, dill, blessed, async-timeout, yarl, uvicorn, starlette, responses, multiprocess, inquirer, huggingface-hub, deepdiff, dateutils, croniter, arrow, aiosignal, starsessions, fastapi, aiohttp, lightning-cloud, datasets, torchmetrics, pytorch-lightning, lightning\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 arrow-1.2.3 async-timeout-4.0.2 bitsandbytes-0.39.0 blessed-1.20.0 croniter-1.3.15 datasets-2.12.0 dateutils-0.6.12 deepdiff-6.3.0 dill-0.3.6 docstring-parser-0.15 fastapi-0.88.0 frozenlist-1.3.3 h11-0.14.0 huggingface-hub-0.15.1 inquirer-3.1.3 jsonargparse-4.21.2 lightning-2.1.0.dev0 lightning-cloud-0.5.36 lightning-utilities-0.8.0 multidict-6.0.4 multiprocess-0.70.14 ordered-set-4.1.0 pyjwt-2.7.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.3 readchar-4.0.5 responses-0.18.0 sentencepiece-0.1.99 starlette-0.22.0 starsessions-1.3.0 torchmetrics-0.11.4 typeshed-client-2.3.0 uvicorn-0.22.0 websockets-11.0.3 xxhash-3.2.0 yarl-1.9.2 zstandard-0.21.0\n"]}]},{"cell_type":"code","source":["!python lit-llama/scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/open-llama/7B --model_size 7B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6z_FR3LicIwq","executionInfo":{"status":"ok","timestamp":1686669798098,"user_tz":-120,"elapsed":86489,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"ac8700c1-9527-41c3-eaf8-eb87a6f49051"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing lit-llama\n","Saving to disk at checkpoints/lit-llama/7B\n","Processing checkpoints/open-llama/7B/pytorch_model-00002-of-00002.bin\n","Processing checkpoints/open-llama/7B/pytorch_model-00001-of-00002.bin\n"]}]},{"cell_type":"markdown","source":["## Finetuning try 1\n","\n","https://github.com/Lightning-AI/lit-llama/blob/main/howto/finetune_adapter.md"],"metadata":{"id":"2DYCLSAFfG73"}},{"cell_type":"code","source":["# connecting to drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1o8Fh6ZspmqS","executionInfo":{"status":"ok","timestamp":1686669823708,"user_tz":-120,"elapsed":25636,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"f515174b-6f33-43a0-f721-616bdbe3ea90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["### Loading in data from drive and setting up own dataloader (.pt) files like the function in the python file prepare_alpacy do."],"metadata":{"id":"UQ732ZBufCMv"}},{"cell_type":"code","source":["\n","import pandas as pd\n","airbnb_london_filtered_images = pd.read_csv(\"/content/gdrive/My Drive/Thesis/London_Data/discriminator_tabular_data_images.csv\")"],"metadata":{"id":"zikMD2xup37L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","frac_train_size = 0.7\n","\n","train_eval_airbnb_london_filtered_images = airbnb_london_filtered_images[airbnb_london_filtered_images.in_top_third == 1]\n","\n","train_airbnb_london_filtered_advanced = train_eval_airbnb_london_filtered_images.sample(n = int(np.ceil(frac_train_size*train_eval_airbnb_london_filtered_images.shape[0])), random_state = 100)\n","eval_airbnb_london_filtered_advanced = train_eval_airbnb_london_filtered_images.drop(train_airbnb_london_filtered_advanced.index, axis = 0)\n","\n","train_airbnb_london_filtered_advanced.index = list(range(train_airbnb_london_filtered_advanced.shape[0]))\n","eval_airbnb_london_filtered_advanced.index = list(range(eval_airbnb_london_filtered_advanced.shape[0]))\n"],"metadata":{"id":"wPoGSTBRqMjC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Logical dependencies of functions in prepare_alpaca file\n","\n","'prepare' uses\n","> 'prepare_sample' uses\n",">> 'tokenzize' & 'generate promt'\n","\n"],"metadata":{"id":"TrrjHt0co2GL"}},{"cell_type":"code","source":["## Using the following instruction:\n","\n","instruction = \"Summarize the following description into a short title for an AirBnB listing.\"\n"],"metadata":{"id":"kOxkIdFaffZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","from typing import Optional\n","import torch\n","from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n","import sys\n","import torch\n","import requests\n","import json\n","from torch.utils.data import random_split\n","from tqdm import tqdm\n","\n","class Tokenizer:\n","    \"\"\"Tokenizer for LLaMA.\"\"\"\n","\n","    def __init__(self, model_path: Path) -> None:\n","        self.processor = SentencePieceProcessor(model_file=str(model_path))\n","        self.bos_id = self.processor.bos_id()\n","        self.eos_id = self.processor.eos_id()\n","        self.pad_id = self.processor.pad_id()\n","\n","    @property\n","    def vocab_size(self) -> int:\n","        return self.processor.vocab_size()\n","\n","    def encode(\n","        self,\n","        string: str,\n","        bos: bool = True,\n","        eos: bool = False,\n","        max_length: int = -1,\n","        pad: bool = False,\n","        device: Optional[torch.device] = None\n","    ) -> torch.Tensor:\n","        tokens = self.processor.encode(string)\n","        if bos:\n","            tokens = [self.bos_id] + tokens\n","        if eos:\n","            tokens = tokens + [self.eos_id]\n","        if max_length > 0:\n","            tokens = tokens[:max_length]\n","        if pad and len(tokens) < max_length:\n","            tokens += [self.pad_id] * (max_length - len(tokens))\n","\n","        return torch.tensor(tokens, dtype=torch.int, device=device)\n","\n","    def decode(self, tokens: torch.Tensor) -> str:\n","        return self.processor.decode(tokens.tolist())\n","\n","    @staticmethod\n","    def train(input: str, destination: str, vocab_size=32000) -> None:\n","        model_prefix = os.path.join(destination, \"tokenizer\")\n","        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)\n","\n","\n","## general function\n","def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n","    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n","\n","## general function\n","\n","def generate_prompt(example):\n","    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n","    'response' field.\"\"\"\n","\n","    if example[\"input\"]:\n","        return (\n","            \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n","            \"Write a response that appropriately completes the request.\\n\\n\"\n","            f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\"\n","        )\n","    return (\n","        \"Below is an instruction that describes a task. \"\n","        \"Write a response that appropriately completes the request.\\n\\n\"\n","        f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n","    )\n","\n","\n","def prepare_sample(example: dict, tokenizer: Tokenizer, max_length: int, mask_inputs: bool = True):\n","    \"\"\"Processes a single sample.\n","\n","    Each sample in the dataset consists of:\n","    - instruction: A string describing the task\n","    - input: A string holding a special input value for the instruction.\n","        This only applies to some samples, and in others this is empty.\n","    - output: The response string\n","\n","    This function processes this data to produce a prompt text and a label for\n","    supervised training. The input text is formed as a single message including all\n","    the instruction, the input (optional) and the response.\n","    The label/target is the same message but can optionally have the instruction + input text\n","    masked out (mask_inputs=True).\n","\n","    Finally, both the prompt and the label get tokenized. If desired, all tokens\n","    in the label that correspond to the original input prompt get masked out (default).\n","    \"\"\"\n","\n","    full_prompt = generate_prompt(example)\n","    full_prompt_and_response = full_prompt + example[\"output\"]\n","    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=max_length, eos=False)\n","    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=max_length)\n","\n","    # The labels are the full prompt with response, but with the prompt masked out\n","    labels = encoded_full_prompt_and_response.clone()\n","    if mask_inputs:\n","        labels[:len(encoded_full_prompt)]  = -1 ## corresponds to \"ignore index\" inprepare_alpaca.py file\n","\n","\n","    return {**example, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels}\n","\n","\n","\n","\n","#### prepare function is not required!"],"metadata":{"id":"lj9xr8XqoPMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer(\"/content/checkpoints/lit-llama/tokenizer.model\")"],"metadata":{"id":"5hXIcoNftMnM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reocding the prepare function:"],"metadata":{"id":"QDtto7ccfk2W"}},{"cell_type":"code","source":["## setting up new dict\n","\n","data_dict_train  = []\n","data_dict_eval  = []"],"metadata":{"id":"6iumtx1ge-C8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for i in range(train_airbnb_london_filtered_advanced.shape[0]):\n","\n","  data_dict_train.append({\n","      \"instruction\": \"Summarize the following description into a short title for an AirBnB listing.\",\n","      \"input\":   train_airbnb_london_filtered_advanced.description[i],\n","      \"output\": train_airbnb_london_filtered_advanced.name[i]\n","  })\n","\n","for i in range(eval_airbnb_london_filtered_advanced.shape[0]):\n","\n","  data_dict_eval.append({\n","      \"instruction\": \"Summarize the following description into a short title for an AirBnB listing.\",\n","      \"input\":   eval_airbnb_london_filtered_advanced.description[i],\n","      \"output\": eval_airbnb_london_filtered_advanced.name[i]\n","  })\n"],"metadata":{"id":"sLkxLmIne-DC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a new folder called \"data\""],"metadata":{"id":"UozUHNijpy3-"}},{"cell_type":"code","source":["!mkdir data"],"metadata":{"id":"Nhw66XaGqFLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Processing train split ...\")\n","max_seq_length = 256\n","mask_inputs = True\n","train_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in data_dict_train]\n","torch.save(train_set, \"/content/data/train.pt\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686388232471,"user_tz":-120,"elapsed":7152,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"3c6fb03f-e41d-4ef7-860f-25c39702be1f","id":"U9aLarO4e-DG"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing train split ...\n"]}]},{"cell_type":"code","source":["print(\"Processing test split ...\")\n","max_seq_length = 256\n","mask_inputs = True\n","test_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in data_dict_eval]\n","torch.save(train_set, \"/content/data/test.pt\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686388235488,"user_tz":-120,"elapsed":3036,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"873670e1-b603-4cbe-e5bf-aeada2d1bce5","id":"YWy_l6xhe-DL"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing test split ...\n"]}]},{"cell_type":"code","source":["epoch_size = len(train_set)\n","epoch_size\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXGOMErRxGJ-","executionInfo":{"status":"ok","timestamp":1686388235496,"user_tz":-120,"elapsed":50,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"1aca695c-fd93-48b7-87ef-7df5f552ddb6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4048"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["#### Actual fine-tuning\n","\n","The finetuning file is called. However, a few changes need to be made to this file (lora.py)\n","\n","* in line 39, the size must be set to 4048 (size of my training set)\n","* in line 42, the lora_r hyper-param can be adjusted. I kept it at r = 8 as was suggested by LORA authors!\n","* in line 49, 'data_dict' must be set to \"data\"\n","* in line 52, 'out_dir' can be set to \"out/adapter/airbnb\" (Not necessary, just nicer!)"],"metadata":{"id":"i6bhdYvVia9P"}},{"cell_type":"code","source":["!python lit-llama/finetune/lora.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ie65XLXjqfWm","executionInfo":{"status":"ok","timestamp":1686325184641,"user_tz":-120,"elapsed":764729,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"54cf1068-be16-49e2-ce32-9a35de911b24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Global seed set to 1337\n","iter 0: loss 4.8885, time: 1971.93ms\n","iter 1: loss 3.6379, time: 243.85ms\n","iter 2: loss 3.2024, time: 267.48ms\n","iter 3: loss nan, time: 245.99ms\n","iter 4: loss 3.7454, time: 241.74ms\n","iter 5: loss 2.9494, time: 245.66ms\n","iter 6: loss 3.5960, time: 243.25ms\n","iter 7: loss 4.7820, time: 242.33ms\n","iter 8: loss 4.1417, time: 241.68ms\n","iter 9: loss 4.1249, time: 244.49ms\n","iter 10: loss 5.3503, time: 242.76ms\n","iter 11: loss 4.1460, time: 242.69ms\n","iter 12: loss 3.1389, time: 243.62ms\n","iter 13: loss 3.4740, time: 242.35ms\n","iter 14: loss 4.7652, time: 243.10ms\n","iter 15: loss 4.4754, time: 241.69ms\n","iter 16: loss 3.3450, time: 242.08ms\n","iter 17: loss 3.5197, time: 241.66ms\n","iter 18: loss 6.0065, time: 241.32ms\n","iter 19: loss 3.8131, time: 242.69ms\n","iter 20: loss 2.7893, time: 243.69ms\n","iter 21: loss 3.6749, time: 240.87ms\n","iter 22: loss 2.6452, time: 241.16ms\n","iter 23: loss 5.0563, time: 241.72ms\n","iter 24: loss 4.1168, time: 242.25ms\n","iter 25: loss 5.2320, time: 241.57ms\n","iter 26: loss 3.3037, time: 241.28ms\n","iter 27: loss 3.3532, time: 241.57ms\n","iter 28: loss 5.7482, time: 242.21ms\n","iter 29: loss nan, time: 243.06ms\n","iter 30: loss 3.7819, time: 241.83ms\n","iter 31: loss 3.7901, time: 346.51ms\n","iter 32: loss 3.8829, time: 242.31ms\n","iter 33: loss 3.6263, time: 242.65ms\n","iter 34: loss 2.8521, time: 242.91ms\n","iter 35: loss 3.2774, time: 243.29ms\n","iter 36: loss 2.9498, time: 243.71ms\n","iter 37: loss 3.1872, time: 244.12ms\n","iter 38: loss 3.1625, time: 243.30ms\n","iter 39: loss 4.2432, time: 243.19ms\n","iter 40: loss 3.5870, time: 242.69ms\n","iter 41: loss 4.0681, time: 242.97ms\n","iter 42: loss nan, time: 242.58ms\n","iter 43: loss 2.7657, time: 242.89ms\n","iter 44: loss nan, time: 242.69ms\n","iter 45: loss 4.3635, time: 242.71ms\n","iter 46: loss 3.5342, time: 242.49ms\n","iter 47: loss 3.9496, time: 242.47ms\n","iter 48: loss 4.6404, time: 242.16ms\n","iter 49: loss 4.1006, time: 242.26ms\n","iter 50: loss 2.8791, time: 241.84ms\n","iter 51: loss 3.0679, time: 242.24ms\n","iter 52: loss 3.5745, time: 242.84ms\n","iter 53: loss 4.1850, time: 243.56ms\n","iter 54: loss 3.4172, time: 243.06ms\n","iter 55: loss 4.3863, time: 243.81ms\n","iter 56: loss 2.8578, time: 243.02ms\n","iter 57: loss 5.6036, time: 242.69ms\n","iter 58: loss 3.8253, time: 241.98ms\n","iter 59: loss 5.0897, time: 242.24ms\n","iter 60: loss 3.2903, time: 255.26ms\n","iter 61: loss 4.4778, time: 242.92ms\n","iter 62: loss 3.2371, time: 241.50ms\n","iter 63: loss 3.5010, time: 245.17ms\n","iter 64: loss nan, time: 243.12ms\n","iter 65: loss 3.4421, time: 242.52ms\n","iter 66: loss nan, time: 242.63ms\n","iter 67: loss 3.6123, time: 243.04ms\n","iter 68: loss 4.3102, time: 243.08ms\n","iter 69: loss 4.7362, time: 246.05ms\n","iter 70: loss 3.8979, time: 242.59ms\n","iter 71: loss 4.7452, time: 240.97ms\n","iter 72: loss 4.2248, time: 241.07ms\n","iter 73: loss 5.4309, time: 241.79ms\n","iter 74: loss 3.7493, time: 241.49ms\n","iter 75: loss 5.1113, time: 242.90ms\n","iter 76: loss 7.2614, time: 243.35ms\n","iter 77: loss 4.5833, time: 242.07ms\n","iter 78: loss 4.1447, time: 242.05ms\n","iter 79: loss 4.7067, time: 241.14ms\n","iter 80: loss 4.1515, time: 241.82ms\n","iter 81: loss 3.7813, time: 241.17ms\n","iter 82: loss 3.8835, time: 242.46ms\n","iter 83: loss 2.6020, time: 241.95ms\n","iter 84: loss nan, time: 241.23ms\n","iter 85: loss 3.1133, time: 242.39ms\n","iter 86: loss 4.8877, time: 241.53ms\n","iter 87: loss 3.7353, time: 242.01ms\n","iter 88: loss 3.4383, time: 241.81ms\n","iter 89: loss 3.1657, time: 242.19ms\n","iter 90: loss 3.1608, time: 242.08ms\n","iter 91: loss 2.8308, time: 241.64ms\n","iter 92: loss 3.4271, time: 248.62ms\n","iter 93: loss 2.7488, time: 243.36ms\n","iter 94: loss 4.2106, time: 243.44ms\n","iter 95: loss nan, time: 243.37ms\n","iter 96: loss 3.5951, time: 240.48ms\n","iter 97: loss 3.9528, time: 242.33ms\n","iter 98: loss 4.1742, time: 242.01ms\n","iter 99: loss 4.4079, time: 241.82ms\n","iter 100: loss 3.9919, time: 241.79ms\n","iter 101: loss 3.5508, time: 241.47ms\n","iter 102: loss 3.7529, time: 242.80ms\n","iter 103: loss 4.1235, time: 241.21ms\n","iter 104: loss 4.5119, time: 240.42ms\n","iter 105: loss 3.4150, time: 241.49ms\n","iter 106: loss 3.5720, time: 241.24ms\n","iter 107: loss 5.9008, time: 241.05ms\n","iter 108: loss 4.8223, time: 240.93ms\n","iter 109: loss 4.0504, time: 241.18ms\n","iter 110: loss 3.1565, time: 220.46ms\n","iter 111: loss 5.6614, time: 241.78ms\n","iter 112: loss 2.8647, time: 242.33ms\n","iter 113: loss nan, time: 242.85ms\n","iter 114: loss 4.0665, time: 242.98ms\n","iter 115: loss nan, time: 241.37ms\n","iter 116: loss 4.8397, time: 246.32ms\n","iter 117: loss 3.9874, time: 243.57ms\n","iter 118: loss nan, time: 242.39ms\n","iter 119: loss 4.6061, time: 241.35ms\n","iter 120: loss 3.0769, time: 241.57ms\n","iter 121: loss 3.0899, time: 243.88ms\n","iter 122: loss nan, time: 243.36ms\n","iter 123: loss 5.0649, time: 243.93ms\n","iter 124: loss nan, time: 242.40ms\n","iter 125: loss 3.4736, time: 242.44ms\n","iter 126: loss 2.7914, time: 242.33ms\n","iter 127: loss 3.8581, time: 244.57ms\n","iter 128: loss 4.6709, time: 241.03ms\n","iter 129: loss 3.4821, time: 248.48ms\n","iter 130: loss 3.7079, time: 242.06ms\n","iter 131: loss 4.8924, time: 241.40ms\n","iter 132: loss 4.3317, time: 241.91ms\n","iter 133: loss 3.8677, time: 241.12ms\n","iter 134: loss 3.1504, time: 241.89ms\n","iter 135: loss 3.4839, time: 242.13ms\n","iter 136: loss 3.7895, time: 216.66ms\n","iter 137: loss 4.8613, time: 241.24ms\n","iter 138: loss 3.8397, time: 241.52ms\n","iter 139: loss 3.4223, time: 242.54ms\n","iter 140: loss 3.6886, time: 241.58ms\n","iter 141: loss 2.6717, time: 241.75ms\n","iter 142: loss 2.9791, time: 243.24ms\n","iter 143: loss 3.8179, time: 242.02ms\n","iter 144: loss 4.3913, time: 241.48ms\n","iter 145: loss nan, time: 243.89ms\n","iter 146: loss 3.9782, time: 241.46ms\n","iter 147: loss 6.0473, time: 243.07ms\n","iter 148: loss 3.7585, time: 241.07ms\n","iter 149: loss 4.3860, time: 241.14ms\n","iter 150: loss 3.8754, time: 241.00ms\n","iter 151: loss nan, time: 240.82ms\n","iter 152: loss 2.2114, time: 241.09ms\n","iter 153: loss 3.8513, time: 240.29ms\n","iter 154: loss 3.3929, time: 242.09ms\n","iter 155: loss 2.5524, time: 240.95ms\n","iter 156: loss 3.8793, time: 240.52ms\n","iter 157: loss 8.1572, time: 241.53ms\n","iter 158: loss 1.9914, time: 241.21ms\n","iter 159: loss 3.8224, time: 244.27ms\n","iter 160: loss 3.3966, time: 240.98ms\n","iter 161: loss 2.8529, time: 240.56ms\n","iter 162: loss 5.5239, time: 241.38ms\n","iter 163: loss 3.0244, time: 241.99ms\n","iter 164: loss 4.7315, time: 240.79ms\n","iter 165: loss nan, time: 239.76ms\n","iter 166: loss 5.1679, time: 241.38ms\n","iter 167: loss 3.9476, time: 241.84ms\n","iter 168: loss 3.4803, time: 244.88ms\n","iter 169: loss 4.5485, time: 243.02ms\n","iter 170: loss 3.2841, time: 242.94ms\n","iter 171: loss 4.1860, time: 243.95ms\n","iter 172: loss 5.3286, time: 242.21ms\n","iter 173: loss 3.5674, time: 240.71ms\n","iter 174: loss 2.8898, time: 241.46ms\n","iter 175: loss nan, time: 241.33ms\n","iter 176: loss 4.9337, time: 242.15ms\n","iter 177: loss 3.7560, time: 242.57ms\n","iter 178: loss 4.2182, time: 243.96ms\n","iter 179: loss 3.8343, time: 244.33ms\n","iter 180: loss 4.3075, time: 244.05ms\n","iter 181: loss 5.2783, time: 243.78ms\n","iter 182: loss 3.7497, time: 244.59ms\n","iter 183: loss 4.1439, time: 243.58ms\n","iter 184: loss 4.4153, time: 242.46ms\n","iter 185: loss 3.5052, time: 243.96ms\n","iter 186: loss nan, time: 242.80ms\n","iter 187: loss 3.5851, time: 241.57ms\n","iter 188: loss 3.9663, time: 242.76ms\n","iter 189: loss 5.1398, time: 242.61ms\n","iter 190: loss 3.9402, time: 242.21ms\n","iter 191: loss 4.2595, time: 245.66ms\n","iter 192: loss 3.2408, time: 241.98ms\n","iter 193: loss 4.1439, time: 242.80ms\n","iter 194: loss 4.4089, time: 240.95ms\n","iter 195: loss 2.4521, time: 242.46ms\n","iter 196: loss 4.7252, time: 242.74ms\n","iter 197: loss 3.9194, time: 241.53ms\n","iter 198: loss 3.5846, time: 242.08ms\n","iter 199: loss 3.9309, time: 243.16ms\n","iter 200: loss 2.9842, time: 243.09ms\n","iter 201: loss 4.3436, time: 242.29ms\n","iter 202: loss 2.9465, time: 241.21ms\n","iter 203: loss 4.2783, time: 242.79ms\n","iter 204: loss 5.5737, time: 242.26ms\n","iter 205: loss nan, time: 241.84ms\n","iter 206: loss 7.5013, time: 240.48ms\n","iter 207: loss 3.4333, time: 240.56ms\n","iter 208: loss 3.1639, time: 242.05ms\n","iter 209: loss 4.2122, time: 242.42ms\n","iter 210: loss 4.7569, time: 241.64ms\n","iter 211: loss 2.9742, time: 241.64ms\n","iter 212: loss 3.8839, time: 241.23ms\n","iter 213: loss 3.8785, time: 242.75ms\n","iter 214: loss 3.0680, time: 240.65ms\n","iter 215: loss 4.9145, time: 240.12ms\n","iter 216: loss 4.8448, time: 240.45ms\n","iter 217: loss 4.5695, time: 241.85ms\n","iter 218: loss 7.6093, time: 241.58ms\n","iter 219: loss 2.4848, time: 240.29ms\n","iter 220: loss 4.3281, time: 242.17ms\n","iter 221: loss 3.3985, time: 240.91ms\n","iter 222: loss 3.0240, time: 241.42ms\n","iter 223: loss 3.1214, time: 243.15ms\n","iter 224: loss 7.6031, time: 240.94ms\n","iter 225: loss 3.8529, time: 243.33ms\n","iter 226: loss 3.2928, time: 242.19ms\n","iter 227: loss 5.1137, time: 241.89ms\n","iter 228: loss 3.5962, time: 241.78ms\n","iter 229: loss 2.8016, time: 243.34ms\n","iter 230: loss nan, time: 243.15ms\n","iter 231: loss 3.5326, time: 246.98ms\n","iter 232: loss 2.9783, time: 246.22ms\n","iter 233: loss 3.6667, time: 242.30ms\n","iter 234: loss 4.5387, time: 242.44ms\n","iter 235: loss 6.0321, time: 243.24ms\n","iter 236: loss 4.9443, time: 241.70ms\n","iter 237: loss 3.4728, time: 243.27ms\n","iter 238: loss 3.8369, time: 243.82ms\n","iter 239: loss 3.4886, time: 242.72ms\n","iter 240: loss 3.9111, time: 242.23ms\n","iter 241: loss 2.9093, time: 241.55ms\n","iter 242: loss nan, time: 242.51ms\n","iter 243: loss 5.5670, time: 241.54ms\n","iter 244: loss 4.6528, time: 242.26ms\n","iter 245: loss 5.2813, time: 241.72ms\n","iter 246: loss 3.1747, time: 243.05ms\n","iter 247: loss 3.4401, time: 241.11ms\n","iter 248: loss nan, time: 242.94ms\n","iter 249: loss 3.6804, time: 242.80ms\n","iter 250: loss nan, time: 242.71ms\n","iter 251: loss 3.4815, time: 199.37ms\n","iter 252: loss 5.4225, time: 242.24ms\n","iter 253: loss 3.8111, time: 243.14ms\n","iter 254: loss 3.9654, time: 243.51ms\n","iter 255: loss 4.1126, time: 245.67ms\n","iter 256: loss 4.6870, time: 240.64ms\n","iter 257: loss 3.9424, time: 241.90ms\n","iter 258: loss 3.9758, time: 242.49ms\n","iter 259: loss 4.2516, time: 242.11ms\n","iter 260: loss 2.6773, time: 240.90ms\n","iter 261: loss 3.0228, time: 242.08ms\n","iter 262: loss nan, time: 242.89ms\n","iter 263: loss 6.0431, time: 242.09ms\n","iter 264: loss 4.6411, time: 241.73ms\n","iter 265: loss 9.2735, time: 242.01ms\n","iter 266: loss 3.7607, time: 241.33ms\n","iter 267: loss 3.6667, time: 242.38ms\n","iter 268: loss 5.0428, time: 240.34ms\n","iter 269: loss 4.0804, time: 241.53ms\n","iter 270: loss 4.4671, time: 243.81ms\n","iter 271: loss 4.7835, time: 242.26ms\n","iter 272: loss 5.1793, time: 241.18ms\n","iter 273: loss 3.2385, time: 241.56ms\n","iter 274: loss 3.5486, time: 241.72ms\n","iter 275: loss 4.1107, time: 242.24ms\n","iter 276: loss 2.7041, time: 241.38ms\n","iter 277: loss 3.6282, time: 241.46ms\n","iter 278: loss 5.5283, time: 241.05ms\n","iter 279: loss 4.3347, time: 241.36ms\n","iter 280: loss 4.5451, time: 241.63ms\n","iter 281: loss 3.2407, time: 241.64ms\n","iter 282: loss 4.0298, time: 243.44ms\n","iter 283: loss 2.7139, time: 242.13ms\n","iter 284: loss 4.5926, time: 241.91ms\n","iter 285: loss 3.3263, time: 242.29ms\n","iter 286: loss 3.6316, time: 241.91ms\n","iter 287: loss 6.6463, time: 245.03ms\n","iter 288: loss 5.0463, time: 241.64ms\n","iter 289: loss 3.7312, time: 241.88ms\n","iter 290: loss 3.6458, time: 243.84ms\n","iter 291: loss 4.5336, time: 247.43ms\n","iter 292: loss 4.1479, time: 242.68ms\n","iter 293: loss nan, time: 243.22ms\n","iter 294: loss 4.9027, time: 243.27ms\n","iter 295: loss 2.1923, time: 242.83ms\n","iter 296: loss nan, time: 244.33ms\n","iter 297: loss 2.2916, time: 242.25ms\n","iter 298: loss 3.4806, time: 241.52ms\n","iter 299: loss 3.6143, time: 242.26ms\n","iter 300: loss nan, time: 242.46ms\n","iter 301: loss 6.7660, time: 242.75ms\n","iter 302: loss 4.5982, time: 242.50ms\n","iter 303: loss 4.6394, time: 242.95ms\n","iter 304: loss 4.2135, time: 243.23ms\n","iter 305: loss 2.7867, time: 241.66ms\n","iter 306: loss 3.1019, time: 242.86ms\n","iter 307: loss 2.8089, time: 241.00ms\n","iter 308: loss 8.0978, time: 242.27ms\n","iter 309: loss 5.6107, time: 241.09ms\n","iter 310: loss 3.9836, time: 241.39ms\n","iter 311: loss 6.7056, time: 241.00ms\n","iter 312: loss nan, time: 241.47ms\n","iter 313: loss 4.0470, time: 241.84ms\n","iter 314: loss 3.9343, time: 241.08ms\n","iter 315: loss 3.6924, time: 241.93ms\n","iter 316: loss 4.0191, time: 242.91ms\n","iter 317: loss 5.0460, time: 242.91ms\n","iter 318: loss 3.7787, time: 241.04ms\n","iter 319: loss 3.8340, time: 243.32ms\n","iter 320: loss 2.1981, time: 241.22ms\n","iter 321: loss 4.1021, time: 242.25ms\n","iter 322: loss 6.5734, time: 241.55ms\n","iter 323: loss 3.6860, time: 242.70ms\n","iter 324: loss 4.4307, time: 241.61ms\n","iter 325: loss 4.7130, time: 255.99ms\n","iter 326: loss nan, time: 241.58ms\n","iter 327: loss 3.6653, time: 242.86ms\n","iter 328: loss 4.2901, time: 242.41ms\n","iter 329: loss 4.1333, time: 242.56ms\n","iter 330: loss 4.0176, time: 241.56ms\n","iter 331: loss 3.7503, time: 242.57ms\n","iter 332: loss 3.2215, time: 242.90ms\n","iter 333: loss nan, time: 241.99ms\n","iter 334: loss 4.4790, time: 242.03ms\n","iter 335: loss 3.0704, time: 242.31ms\n","iter 336: loss 3.9103, time: 241.96ms\n","iter 337: loss 3.3637, time: 242.04ms\n","iter 338: loss 3.4841, time: 244.81ms\n","iter 339: loss 3.0905, time: 242.36ms\n","iter 340: loss 3.2313, time: 241.78ms\n","iter 341: loss 3.6554, time: 243.16ms\n","iter 342: loss nan, time: 242.26ms\n","iter 343: loss 3.8197, time: 242.95ms\n","iter 344: loss 3.6700, time: 242.81ms\n","iter 345: loss 2.6737, time: 242.18ms\n","iter 346: loss 6.1026, time: 241.90ms\n","iter 347: loss 6.0284, time: 242.72ms\n","iter 348: loss 2.7950, time: 242.05ms\n","iter 349: loss 5.2506, time: 242.28ms\n","iter 350: loss 4.5993, time: 265.68ms\n","iter 351: loss 2.8880, time: 244.96ms\n","iter 352: loss 3.8938, time: 242.19ms\n","iter 353: loss 2.7257, time: 246.68ms\n","iter 354: loss 4.4777, time: 242.10ms\n","iter 355: loss 3.7434, time: 243.70ms\n","iter 356: loss 3.4312, time: 245.98ms\n","iter 357: loss 6.0725, time: 243.30ms\n","iter 358: loss nan, time: 243.87ms\n","iter 359: loss 2.9065, time: 252.39ms\n","iter 360: loss 2.8794, time: 248.75ms\n","iter 361: loss 4.1887, time: 243.68ms\n","iter 362: loss 2.9203, time: 247.46ms\n","iter 363: loss 4.4439, time: 248.00ms\n","iter 364: loss 4.1190, time: 243.21ms\n","iter 365: loss 3.2821, time: 242.19ms\n","iter 366: loss 4.1832, time: 243.01ms\n","iter 367: loss 4.7282, time: 241.38ms\n","iter 368: loss 3.5669, time: 241.32ms\n","iter 369: loss nan, time: 242.55ms\n","iter 370: loss 3.4794, time: 241.62ms\n","iter 371: loss 3.9423, time: 243.22ms\n","iter 372: loss nan, time: 242.53ms\n","iter 373: loss 3.6860, time: 241.48ms\n","iter 374: loss 3.5098, time: 242.65ms\n","iter 375: loss 5.2490, time: 241.85ms\n","iter 376: loss nan, time: 242.01ms\n","iter 377: loss 3.4802, time: 241.51ms\n","iter 378: loss 6.9610, time: 241.68ms\n","iter 379: loss 3.2428, time: 241.67ms\n","iter 380: loss 4.5493, time: 253.19ms\n","iter 381: loss 4.6260, time: 241.51ms\n","iter 382: loss 2.8709, time: 242.56ms\n","iter 383: loss 4.2475, time: 244.33ms\n","iter 384: loss 3.2994, time: 240.97ms\n","iter 385: loss 2.5971, time: 242.23ms\n","iter 386: loss 4.6969, time: 242.29ms\n","iter 387: loss 2.9445, time: 242.37ms\n","iter 388: loss 3.4166, time: 242.00ms\n","iter 389: loss 4.3519, time: 241.52ms\n","iter 390: loss 3.5152, time: 241.75ms\n","iter 391: loss 4.6799, time: 242.09ms\n","iter 392: loss 3.3537, time: 245.66ms\n","iter 393: loss 1.9640, time: 242.33ms\n","iter 394: loss 3.1151, time: 242.27ms\n","iter 395: loss 4.6632, time: 242.17ms\n","iter 396: loss 3.0609, time: 241.50ms\n","iter 397: loss 2.1487, time: 243.04ms\n","iter 398: loss 7.1932, time: 242.59ms\n","iter 399: loss 4.0025, time: 242.05ms\n","iter 400: loss 5.5795, time: 242.13ms\n","iter 401: loss 4.7208, time: 241.49ms\n","iter 402: loss 4.3675, time: 241.89ms\n","iter 403: loss 4.3911, time: 243.73ms\n","iter 404: loss 6.3571, time: 242.96ms\n","iter 405: loss 3.9822, time: 243.91ms\n","iter 406: loss 3.3077, time: 242.63ms\n","iter 407: loss 2.9884, time: 243.21ms\n","iter 408: loss 4.0115, time: 242.35ms\n","iter 409: loss 4.1335, time: 243.09ms\n","iter 410: loss 3.2082, time: 242.18ms\n","iter 411: loss 3.1891, time: 243.16ms\n","iter 412: loss 2.7774, time: 241.15ms\n","iter 413: loss 3.1255, time: 243.04ms\n","iter 414: loss 3.0400, time: 250.92ms\n","iter 415: loss 4.7159, time: 246.14ms\n","iter 416: loss 3.6219, time: 242.42ms\n","iter 417: loss nan, time: 242.63ms\n","iter 418: loss nan, time: 243.04ms\n","iter 419: loss nan, time: 243.68ms\n","iter 420: loss 4.3509, time: 243.51ms\n","iter 421: loss 2.9172, time: 243.77ms\n","iter 422: loss 3.6917, time: 243.29ms\n","iter 423: loss nan, time: 244.58ms\n","iter 424: loss nan, time: 244.89ms\n","iter 425: loss 3.2714, time: 242.02ms\n","iter 426: loss 3.7641, time: 241.59ms\n","iter 427: loss 4.1914, time: 242.12ms\n","iter 428: loss 2.8877, time: 240.97ms\n","iter 429: loss 4.1106, time: 241.35ms\n","iter 430: loss 3.8145, time: 241.73ms\n","iter 431: loss 2.7061, time: 240.85ms\n","iter 432: loss 3.2125, time: 241.23ms\n","iter 433: loss 3.6463, time: 241.01ms\n","iter 434: loss 4.0255, time: 241.63ms\n","iter 435: loss 3.5609, time: 243.00ms\n","iter 436: loss 3.7119, time: 241.63ms\n","iter 437: loss 4.1532, time: 241.18ms\n","iter 438: loss 5.0192, time: 242.81ms\n","iter 439: loss 4.7856, time: 241.22ms\n","iter 440: loss 3.4045, time: 241.55ms\n","iter 441: loss 4.6997, time: 240.71ms\n","iter 442: loss 3.7729, time: 242.48ms\n","iter 443: loss 5.5632, time: 242.45ms\n","iter 444: loss 3.7455, time: 243.53ms\n","iter 445: loss 3.0406, time: 247.51ms\n","iter 446: loss 4.4918, time: 253.48ms\n","iter 447: loss 3.2113, time: 246.25ms\n","iter 448: loss 3.9449, time: 240.47ms\n","iter 449: loss 4.5687, time: 241.69ms\n","iter 450: loss 4.2684, time: 241.04ms\n","iter 451: loss 4.3107, time: 242.12ms\n","iter 452: loss 5.3704, time: 242.62ms\n","iter 453: loss 3.7428, time: 241.85ms\n","iter 454: loss 2.8850, time: 246.39ms\n","iter 455: loss 3.8204, time: 242.02ms\n","iter 456: loss 5.1274, time: 243.56ms\n","iter 457: loss 3.7330, time: 242.97ms\n","iter 458: loss 3.0120, time: 241.31ms\n","iter 459: loss 4.5020, time: 244.51ms\n","iter 460: loss 2.9482, time: 244.57ms\n","iter 461: loss 4.1624, time: 251.09ms\n","iter 462: loss 1.8801, time: 243.64ms\n","iter 463: loss 12.6866, time: 243.75ms\n","iter 464: loss 5.6064, time: 243.30ms\n","iter 465: loss 2.8792, time: 242.85ms\n","iter 466: loss 4.0005, time: 242.48ms\n","iter 467: loss 3.3420, time: 242.09ms\n","iter 468: loss 4.7203, time: 242.01ms\n","iter 469: loss 2.7124, time: 240.65ms\n","iter 470: loss 4.1971, time: 240.95ms\n","iter 471: loss 4.4171, time: 242.21ms\n","iter 472: loss 5.3609, time: 240.94ms\n","iter 473: loss 6.7576, time: 241.89ms\n","iter 474: loss 2.9900, time: 240.92ms\n","iter 475: loss 3.9120, time: 240.65ms\n","iter 476: loss 4.1953, time: 241.21ms\n","iter 477: loss 2.2667, time: 241.92ms\n","iter 478: loss 3.8276, time: 241.44ms\n","iter 479: loss nan, time: 243.18ms\n","iter 480: loss 3.9562, time: 240.87ms\n","iter 481: loss 4.0122, time: 242.79ms\n","iter 482: loss 3.0989, time: 241.05ms\n","iter 483: loss 3.7520, time: 241.03ms\n","iter 484: loss 4.3188, time: 240.67ms\n","iter 485: loss nan, time: 242.03ms\n","iter 486: loss 3.2985, time: 241.09ms\n","iter 487: loss 4.7474, time: 240.99ms\n","iter 488: loss 3.8386, time: 242.41ms\n","iter 489: loss 4.6738, time: 242.10ms\n","iter 490: loss 3.8407, time: 241.92ms\n","iter 491: loss 3.4364, time: 241.47ms\n","iter 492: loss 3.2343, time: 241.35ms\n","iter 493: loss 1.5994, time: 242.08ms\n","iter 494: loss 3.1623, time: 241.24ms\n","iter 495: loss 3.5393, time: 241.21ms\n","iter 496: loss 10.9411, time: 241.46ms\n","iter 497: loss 5.1118, time: 244.11ms\n","iter 498: loss 4.6317, time: 244.76ms\n","iter 499: loss 3.4196, time: 243.66ms\n","iter 500: loss 3.9063, time: 242.07ms\n","iter 501: loss 3.1072, time: 243.03ms\n","iter 502: loss 3.0131, time: 241.91ms\n","iter 503: loss 1.8264, time: 240.84ms\n","iter 504: loss 3.9803, time: 242.60ms\n","iter 505: loss 4.3718, time: 242.42ms\n","iter 506: loss nan, time: 241.67ms\n","iter 507: loss 4.0849, time: 242.76ms\n","iter 508: loss 4.1900, time: 244.02ms\n","iter 509: loss 4.5002, time: 242.15ms\n","iter 510: loss 3.0916, time: 243.37ms\n","iter 511: loss 4.3847, time: 244.42ms\n","iter 512: loss 4.0912, time: 241.33ms\n","iter 513: loss 3.0571, time: 242.64ms\n","iter 514: loss 3.5046, time: 242.45ms\n","iter 515: loss 2.7390, time: 242.02ms\n","iter 516: loss 3.2287, time: 242.47ms\n","iter 517: loss 4.5312, time: 243.43ms\n","iter 518: loss 2.9102, time: 243.09ms\n","iter 519: loss 4.3086, time: 248.60ms\n","iter 520: loss 3.3369, time: 248.88ms\n","iter 521: loss 3.5426, time: 243.81ms\n","iter 522: loss 4.0469, time: 248.27ms\n","iter 523: loss 3.8091, time: 240.84ms\n","iter 524: loss 3.8773, time: 240.62ms\n","iter 525: loss nan, time: 239.70ms\n","iter 526: loss 2.3077, time: 241.40ms\n","iter 527: loss 2.8634, time: 240.97ms\n","iter 528: loss 2.5519, time: 241.32ms\n","iter 529: loss 2.5583, time: 241.29ms\n","iter 530: loss 4.6547, time: 241.10ms\n","iter 531: loss 4.2558, time: 241.90ms\n","iter 532: loss 3.6847, time: 221.99ms\n","iter 533: loss 3.7181, time: 240.72ms\n","iter 534: loss 3.5313, time: 243.84ms\n","iter 535: loss nan, time: 242.90ms\n","iter 536: loss 4.5455, time: 245.54ms\n","iter 537: loss nan, time: 243.15ms\n","iter 538: loss 2.8585, time: 245.12ms\n","iter 539: loss 4.8643, time: 243.23ms\n","iter 540: loss 5.4128, time: 245.21ms\n","iter 541: loss 3.6904, time: 245.65ms\n","iter 542: loss 4.8804, time: 242.65ms\n","iter 543: loss 6.2552, time: 244.87ms\n","iter 544: loss 3.8282, time: 242.40ms\n","iter 545: loss 4.4555, time: 241.18ms\n","iter 546: loss 4.4143, time: 241.08ms\n","iter 547: loss 3.4719, time: 242.80ms\n","iter 548: loss nan, time: 243.09ms\n","iter 549: loss 3.5687, time: 243.40ms\n","iter 550: loss 5.0421, time: 242.32ms\n","iter 551: loss 4.5124, time: 242.53ms\n","iter 552: loss nan, time: 241.31ms\n","iter 553: loss 3.5245, time: 241.83ms\n","iter 554: loss 3.0799, time: 241.86ms\n","iter 555: loss 2.6798, time: 241.26ms\n","iter 556: loss 3.7099, time: 240.49ms\n","iter 557: loss 3.2251, time: 241.25ms\n","iter 558: loss 3.3239, time: 241.88ms\n","iter 559: loss 2.4741, time: 242.11ms\n","iter 560: loss 2.4145, time: 242.01ms\n","iter 561: loss 3.3950, time: 241.23ms\n","iter 562: loss 3.4483, time: 241.52ms\n","iter 563: loss nan, time: 241.43ms\n","iter 564: loss 4.3249, time: 244.24ms\n","iter 565: loss 4.3812, time: 240.88ms\n","iter 566: loss 3.2482, time: 241.47ms\n","iter 567: loss 4.7149, time: 241.65ms\n","iter 568: loss 2.6834, time: 241.13ms\n","iter 569: loss nan, time: 241.68ms\n","iter 570: loss nan, time: 242.88ms\n","iter 571: loss 4.6543, time: 243.10ms\n","iter 572: loss 3.9339, time: 244.07ms\n","iter 573: loss 4.2390, time: 244.42ms\n","iter 574: loss nan, time: 243.35ms\n","iter 575: loss 3.7187, time: 248.15ms\n","iter 576: loss 3.8954, time: 241.90ms\n","iter 577: loss 4.4796, time: 244.23ms\n","iter 578: loss nan, time: 243.66ms\n","iter 579: loss 3.6265, time: 242.24ms\n","iter 580: loss nan, time: 242.21ms\n","iter 581: loss 4.9319, time: 240.65ms\n","iter 582: loss 4.5352, time: 241.58ms\n","iter 583: loss 4.5752, time: 241.36ms\n","iter 584: loss 4.3496, time: 241.35ms\n","iter 585: loss 3.8785, time: 241.11ms\n","iter 586: loss nan, time: 240.94ms\n","iter 587: loss nan, time: 240.71ms\n","iter 588: loss 4.2003, time: 240.95ms\n","iter 589: loss 4.3132, time: 241.10ms\n","iter 590: loss 3.3384, time: 241.09ms\n","iter 591: loss 4.2044, time: 242.20ms\n","iter 592: loss 4.1901, time: 241.87ms\n","iter 593: loss nan, time: 241.32ms\n","iter 594: loss 3.6799, time: 240.75ms\n","iter 595: loss 2.8872, time: 241.68ms\n","iter 596: loss 5.1927, time: 240.86ms\n","iter 597: loss 4.3974, time: 241.39ms\n","iter 598: loss 4.5395, time: 240.95ms\n","iter 599: loss 4.0693, time: 240.22ms\n","iter 600: loss 3.2848, time: 241.15ms\n","iter 601: loss 2.7080, time: 241.00ms\n","iter 602: loss 4.1945, time: 241.86ms\n","iter 603: loss nan, time: 241.45ms\n","iter 604: loss 4.3375, time: 241.27ms\n","iter 605: loss nan, time: 244.18ms\n","iter 606: loss 3.2399, time: 242.13ms\n","iter 607: loss nan, time: 242.87ms\n","iter 608: loss 4.1970, time: 239.55ms\n","iter 609: loss 4.1505, time: 241.20ms\n","iter 610: loss 2.7869, time: 240.02ms\n","iter 611: loss nan, time: 240.73ms\n","iter 612: loss 3.6027, time: 240.75ms\n","iter 613: loss 3.1685, time: 240.90ms\n","iter 614: loss 3.7752, time: 240.28ms\n","iter 615: loss 3.3598, time: 241.55ms\n","iter 616: loss 3.5107, time: 241.22ms\n","iter 617: loss 3.1094, time: 241.85ms\n","iter 618: loss nan, time: 241.31ms\n","iter 619: loss 3.2624, time: 240.38ms\n","iter 620: loss 3.4048, time: 240.60ms\n","iter 621: loss nan, time: 242.79ms\n","iter 622: loss 3.5671, time: 241.99ms\n","iter 623: loss 3.4687, time: 242.02ms\n","iter 624: loss 2.1392, time: 247.29ms\n","iter 625: loss 4.6324, time: 242.22ms\n","iter 626: loss 3.5765, time: 242.10ms\n","iter 627: loss 3.9877, time: 241.87ms\n","iter 628: loss 3.4093, time: 241.81ms\n","iter 629: loss 4.0288, time: 241.86ms\n","iter 630: loss 3.4998, time: 242.20ms\n","iter 631: loss 3.9717, time: 243.33ms\n","iter 632: loss 3.3377, time: 224.41ms\n","iter 633: loss 3.4563, time: 242.77ms\n","iter 634: loss 3.9221, time: 191.72ms\n","iter 635: loss nan, time: 242.24ms\n","iter 636: loss 4.3441, time: 242.64ms\n","iter 637: loss 4.5465, time: 240.83ms\n","iter 638: loss nan, time: 241.81ms\n","iter 639: loss 3.5711, time: 244.23ms\n","iter 640: loss 3.2532, time: 241.27ms\n","iter 641: loss nan, time: 240.88ms\n","iter 642: loss 6.9143, time: 241.19ms\n","iter 643: loss 2.5591, time: 241.01ms\n","iter 644: loss 2.3361, time: 241.47ms\n","iter 645: loss 3.3237, time: 241.84ms\n","iter 646: loss 3.6078, time: 242.02ms\n","iter 647: loss nan, time: 241.22ms\n","iter 648: loss nan, time: 241.49ms\n","iter 649: loss nan, time: 241.28ms\n","iter 650: loss 3.4503, time: 241.32ms\n","iter 651: loss 2.6481, time: 241.96ms\n","iter 652: loss 3.1129, time: 241.56ms\n","iter 653: loss nan, time: 241.67ms\n","iter 654: loss 11.5110, time: 241.42ms\n","iter 655: loss 3.2270, time: 242.32ms\n","iter 656: loss 4.0089, time: 240.59ms\n","iter 657: loss 3.2123, time: 241.52ms\n","iter 658: loss 2.9767, time: 242.62ms\n","iter 659: loss 4.7810, time: 241.45ms\n","iter 660: loss nan, time: 241.16ms\n","iter 661: loss 5.1201, time: 242.73ms\n","iter 662: loss nan, time: 241.28ms\n","iter 663: loss 3.2673, time: 242.12ms\n","iter 664: loss 3.2268, time: 240.91ms\n","iter 665: loss 2.7886, time: 241.83ms\n","iter 666: loss 4.0527, time: 240.65ms\n","iter 667: loss 2.8943, time: 241.76ms\n","iter 668: loss 4.4507, time: 242.91ms\n","iter 669: loss 3.1453, time: 243.07ms\n","iter 670: loss 2.9332, time: 240.83ms\n","iter 671: loss 3.6625, time: 246.57ms\n","iter 672: loss 3.7022, time: 240.82ms\n","iter 673: loss 2.8030, time: 242.19ms\n","iter 674: loss nan, time: 246.41ms\n","iter 675: loss 3.7681, time: 242.03ms\n","iter 676: loss 2.6144, time: 242.06ms\n","iter 677: loss 2.6980, time: 245.32ms\n","iter 678: loss 2.4324, time: 245.25ms\n","iter 679: loss 2.7869, time: 249.76ms\n","iter 680: loss nan, time: 245.94ms\n","iter 681: loss 3.7258, time: 244.59ms\n","iter 682: loss 4.2402, time: 245.69ms\n","iter 683: loss nan, time: 244.39ms\n","iter 684: loss 3.1157, time: 241.78ms\n","iter 685: loss 8.8326, time: 241.89ms\n","iter 686: loss 4.4246, time: 244.47ms\n","iter 687: loss 3.3259, time: 242.56ms\n","iter 688: loss 3.9821, time: 249.42ms\n","iter 689: loss 3.0064, time: 244.77ms\n","iter 690: loss 3.1417, time: 242.62ms\n","iter 691: loss 3.1531, time: 243.76ms\n","iter 692: loss nan, time: 243.59ms\n","iter 693: loss 2.1555, time: 241.23ms\n","iter 694: loss 2.7780, time: 243.16ms\n","iter 695: loss 3.4667, time: 243.73ms\n","iter 696: loss 4.0355, time: 227.94ms\n","iter 697: loss 2.8303, time: 242.27ms\n","iter 698: loss 3.1971, time: 242.09ms\n","iter 699: loss 4.1367, time: 242.22ms\n","iter 700: loss 4.0379, time: 240.93ms\n","iter 701: loss 3.0283, time: 240.51ms\n","iter 702: loss nan, time: 241.97ms\n","iter 703: loss 4.1765, time: 244.27ms\n","iter 704: loss 4.2519, time: 241.60ms\n","iter 705: loss 2.4757, time: 240.29ms\n","iter 706: loss 3.9761, time: 240.94ms\n","iter 707: loss 2.0626, time: 240.50ms\n","iter 708: loss 4.5202, time: 240.73ms\n","iter 709: loss 4.9051, time: 240.15ms\n","iter 710: loss 3.4359, time: 241.04ms\n","iter 711: loss 3.9240, time: 240.61ms\n","iter 712: loss 2.8530, time: 253.12ms\n","iter 713: loss 2.9097, time: 240.91ms\n","iter 714: loss 2.9170, time: 240.13ms\n","iter 715: loss 4.4670, time: 242.03ms\n","iter 716: loss 2.4754, time: 241.07ms\n","iter 717: loss 3.6930, time: 240.58ms\n","iter 718: loss 3.2269, time: 239.82ms\n","iter 719: loss 8.7831, time: 240.86ms\n","iter 720: loss nan, time: 240.38ms\n","iter 721: loss 3.6143, time: 241.17ms\n","iter 722: loss 2.9266, time: 239.76ms\n","iter 723: loss nan, time: 241.28ms\n","iter 724: loss 4.2147, time: 241.16ms\n","iter 725: loss nan, time: 241.53ms\n","iter 726: loss 2.9488, time: 240.95ms\n","iter 727: loss 2.8067, time: 240.86ms\n","iter 728: loss 4.4667, time: 241.04ms\n","iter 729: loss nan, time: 243.05ms\n","iter 730: loss 2.7350, time: 241.05ms\n","iter 731: loss 2.6260, time: 241.12ms\n","iter 732: loss 4.3017, time: 242.33ms\n","iter 733: loss nan, time: 241.95ms\n","iter 734: loss 2.3591, time: 242.78ms\n","iter 735: loss 3.8210, time: 246.05ms\n","iter 736: loss 3.7185, time: 242.48ms\n","iter 737: loss 3.9811, time: 242.71ms\n","iter 738: loss nan, time: 240.69ms\n","iter 739: loss 3.7758, time: 241.54ms\n","iter 740: loss 2.7710, time: 243.84ms\n","iter 741: loss 2.8697, time: 241.55ms\n","iter 742: loss 3.8854, time: 243.01ms\n","iter 743: loss 4.1079, time: 244.21ms\n","iter 744: loss 9.2758, time: 242.63ms\n","iter 745: loss 2.3958, time: 248.61ms\n","iter 746: loss 2.7213, time: 242.06ms\n","iter 747: loss 4.4256, time: 242.30ms\n","iter 748: loss 5.4335, time: 242.29ms\n","iter 749: loss 2.3696, time: 243.04ms\n","iter 750: loss 3.3001, time: 241.34ms\n","iter 751: loss 3.4020, time: 256.47ms\n","iter 752: loss 4.4210, time: 240.38ms\n","iter 753: loss 2.8824, time: 241.15ms\n","iter 754: loss 3.3671, time: 241.02ms\n","iter 755: loss nan, time: 241.34ms\n","iter 756: loss 2.6972, time: 241.89ms\n","iter 757: loss 3.0350, time: 242.01ms\n","iter 758: loss 5.0153, time: 241.31ms\n","iter 759: loss 3.8443, time: 240.11ms\n","iter 760: loss 3.5713, time: 241.40ms\n","iter 761: loss 2.6185, time: 241.37ms\n","iter 762: loss 4.6546, time: 241.20ms\n","iter 763: loss 4.0150, time: 240.24ms\n","iter 764: loss 5.7028, time: 242.01ms\n","iter 765: loss 2.9601, time: 240.58ms\n","iter 766: loss nan, time: 241.46ms\n","iter 767: loss 2.6926, time: 243.05ms\n","iter 768: loss 3.3568, time: 240.00ms\n","iter 769: loss nan, time: 240.44ms\n","iter 770: loss nan, time: 242.65ms\n","iter 771: loss 3.6532, time: 240.37ms\n","iter 772: loss 2.6270, time: 242.01ms\n","iter 773: loss 4.5688, time: 242.23ms\n","iter 774: loss 3.8776, time: 242.13ms\n","iter 775: loss 3.1212, time: 242.46ms\n","iter 776: loss nan, time: 240.87ms\n","iter 777: loss 4.1936, time: 241.25ms\n","iter 778: loss 5.1963, time: 242.33ms\n","iter 779: loss 3.5099, time: 241.18ms\n","iter 780: loss 3.0150, time: 240.47ms\n","iter 781: loss 3.1216, time: 242.26ms\n","iter 782: loss 9.0681, time: 242.88ms\n","iter 783: loss 2.6659, time: 241.37ms\n","iter 784: loss 2.4996, time: 241.42ms\n","iter 785: loss nan, time: 241.41ms\n","iter 786: loss 5.1489, time: 241.71ms\n","iter 787: loss 3.6236, time: 241.76ms\n","iter 788: loss 2.7289, time: 242.19ms\n","iter 789: loss 3.7889, time: 242.12ms\n","iter 790: loss 4.5197, time: 242.42ms\n","iter 791: loss 2.1383, time: 243.68ms\n","iter 792: loss 3.0330, time: 241.62ms\n","iter 793: loss 2.7102, time: 241.52ms\n","iter 794: loss 3.2991, time: 240.87ms\n","iter 795: loss 3.9364, time: 242.62ms\n","iter 796: loss 3.4604, time: 242.82ms\n","iter 797: loss 4.6748, time: 243.82ms\n","iter 798: loss nan, time: 242.42ms\n","iter 799: loss 2.8264, time: 246.63ms\n","iter 800: loss 2.4917, time: 242.39ms\n","iter 801: loss 2.3830, time: 242.79ms\n","iter 802: loss 3.5671, time: 243.30ms\n","iter 803: loss 3.6166, time: 242.49ms\n","iter 804: loss 3.2135, time: 243.19ms\n","iter 805: loss 3.7409, time: 243.37ms\n","iter 806: loss 3.6017, time: 242.84ms\n","iter 807: loss 2.7006, time: 241.86ms\n","iter 808: loss nan, time: 241.56ms\n","iter 809: loss nan, time: 242.59ms\n","iter 810: loss 3.6215, time: 241.84ms\n","iter 811: loss 2.0736, time: 242.80ms\n","iter 812: loss 3.9141, time: 241.99ms\n","iter 813: loss 2.7968, time: 240.77ms\n","iter 814: loss 8.3566, time: 241.32ms\n","iter 815: loss 3.4428, time: 241.01ms\n","iter 816: loss 8.2742, time: 242.71ms\n","iter 817: loss nan, time: 241.13ms\n","iter 818: loss 5.1015, time: 241.65ms\n","iter 819: loss 2.7924, time: 241.58ms\n","iter 820: loss 3.7628, time: 242.24ms\n","iter 821: loss 4.0726, time: 241.85ms\n","iter 822: loss 2.8030, time: 241.05ms\n","iter 823: loss 3.3686, time: 241.23ms\n","iter 824: loss nan, time: 242.37ms\n","iter 825: loss 4.0376, time: 241.53ms\n","iter 826: loss 2.4701, time: 242.02ms\n","iter 827: loss 4.1494, time: 241.77ms\n","iter 828: loss 2.6627, time: 242.13ms\n","iter 829: loss 2.9802, time: 241.14ms\n","iter 830: loss 3.8752, time: 241.51ms\n","iter 831: loss 2.3259, time: 244.67ms\n","iter 832: loss 2.7034, time: 242.71ms\n","iter 833: loss 7.4404, time: 241.30ms\n","iter 834: loss 3.1604, time: 242.49ms\n","iter 835: loss 3.6292, time: 241.58ms\n","iter 836: loss 6.5754, time: 242.67ms\n","iter 837: loss nan, time: 241.61ms\n","iter 838: loss 2.9163, time: 242.89ms\n","iter 839: loss 3.0299, time: 242.23ms\n","iter 840: loss 3.3657, time: 242.26ms\n","iter 841: loss 3.8084, time: 243.09ms\n","iter 842: loss 2.4413, time: 241.93ms\n","iter 843: loss 2.5099, time: 241.61ms\n","iter 844: loss 2.8310, time: 242.58ms\n","iter 845: loss 3.6289, time: 242.94ms\n","iter 846: loss 3.5551, time: 241.57ms\n","iter 847: loss 3.4908, time: 242.10ms\n","iter 848: loss nan, time: 244.39ms\n","iter 849: loss 3.2666, time: 243.38ms\n","iter 850: loss 3.8017, time: 243.04ms\n","iter 851: loss 4.3343, time: 243.07ms\n","iter 852: loss 3.4611, time: 247.84ms\n","iter 853: loss 3.7169, time: 244.40ms\n","iter 854: loss 3.8140, time: 243.06ms\n","iter 855: loss nan, time: 242.91ms\n","iter 856: loss 3.3339, time: 242.84ms\n","iter 857: loss 2.2119, time: 243.72ms\n","iter 858: loss 3.4252, time: 243.17ms\n","iter 859: loss 3.3739, time: 243.86ms\n","iter 860: loss 2.7522, time: 242.42ms\n","iter 861: loss 2.0874, time: 242.85ms\n","iter 862: loss 2.7735, time: 242.28ms\n","iter 863: loss 1.4856, time: 243.41ms\n","iter 864: loss 2.5906, time: 240.32ms\n","iter 865: loss nan, time: 241.08ms\n","iter 866: loss 2.5998, time: 240.61ms\n","iter 867: loss 2.7734, time: 240.94ms\n","iter 868: loss 2.4969, time: 240.66ms\n","iter 869: loss nan, time: 241.42ms\n","iter 870: loss nan, time: 241.56ms\n","iter 871: loss 3.2337, time: 240.79ms\n","iter 872: loss 3.1745, time: 240.42ms\n","iter 873: loss 2.7192, time: 240.22ms\n","iter 874: loss 3.2540, time: 241.70ms\n","iter 875: loss 3.8500, time: 240.83ms\n","iter 876: loss nan, time: 242.10ms\n","iter 877: loss 2.7668, time: 241.19ms\n","iter 878: loss 3.7920, time: 241.99ms\n","iter 879: loss 3.0089, time: 240.79ms\n","iter 880: loss 3.3769, time: 241.46ms\n","iter 881: loss nan, time: 241.76ms\n","iter 882: loss 2.3745, time: 242.56ms\n","iter 883: loss 1.4759, time: 241.46ms\n","iter 884: loss 3.5256, time: 242.28ms\n","iter 885: loss 3.7926, time: 242.71ms\n","iter 886: loss 2.7208, time: 242.51ms\n","iter 887: loss 2.1047, time: 242.80ms\n","iter 888: loss 3.3635, time: 242.35ms\n","iter 889: loss 2.8122, time: 241.76ms\n","iter 890: loss 3.7007, time: 243.32ms\n","iter 891: loss 3.5241, time: 243.56ms\n","iter 892: loss 3.1404, time: 243.15ms\n","iter 893: loss 3.2767, time: 242.28ms\n","iter 894: loss 4.0134, time: 242.68ms\n","iter 895: loss 2.7551, time: 246.59ms\n","iter 896: loss 3.1619, time: 242.13ms\n","iter 897: loss 3.6259, time: 242.30ms\n","iter 898: loss 4.9575, time: 241.94ms\n","iter 899: loss 3.0829, time: 241.68ms\n","iter 900: loss 2.4147, time: 241.21ms\n","iter 901: loss 3.1257, time: 241.50ms\n","iter 902: loss nan, time: 241.49ms\n","iter 903: loss 3.8612, time: 240.76ms\n","iter 904: loss 2.2333, time: 244.84ms\n","iter 905: loss 3.3656, time: 242.91ms\n","iter 906: loss 2.4090, time: 243.16ms\n","iter 907: loss 3.0020, time: 241.79ms\n","iter 908: loss 2.7049, time: 241.09ms\n","iter 909: loss 3.4473, time: 242.15ms\n","iter 910: loss 4.0980, time: 241.68ms\n","iter 911: loss 4.1709, time: 241.32ms\n","iter 912: loss 3.2946, time: 242.29ms\n","iter 913: loss 3.2487, time: 241.95ms\n","iter 914: loss 2.6841, time: 241.24ms\n","iter 915: loss nan, time: 243.48ms\n","iter 916: loss 2.5570, time: 241.25ms\n","iter 917: loss 4.0049, time: 241.42ms\n","iter 918: loss 2.1648, time: 241.57ms\n","iter 919: loss 3.4139, time: 242.70ms\n","iter 920: loss 3.0931, time: 241.32ms\n","iter 921: loss nan, time: 244.24ms\n","iter 922: loss 3.2074, time: 242.57ms\n","iter 923: loss 2.9843, time: 242.43ms\n","iter 924: loss 3.5901, time: 240.19ms\n","iter 925: loss nan, time: 241.21ms\n","iter 926: loss 3.0784, time: 242.25ms\n","iter 927: loss nan, time: 243.26ms\n","iter 928: loss 2.0504, time: 240.07ms\n","iter 929: loss 2.2253, time: 240.56ms\n","iter 930: loss 2.0518, time: 241.32ms\n","iter 931: loss 3.4467, time: 241.46ms\n","iter 932: loss 1.6698, time: 240.54ms\n","iter 933: loss 2.2648, time: 241.24ms\n","iter 934: loss nan, time: 241.77ms\n","iter 935: loss nan, time: 241.77ms\n","iter 936: loss 2.7780, time: 241.62ms\n","iter 937: loss 2.7241, time: 240.93ms\n","iter 938: loss 3.6996, time: 240.99ms\n","iter 939: loss 3.2194, time: 241.89ms\n","iter 940: loss 2.5263, time: 241.46ms\n","iter 941: loss 2.6198, time: 241.50ms\n","iter 942: loss 2.4439, time: 243.56ms\n","iter 943: loss 2.7729, time: 241.22ms\n","iter 944: loss nan, time: 241.35ms\n","iter 945: loss 4.5414, time: 241.78ms\n","iter 946: loss 2.4186, time: 241.04ms\n","iter 947: loss nan, time: 240.81ms\n","iter 948: loss 2.4069, time: 242.76ms\n","iter 949: loss 3.0213, time: 240.91ms\n","iter 950: loss 3.1232, time: 242.77ms\n","iter 951: loss 3.2599, time: 242.35ms\n","iter 952: loss 2.9657, time: 241.16ms\n","iter 953: loss nan, time: 241.27ms\n","iter 954: loss nan, time: 241.40ms\n","iter 955: loss 1.9133, time: 241.59ms\n","iter 956: loss 3.3653, time: 241.29ms\n","iter 957: loss 3.3594, time: 240.45ms\n","iter 958: loss 3.1846, time: 242.72ms\n","iter 959: loss 2.1152, time: 243.19ms\n","iter 960: loss nan, time: 241.96ms\n","iter 961: loss 2.6168, time: 243.00ms\n","iter 962: loss 4.3422, time: 241.50ms\n","iter 963: loss 1.8356, time: 241.57ms\n","iter 964: loss nan, time: 241.14ms\n","iter 965: loss 1.7594, time: 240.88ms\n","iter 966: loss 3.5511, time: 240.89ms\n","iter 967: loss 2.6101, time: 242.21ms\n","iter 968: loss 4.2870, time: 241.91ms\n","iter 969: loss 3.3246, time: 242.44ms\n","iter 970: loss 3.6493, time: 242.96ms\n","iter 971: loss 2.9043, time: 242.65ms\n","iter 972: loss 2.0778, time: 243.41ms\n","iter 973: loss 2.1106, time: 245.17ms\n","iter 974: loss 2.6428, time: 244.66ms\n","iter 975: loss 1.9794, time: 243.62ms\n","iter 976: loss 1.8683, time: 243.86ms\n","iter 977: loss nan, time: 242.79ms\n","iter 978: loss 2.4474, time: 241.75ms\n","iter 979: loss 2.3266, time: 240.77ms\n","iter 980: loss 2.3911, time: 240.56ms\n","iter 981: loss nan, time: 241.66ms\n","iter 982: loss nan, time: 239.73ms\n","iter 983: loss 2.8439, time: 241.23ms\n","iter 984: loss 2.3619, time: 249.98ms\n","iter 985: loss nan, time: 241.59ms\n","iter 986: loss 3.9630, time: 240.66ms\n","iter 987: loss 3.8380, time: 240.54ms\n","iter 988: loss 2.1697, time: 240.75ms\n","iter 989: loss nan, time: 241.75ms\n","iter 990: loss nan, time: 240.42ms\n","iter 991: loss 3.1254, time: 244.00ms\n","iter 992: loss 3.1311, time: 241.45ms\n","iter 993: loss 2.4200, time: 241.81ms\n","iter 994: loss 2.3412, time: 241.94ms\n","iter 995: loss 3.1835, time: 241.65ms\n","iter 996: loss 3.7116, time: 243.75ms\n","iter 997: loss 2.8700, time: 241.96ms\n","iter 998: loss 3.2099, time: 241.75ms\n","iter 999: loss 2.7236, time: 242.68ms\n","iter 1000: loss 2.5822, time: 242.11ms\n","iter 1001: loss 2.4285, time: 243.10ms\n","iter 1002: loss 2.4288, time: 241.92ms\n","iter 1003: loss nan, time: 241.73ms\n","iter 1004: loss 2.9016, time: 242.82ms\n","iter 1005: loss nan, time: 241.80ms\n","iter 1006: loss 2.2431, time: 242.12ms\n","iter 1007: loss 4.0126, time: 242.06ms\n","iter 1008: loss 2.4228, time: 241.52ms\n","iter 1009: loss 2.1301, time: 242.17ms\n","iter 1010: loss nan, time: 241.74ms\n","iter 1011: loss 2.0529, time: 241.64ms\n","iter 1012: loss 2.5817, time: 242.00ms\n","iter 1013: loss 3.2800, time: 242.22ms\n","iter 1014: loss nan, time: 241.82ms\n","iter 1015: loss 1.7353, time: 240.92ms\n","iter 1016: loss 2.8981, time: 241.61ms\n","iter 1017: loss 1.5346, time: 242.82ms\n","iter 1018: loss 3.1981, time: 243.44ms\n","iter 1019: loss nan, time: 242.64ms\n","iter 1020: loss 2.0707, time: 243.91ms\n","iter 1021: loss 2.3780, time: 242.70ms\n","iter 1022: loss 2.4317, time: 241.46ms\n","iter 1023: loss 4.2999, time: 243.52ms\n","iter 1024: loss nan, time: 240.31ms\n","iter 1025: loss 2.3552, time: 241.82ms\n","iter 1026: loss 3.7941, time: 241.81ms\n","iter 1027: loss 2.6352, time: 241.94ms\n","iter 1028: loss 2.0212, time: 240.92ms\n","iter 1029: loss 3.3115, time: 241.70ms\n","iter 1030: loss 2.6182, time: 242.45ms\n","iter 1031: loss nan, time: 242.30ms\n","iter 1032: loss 3.3066, time: 241.54ms\n","iter 1033: loss 3.5263, time: 241.43ms\n","iter 1034: loss 1.8111, time: 241.54ms\n","iter 1035: loss 2.7713, time: 241.99ms\n","iter 1036: loss 2.0668, time: 241.90ms\n","iter 1037: loss 3.0381, time: 240.93ms\n","iter 1038: loss 2.4309, time: 241.43ms\n","iter 1039: loss 2.0134, time: 241.13ms\n","iter 1040: loss 2.8753, time: 240.04ms\n","iter 1041: loss 2.4477, time: 241.01ms\n","iter 1042: loss 3.0723, time: 241.37ms\n","iter 1043: loss 2.4536, time: 241.54ms\n","iter 1044: loss 1.8028, time: 241.27ms\n","iter 1045: loss 2.1801, time: 241.34ms\n","iter 1046: loss nan, time: 242.38ms\n","iter 1047: loss 3.9879, time: 241.85ms\n","iter 1048: loss 3.2428, time: 240.99ms\n","iter 1049: loss 2.3463, time: 242.00ms\n","iter 1050: loss 3.1692, time: 241.48ms\n","iter 1051: loss 2.1141, time: 241.51ms\n","iter 1052: loss 2.3881, time: 241.41ms\n","iter 1053: loss 1.8900, time: 241.70ms\n","iter 1054: loss 2.0498, time: 241.91ms\n","iter 1055: loss 2.3056, time: 244.81ms\n","iter 1056: loss nan, time: 241.86ms\n","iter 1057: loss 4.5310, time: 242.22ms\n","iter 1058: loss 1.5623, time: 241.92ms\n","iter 1059: loss 3.3530, time: 242.18ms\n","iter 1060: loss 2.1225, time: 241.24ms\n","iter 1061: loss 1.6701, time: 241.32ms\n","iter 1062: loss 2.6522, time: 240.64ms\n","iter 1063: loss 2.1935, time: 241.37ms\n","iter 1064: loss nan, time: 240.96ms\n","iter 1065: loss 2.3275, time: 241.52ms\n","iter 1066: loss 2.7143, time: 241.97ms\n","iter 1067: loss 2.0481, time: 241.40ms\n","iter 1068: loss 1.5505, time: 241.54ms\n","iter 1069: loss 1.7625, time: 241.71ms\n","iter 1070: loss 3.7637, time: 241.30ms\n","iter 1071: loss 2.0911, time: 240.62ms\n","iter 1072: loss 1.8226, time: 241.19ms\n","iter 1073: loss 1.9981, time: 240.68ms\n","iter 1074: loss 2.1419, time: 243.81ms\n","iter 1075: loss 3.3383, time: 241.47ms\n","iter 1076: loss 3.1541, time: 243.23ms\n","iter 1077: loss nan, time: 242.39ms\n","iter 1078: loss 1.6215, time: 242.79ms\n","iter 1079: loss 2.4454, time: 242.51ms\n","iter 1080: loss 1.8946, time: 245.31ms\n","iter 1081: loss 2.9356, time: 242.57ms\n","iter 1082: loss 3.3105, time: 244.85ms\n","iter 1083: loss 3.3552, time: 243.46ms\n","iter 1084: loss 3.9660, time: 244.52ms\n","iter 1085: loss 2.2915, time: 242.92ms\n","iter 1086: loss 1.7409, time: 242.65ms\n","iter 1087: loss 2.6845, time: 246.01ms\n","iter 1088: loss 2.2392, time: 242.87ms\n","iter 1089: loss 2.1461, time: 242.82ms\n","iter 1090: loss 2.8003, time: 241.71ms\n","iter 1091: loss 3.8644, time: 241.75ms\n","iter 1092: loss nan, time: 242.14ms\n","iter 1093: loss 4.1321, time: 241.55ms\n","iter 1094: loss 1.9514, time: 241.19ms\n","iter 1095: loss 2.9667, time: 241.19ms\n","iter 1096: loss 3.0449, time: 241.65ms\n","iter 1097: loss 2.8869, time: 242.06ms\n","iter 1098: loss 2.5141, time: 240.74ms\n","iter 1099: loss 2.5848, time: 242.11ms\n","iter 1100: loss nan, time: 242.94ms\n","iter 1101: loss 2.3773, time: 241.87ms\n","iter 1102: loss 1.1362, time: 241.55ms\n","iter 1103: loss 1.9785, time: 242.35ms\n","iter 1104: loss 2.3561, time: 241.50ms\n","iter 1105: loss 2.8586, time: 242.18ms\n","iter 1106: loss 2.9628, time: 241.62ms\n","iter 1107: loss 3.1378, time: 243.58ms\n","iter 1108: loss 2.0613, time: 242.22ms\n","iter 1109: loss 2.8386, time: 243.56ms\n","iter 1110: loss 2.4545, time: 241.38ms\n","iter 1111: loss 3.4343, time: 242.06ms\n","iter 1112: loss 3.1781, time: 242.11ms\n","iter 1113: loss 1.9363, time: 241.35ms\n","iter 1114: loss 1.7853, time: 240.70ms\n","iter 1115: loss nan, time: 241.22ms\n","iter 1116: loss 1.5179, time: 240.77ms\n","iter 1117: loss 2.2931, time: 240.99ms\n","iter 1118: loss nan, time: 240.74ms\n","iter 1119: loss 2.0471, time: 243.21ms\n","iter 1120: loss 2.4270, time: 240.37ms\n","iter 1121: loss 3.3246, time: 241.72ms\n","iter 1122: loss 2.0810, time: 240.82ms\n","iter 1123: loss 2.0863, time: 240.33ms\n","iter 1124: loss 3.4744, time: 240.43ms\n","iter 1125: loss 2.3720, time: 240.75ms\n","iter 1126: loss 2.7730, time: 241.78ms\n","iter 1127: loss 3.0502, time: 227.07ms\n","iter 1128: loss 1.9985, time: 241.39ms\n","iter 1129: loss 3.0958, time: 241.60ms\n","iter 1130: loss nan, time: 241.15ms\n","iter 1131: loss 2.7711, time: 243.34ms\n","iter 1132: loss nan, time: 241.35ms\n","iter 1133: loss nan, time: 240.90ms\n","iter 1134: loss 3.3796, time: 245.62ms\n","iter 1135: loss 3.4346, time: 241.87ms\n","iter 1136: loss 2.0150, time: 242.32ms\n","iter 1137: loss nan, time: 242.21ms\n","iter 1138: loss 2.9787, time: 247.37ms\n","iter 1139: loss nan, time: 243.06ms\n","iter 1140: loss 1.8085, time: 242.80ms\n","iter 1141: loss 2.5710, time: 244.65ms\n","iter 1142: loss nan, time: 243.26ms\n","iter 1143: loss 3.6674, time: 243.53ms\n","iter 1144: loss 2.1937, time: 243.30ms\n","iter 1145: loss 2.6759, time: 243.56ms\n","iter 1146: loss nan, time: 244.94ms\n","iter 1147: loss 2.0458, time: 241.69ms\n","iter 1148: loss 2.5501, time: 242.22ms\n","iter 1149: loss 1.9359, time: 242.00ms\n","iter 1150: loss 1.8025, time: 242.67ms\n","iter 1151: loss nan, time: 244.53ms\n","iter 1152: loss 2.0656, time: 240.68ms\n","iter 1153: loss 2.4244, time: 242.65ms\n","iter 1154: loss 4.5140, time: 241.98ms\n","iter 1155: loss nan, time: 241.87ms\n","iter 1156: loss 1.8706, time: 242.18ms\n","iter 1157: loss 2.1264, time: 241.73ms\n","iter 1158: loss nan, time: 241.88ms\n","iter 1159: loss nan, time: 242.06ms\n","iter 1160: loss 3.3267, time: 242.30ms\n","iter 1161: loss nan, time: 242.64ms\n","iter 1162: loss 2.6913, time: 241.56ms\n","iter 1163: loss nan, time: 242.26ms\n","iter 1164: loss 2.6106, time: 242.01ms\n","iter 1165: loss 1.6850, time: 242.09ms\n","iter 1166: loss 2.1684, time: 242.02ms\n","iter 1167: loss 1.0960, time: 241.00ms\n","iter 1168: loss 5.2303, time: 242.62ms\n","iter 1169: loss 2.4649, time: 241.58ms\n","iter 1170: loss 2.6197, time: 241.76ms\n","iter 1171: loss 2.4653, time: 241.66ms\n","iter 1172: loss 2.1477, time: 240.90ms\n","iter 1173: loss 1.9501, time: 241.96ms\n","iter 1174: loss 3.3617, time: 241.05ms\n","iter 1175: loss 3.0440, time: 241.96ms\n","iter 1176: loss 1.8687, time: 240.47ms\n","iter 1177: loss 1.2713, time: 241.75ms\n","iter 1178: loss 1.6022, time: 243.40ms\n","iter 1179: loss 2.2003, time: 243.31ms\n","iter 1180: loss 1.8422, time: 242.18ms\n","iter 1181: loss 2.0328, time: 242.02ms\n","iter 1182: loss 2.2492, time: 241.91ms\n","iter 1183: loss nan, time: 244.49ms\n","iter 1184: loss nan, time: 241.08ms\n","iter 1185: loss 2.0502, time: 241.74ms\n","iter 1186: loss 1.5583, time: 242.31ms\n","iter 1187: loss nan, time: 242.62ms\n","iter 1188: loss 2.3937, time: 242.89ms\n","iter 1189: loss 2.6074, time: 242.66ms\n","iter 1190: loss 3.2017, time: 243.30ms\n","iter 1191: loss 1.4425, time: 243.60ms\n","iter 1192: loss 2.5280, time: 243.43ms\n","iter 1193: loss 3.1903, time: 243.05ms\n","iter 1194: loss 1.6899, time: 242.54ms\n","iter 1195: loss 2.4314, time: 242.76ms\n","iter 1196: loss nan, time: 242.92ms\n","iter 1197: loss nan, time: 243.21ms\n","iter 1198: loss 1.4928, time: 242.96ms\n","iter 1199: loss 1.8161, time: 243.45ms\n","iter 1200: loss nan, time: 242.86ms\n","iter 1201: loss 2.5025, time: 243.90ms\n","iter 1202: loss 2.4630, time: 243.30ms\n","iter 1203: loss 1.6976, time: 242.56ms\n","iter 1204: loss 2.6291, time: 242.25ms\n","iter 1205: loss 2.2990, time: 241.56ms\n","iter 1206: loss nan, time: 241.31ms\n","iter 1207: loss 2.1198, time: 243.26ms\n","iter 1208: loss nan, time: 242.18ms\n","iter 1209: loss 2.0956, time: 242.19ms\n","iter 1210: loss 2.7434, time: 241.89ms\n","iter 1211: loss 2.8331, time: 241.72ms\n","iter 1212: loss nan, time: 242.72ms\n","iter 1213: loss 2.3154, time: 241.57ms\n","iter 1214: loss 2.2972, time: 243.74ms\n","iter 1215: loss 1.9571, time: 244.31ms\n","iter 1216: loss 1.0050, time: 241.46ms\n","iter 1217: loss 3.2327, time: 241.98ms\n","iter 1218: loss 2.4885, time: 241.92ms\n","iter 1219: loss 1.6714, time: 241.91ms\n","iter 1220: loss 2.2295, time: 241.74ms\n","iter 1221: loss 2.7621, time: 241.64ms\n","iter 1222: loss 2.1497, time: 240.96ms\n","iter 1223: loss 3.3031, time: 241.44ms\n","iter 1224: loss 2.3214, time: 241.61ms\n","iter 1225: loss 1.5287, time: 241.62ms\n","iter 1226: loss 2.1833, time: 240.61ms\n","iter 1227: loss 1.9778, time: 241.15ms\n","iter 1228: loss 2.3839, time: 241.80ms\n","iter 1229: loss 4.8612, time: 242.88ms\n","iter 1230: loss 2.4260, time: 241.08ms\n","iter 1231: loss 3.0912, time: 242.95ms\n","iter 1232: loss 3.8946, time: 241.30ms\n","iter 1233: loss nan, time: 241.42ms\n","iter 1234: loss 2.3738, time: 240.79ms\n","iter 1235: loss 2.7410, time: 240.91ms\n","iter 1236: loss 3.0343, time: 241.25ms\n","iter 1237: loss 3.4674, time: 241.31ms\n","iter 1238: loss 1.9568, time: 240.54ms\n","iter 1239: loss 2.4076, time: 241.40ms\n","iter 1240: loss 2.8616, time: 241.41ms\n","iter 1241: loss 2.4007, time: 241.83ms\n","iter 1242: loss 2.7735, time: 240.89ms\n","iter 1243: loss 1.6214, time: 241.57ms\n","iter 1244: loss 2.2000, time: 243.14ms\n","iter 1245: loss 1.4597, time: 242.98ms\n","iter 1246: loss 1.8860, time: 242.45ms\n","iter 1247: loss 2.7757, time: 245.23ms\n","iter 1248: loss 3.2514, time: 241.89ms\n","iter 1249: loss 2.5760, time: 242.96ms\n","iter 1250: loss 2.6986, time: 246.07ms\n","iter 1251: loss 2.6518, time: 242.24ms\n","iter 1252: loss 3.3715, time: 243.03ms\n","iter 1253: loss nan, time: 243.21ms\n","iter 1254: loss 1.5388, time: 244.13ms\n","iter 1255: loss 2.7856, time: 249.04ms\n","iter 1256: loss 3.2519, time: 245.61ms\n","iter 1257: loss 2.3062, time: 243.37ms\n","iter 1258: loss 2.3644, time: 243.92ms\n","iter 1259: loss nan, time: 243.46ms\n","iter 1260: loss 1.8446, time: 242.66ms\n","iter 1261: loss 2.4948, time: 243.12ms\n","iter 1262: loss 2.5791, time: 241.92ms\n","iter 1263: loss 1.8714, time: 241.18ms\n","iter 1264: loss 1.8262, time: 242.87ms\n","iter 1265: loss 2.4046, time: 242.96ms\n","iter 1266: loss 2.5105, time: 241.69ms\n","iter 1267: loss 1.2765, time: 242.59ms\n","iter 1268: loss 2.4287, time: 243.56ms\n","iter 1269: loss 2.2640, time: 242.83ms\n","iter 1270: loss 2.4854, time: 242.86ms\n","iter 1271: loss 1.3814, time: 244.82ms\n","iter 1272: loss 1.9417, time: 244.18ms\n","iter 1273: loss 1.8506, time: 242.38ms\n","iter 1274: loss 1.6786, time: 243.20ms\n","iter 1275: loss 2.4110, time: 241.05ms\n","iter 1276: loss 2.0585, time: 240.77ms\n","iter 1277: loss 1.7344, time: 249.49ms\n","iter 1278: loss 2.1659, time: 241.76ms\n","iter 1279: loss nan, time: 244.32ms\n","iter 1280: loss 3.2135, time: 240.85ms\n","iter 1281: loss 1.4665, time: 243.06ms\n","iter 1282: loss 2.3577, time: 242.18ms\n","iter 1283: loss 1.4158, time: 241.49ms\n","iter 1284: loss 1.7710, time: 241.57ms\n","iter 1285: loss 2.3048, time: 242.05ms\n","iter 1286: loss 1.8732, time: 242.28ms\n","iter 1287: loss 1.7938, time: 241.00ms\n","iter 1288: loss 1.6845, time: 240.97ms\n","iter 1289: loss 2.8912, time: 240.58ms\n","iter 1290: loss 2.2301, time: 241.93ms\n","iter 1291: loss nan, time: 240.81ms\n","iter 1292: loss 2.2777, time: 240.25ms\n","iter 1293: loss 2.5202, time: 242.62ms\n","iter 1294: loss 3.1739, time: 241.81ms\n","iter 1295: loss 2.3886, time: 242.54ms\n","iter 1296: loss nan, time: 242.02ms\n","iter 1297: loss 2.3418, time: 241.15ms\n","iter 1298: loss nan, time: 242.16ms\n","iter 1299: loss 1.5126, time: 241.98ms\n","iter 1300: loss 3.7750, time: 241.08ms\n","iter 1301: loss 1.5885, time: 244.46ms\n","iter 1302: loss 1.8078, time: 252.22ms\n","iter 1303: loss 2.1168, time: 243.36ms\n","iter 1304: loss 2.4198, time: 246.46ms\n","iter 1305: loss 2.4905, time: 244.63ms\n","iter 1306: loss 2.2099, time: 250.52ms\n","iter 1307: loss 2.8054, time: 243.43ms\n","iter 1308: loss nan, time: 242.82ms\n","iter 1309: loss 2.2468, time: 243.00ms\n","iter 1310: loss 1.4450, time: 243.62ms\n","iter 1311: loss 2.9037, time: 246.83ms\n","iter 1312: loss 2.6738, time: 242.45ms\n","iter 1313: loss 2.6647, time: 244.40ms\n","iter 1314: loss 3.1462, time: 243.97ms\n","iter 1315: loss 3.5591, time: 244.49ms\n","iter 1316: loss nan, time: 242.10ms\n","iter 1317: loss 2.9218, time: 241.83ms\n","iter 1318: loss 2.4897, time: 241.88ms\n","iter 1319: loss 2.5489, time: 242.50ms\n","iter 1320: loss 2.8251, time: 242.33ms\n","iter 1321: loss 3.1662, time: 242.64ms\n","iter 1322: loss 2.4904, time: 242.02ms\n","iter 1323: loss 1.8166, time: 242.53ms\n","iter 1324: loss 2.5434, time: 241.97ms\n","iter 1325: loss 2.4247, time: 241.95ms\n","iter 1326: loss 1.9904, time: 241.46ms\n","iter 1327: loss 1.9563, time: 242.70ms\n","iter 1328: loss nan, time: 241.36ms\n","iter 1329: loss 2.2787, time: 240.29ms\n","iter 1330: loss 1.6281, time: 241.41ms\n","iter 1331: loss 2.5194, time: 241.88ms\n","iter 1332: loss 2.3955, time: 241.82ms\n","iter 1333: loss nan, time: 241.25ms\n","iter 1334: loss 1.9792, time: 241.30ms\n","iter 1335: loss 2.5836, time: 242.10ms\n","iter 1336: loss 1.5894, time: 241.61ms\n","iter 1337: loss 2.5370, time: 241.44ms\n","iter 1338: loss 1.1414, time: 240.97ms\n","iter 1339: loss 2.2372, time: 240.54ms\n","iter 1340: loss 2.3584, time: 241.64ms\n","iter 1341: loss nan, time: 240.66ms\n","iter 1342: loss 2.4193, time: 239.61ms\n","iter 1343: loss 3.0644, time: 242.91ms\n","iter 1344: loss 1.7331, time: 240.69ms\n","iter 1345: loss 3.1732, time: 240.53ms\n","iter 1346: loss 1.6562, time: 240.19ms\n","iter 1347: loss 1.7024, time: 240.56ms\n","iter 1348: loss 1.8365, time: 241.64ms\n","iter 1349: loss 1.9073, time: 240.66ms\n","iter 1350: loss 2.9029, time: 242.11ms\n","iter 1351: loss 2.5258, time: 241.06ms\n","iter 1352: loss 2.7903, time: 241.88ms\n","iter 1353: loss 2.2162, time: 242.74ms\n","iter 1354: loss 1.1733, time: 240.41ms\n","iter 1355: loss 1.3380, time: 241.31ms\n","iter 1356: loss 3.1143, time: 240.85ms\n","iter 1357: loss nan, time: 241.74ms\n","iter 1358: loss 2.5111, time: 241.64ms\n","iter 1359: loss 2.2379, time: 243.50ms\n","iter 1360: loss 2.9012, time: 244.03ms\n","iter 1361: loss 2.3587, time: 243.57ms\n","iter 1362: loss 1.7041, time: 242.54ms\n","iter 1363: loss nan, time: 243.26ms\n","iter 1364: loss nan, time: 243.60ms\n","iter 1365: loss nan, time: 243.06ms\n","iter 1366: loss 2.3303, time: 246.45ms\n","iter 1367: loss 3.5256, time: 245.43ms\n","iter 1368: loss 4.1046, time: 243.36ms\n","iter 1369: loss 1.3188, time: 244.82ms\n","iter 1370: loss 0.9974, time: 243.65ms\n","iter 1371: loss 2.4778, time: 243.97ms\n","iter 1372: loss 2.2253, time: 244.00ms\n","iter 1373: loss 2.2866, time: 243.87ms\n","iter 1374: loss 1.4034, time: 242.58ms\n","iter 1375: loss 3.7803, time: 245.19ms\n","iter 1376: loss nan, time: 241.86ms\n","iter 1377: loss 1.5475, time: 243.03ms\n","iter 1378: loss nan, time: 241.73ms\n","iter 1379: loss nan, time: 241.89ms\n","iter 1380: loss 2.3619, time: 241.06ms\n","iter 1381: loss nan, time: 242.30ms\n","iter 1382: loss 3.0534, time: 241.41ms\n","iter 1383: loss 4.9046, time: 241.51ms\n","iter 1384: loss 1.8027, time: 241.74ms\n","iter 1385: loss 1.0993, time: 240.61ms\n","iter 1386: loss nan, time: 241.24ms\n","iter 1387: loss nan, time: 239.70ms\n","iter 1388: loss 2.0024, time: 240.90ms\n","iter 1389: loss nan, time: 242.34ms\n","iter 1390: loss 3.0465, time: 242.30ms\n","iter 1391: loss 1.7390, time: 241.85ms\n","iter 1392: loss 2.7230, time: 240.49ms\n","iter 1393: loss 2.9753, time: 242.67ms\n","iter 1394: loss 3.0470, time: 242.82ms\n","iter 1395: loss 2.8231, time: 240.66ms\n","iter 1396: loss 2.0288, time: 243.91ms\n","iter 1397: loss 2.6790, time: 241.65ms\n","iter 1398: loss nan, time: 242.44ms\n","iter 1399: loss 4.0359, time: 241.17ms\n","iter 1400: loss 6.8868, time: 240.33ms\n","iter 1401: loss 2.1408, time: 241.10ms\n","iter 1402: loss 2.2464, time: 241.46ms\n","iter 1403: loss 2.3776, time: 241.52ms\n","iter 1404: loss 1.8639, time: 241.28ms\n","iter 1405: loss 1.7644, time: 241.82ms\n","iter 1406: loss 2.1026, time: 242.01ms\n","iter 1407: loss nan, time: 244.02ms\n","iter 1408: loss 3.1729, time: 241.09ms\n","iter 1409: loss 2.7140, time: 241.06ms\n","iter 1410: loss 3.4312, time: 242.69ms\n","iter 1411: loss 2.3601, time: 242.27ms\n","iter 1412: loss 3.1961, time: 242.29ms\n","iter 1413: loss nan, time: 242.10ms\n","iter 1414: loss 2.5735, time: 244.64ms\n","iter 1415: loss 2.9083, time: 243.77ms\n","iter 1416: loss 2.4916, time: 243.62ms\n","iter 1417: loss 1.8469, time: 241.79ms\n","iter 1418: loss 2.1513, time: 242.02ms\n","iter 1419: loss 1.4751, time: 242.31ms\n","iter 1420: loss nan, time: 242.61ms\n","iter 1421: loss 1.1451, time: 242.42ms\n","iter 1422: loss 3.2777, time: 242.06ms\n","iter 1423: loss nan, time: 243.00ms\n","iter 1424: loss 1.3621, time: 242.19ms\n","iter 1425: loss 1.3606, time: 243.16ms\n","iter 1426: loss 2.2503, time: 242.88ms\n","iter 1427: loss nan, time: 243.44ms\n","iter 1428: loss 1.0825, time: 242.70ms\n","iter 1429: loss 4.8814, time: 243.08ms\n","iter 1430: loss 3.0638, time: 242.24ms\n","iter 1431: loss 1.5906, time: 241.98ms\n","iter 1432: loss 2.1487, time: 242.27ms\n","iter 1433: loss 1.5964, time: 241.24ms\n","iter 1434: loss 2.5417, time: 240.79ms\n","iter 1435: loss 5.4562, time: 241.46ms\n","iter 1436: loss 1.3320, time: 241.75ms\n","iter 1437: loss 2.0598, time: 241.46ms\n","iter 1438: loss nan, time: 242.14ms\n","iter 1439: loss 1.9808, time: 244.18ms\n","iter 1440: loss 1.8657, time: 240.53ms\n","iter 1441: loss 1.2153, time: 241.03ms\n","iter 1442: loss 2.3501, time: 241.02ms\n","iter 1443: loss 2.3433, time: 241.30ms\n","iter 1444: loss 2.3903, time: 241.08ms\n","iter 1445: loss 2.0988, time: 241.80ms\n","iter 1446: loss 2.5867, time: 241.21ms\n","iter 1447: loss 1.9979, time: 242.13ms\n","iter 1448: loss 2.4663, time: 242.34ms\n","iter 1449: loss nan, time: 241.67ms\n","iter 1450: loss 1.6402, time: 241.15ms\n","iter 1451: loss 2.0909, time: 241.67ms\n","iter 1452: loss 1.9423, time: 241.29ms\n","iter 1453: loss 1.8238, time: 241.37ms\n","iter 1454: loss nan, time: 241.00ms\n","iter 1455: loss 1.5361, time: 242.28ms\n","iter 1456: loss 1.3389, time: 241.70ms\n","iter 1457: loss 2.0135, time: 242.35ms\n","iter 1458: loss 3.5702, time: 242.22ms\n","iter 1459: loss 2.5744, time: 241.10ms\n","iter 1460: loss 2.1256, time: 241.89ms\n","iter 1461: loss 2.3241, time: 241.24ms\n","iter 1462: loss 2.2569, time: 242.86ms\n","iter 1463: loss 2.2717, time: 242.66ms\n","iter 1464: loss 1.7957, time: 241.48ms\n","iter 1465: loss 2.0961, time: 242.71ms\n","iter 1466: loss 2.5356, time: 241.19ms\n","iter 1467: loss 1.6705, time: 242.05ms\n","iter 1468: loss 1.8972, time: 241.85ms\n","iter 1469: loss nan, time: 241.38ms\n","iter 1470: loss 2.1080, time: 241.57ms\n","iter 1471: loss nan, time: 246.12ms\n","iter 1472: loss nan, time: 241.39ms\n","iter 1473: loss 2.1176, time: 242.32ms\n","iter 1474: loss 2.9903, time: 241.57ms\n","iter 1475: loss nan, time: 241.53ms\n","iter 1476: loss nan, time: 242.11ms\n","iter 1477: loss 3.8984, time: 242.20ms\n","iter 1478: loss 2.7565, time: 241.59ms\n","iter 1479: loss 2.4614, time: 241.78ms\n","iter 1480: loss 2.7087, time: 243.74ms\n","iter 1481: loss 1.7125, time: 243.81ms\n","iter 1482: loss 2.7103, time: 242.52ms\n","iter 1483: loss 2.4130, time: 241.98ms\n","iter 1484: loss 1.2008, time: 243.13ms\n","iter 1485: loss 2.5133, time: 243.34ms\n","iter 1486: loss 1.9866, time: 242.24ms\n","iter 1487: loss 1.9512, time: 240.93ms\n","iter 1488: loss 1.2583, time: 241.89ms\n","iter 1489: loss nan, time: 241.39ms\n","iter 1490: loss 2.0466, time: 243.09ms\n","iter 1491: loss 3.9469, time: 242.19ms\n","iter 1492: loss 3.0658, time: 241.62ms\n","iter 1493: loss nan, time: 241.66ms\n","iter 1494: loss 2.2479, time: 241.28ms\n","iter 1495: loss nan, time: 241.73ms\n","iter 1496: loss 2.1181, time: 241.83ms\n","iter 1497: loss 1.4738, time: 241.98ms\n","iter 1498: loss 2.8217, time: 241.50ms\n","iter 1499: loss nan, time: 241.96ms\n","iter 1500: loss 2.1984, time: 241.83ms\n","iter 1501: loss 1.6473, time: 242.08ms\n","iter 1502: loss 1.4934, time: 240.51ms\n","iter 1503: loss 1.6472, time: 244.04ms\n","iter 1504: loss nan, time: 241.28ms\n","iter 1505: loss 2.0001, time: 241.54ms\n","iter 1506: loss nan, time: 240.94ms\n","iter 1507: loss 2.9982, time: 241.10ms\n","iter 1508: loss 1.5727, time: 241.56ms\n","iter 1509: loss 2.2530, time: 241.95ms\n","iter 1510: loss 3.8727, time: 240.83ms\n","iter 1511: loss 1.5546, time: 241.65ms\n","iter 1512: loss 2.1576, time: 241.83ms\n","iter 1513: loss 1.2887, time: 241.64ms\n","iter 1514: loss 1.4691, time: 240.91ms\n","iter 1515: loss nan, time: 240.38ms\n","iter 1516: loss 0.6267, time: 240.37ms\n","iter 1517: loss 3.7401, time: 240.34ms\n","iter 1518: loss nan, time: 241.08ms\n","iter 1519: loss 1.5083, time: 241.64ms\n","iter 1520: loss 2.5189, time: 241.40ms\n","iter 1521: loss 1.4299, time: 243.12ms\n","iter 1522: loss 2.6297, time: 242.40ms\n","iter 1523: loss 1.6656, time: 240.98ms\n","iter 1524: loss 1.4423, time: 241.46ms\n","iter 1525: loss 1.8945, time: 241.27ms\n","iter 1526: loss 0.4610, time: 241.69ms\n","iter 1527: loss 1.9739, time: 242.72ms\n","iter 1528: loss 2.2685, time: 242.92ms\n","iter 1529: loss 1.2584, time: 242.14ms\n","iter 1530: loss 2.9905, time: 241.77ms\n","iter 1531: loss nan, time: 242.11ms\n","iter 1532: loss 3.1326, time: 243.00ms\n","iter 1533: loss 2.7478, time: 244.03ms\n","iter 1534: loss 2.1664, time: 244.26ms\n","iter 1535: loss 3.1373, time: 246.64ms\n","iter 1536: loss 3.9056, time: 242.56ms\n","iter 1537: loss 3.5689, time: 243.78ms\n","iter 1538: loss 2.2287, time: 246.90ms\n","iter 1539: loss 3.2449, time: 242.91ms\n","iter 1540: loss 2.2428, time: 243.38ms\n","iter 1541: loss 2.0859, time: 243.31ms\n","iter 1542: loss 2.1304, time: 248.35ms\n","iter 1543: loss 1.5014, time: 242.01ms\n","iter 1544: loss 1.6563, time: 241.38ms\n","iter 1545: loss 1.5955, time: 242.30ms\n","iter 1546: loss 2.5168, time: 241.75ms\n","iter 1547: loss 2.8659, time: 242.03ms\n","iter 1548: loss 2.8486, time: 241.30ms\n","iter 1549: loss 2.6704, time: 241.47ms\n","iter 1550: loss 2.4704, time: 243.00ms\n","iter 1551: loss 1.9308, time: 240.59ms\n","iter 1552: loss 2.8490, time: 240.61ms\n","iter 1553: loss 2.0899, time: 241.91ms\n","iter 1554: loss 1.5526, time: 241.54ms\n","iter 1555: loss 2.7110, time: 241.22ms\n","iter 1556: loss 1.9263, time: 240.49ms\n","iter 1557: loss 5.7508, time: 240.79ms\n","iter 1558: loss 2.1043, time: 241.61ms\n","iter 1559: loss 2.3714, time: 241.58ms\n","iter 1560: loss 0.4712, time: 241.20ms\n","iter 1561: loss 3.3823, time: 241.68ms\n","iter 1562: loss 1.9318, time: 242.64ms\n","iter 1563: loss nan, time: 241.55ms\n","iter 1564: loss 3.8324, time: 242.02ms\n","iter 1565: loss 1.2724, time: 241.14ms\n","iter 1566: loss 2.3785, time: 241.74ms\n","iter 1567: loss 2.1425, time: 246.08ms\n","iter 1568: loss 2.7386, time: 240.65ms\n","iter 1569: loss 2.4183, time: 241.67ms\n","iter 1570: loss 3.2251, time: 241.99ms\n","iter 1571: loss 2.4205, time: 242.51ms\n","iter 1572: loss 2.3728, time: 243.11ms\n","iter 1573: loss 1.9482, time: 241.60ms\n","iter 1574: loss 3.3032, time: 241.83ms\n","iter 1575: loss 2.9918, time: 242.52ms\n","iter 1576: loss 1.8289, time: 241.60ms\n","iter 1577: loss 2.1529, time: 242.29ms\n","iter 1578: loss 1.7326, time: 240.81ms\n","iter 1579: loss 1.3197, time: 242.89ms\n","iter 1580: loss 2.0854, time: 241.33ms\n","iter 1581: loss 1.5858, time: 242.38ms\n","iter 1582: loss nan, time: 242.52ms\n","iter 1583: loss 1.9950, time: 242.69ms\n","iter 1584: loss 1.7223, time: 244.50ms\n","iter 1585: loss 1.4777, time: 243.62ms\n","iter 1586: loss 3.6506, time: 242.97ms\n","iter 1587: loss 2.7612, time: 242.83ms\n","iter 1588: loss 1.7299, time: 242.01ms\n","iter 1589: loss 2.7670, time: 241.67ms\n","iter 1590: loss 2.6510, time: 242.51ms\n","iter 1591: loss 3.4198, time: 242.26ms\n","iter 1592: loss 1.9438, time: 243.63ms\n","iter 1593: loss 2.5228, time: 244.14ms\n","iter 1594: loss 4.0205, time: 243.99ms\n","iter 1595: loss 2.0609, time: 243.08ms\n","iter 1596: loss nan, time: 242.54ms\n","iter 1597: loss 2.7564, time: 243.65ms\n","iter 1598: loss 2.1736, time: 244.23ms\n","iter 1599: loss 1.3371, time: 245.47ms\n","iter 1600: loss nan, time: 241.58ms\n","iter 1601: loss 1.3817, time: 242.04ms\n","iter 1602: loss 1.7416, time: 242.54ms\n","iter 1603: loss 1.9455, time: 241.36ms\n","iter 1604: loss 2.0553, time: 241.32ms\n","iter 1605: loss 1.9039, time: 241.97ms\n","iter 1606: loss 2.9633, time: 242.41ms\n","iter 1607: loss 2.6900, time: 243.96ms\n","iter 1608: loss 2.6037, time: 243.59ms\n","iter 1609: loss 3.0565, time: 242.69ms\n","iter 1610: loss 2.6709, time: 241.11ms\n","iter 1611: loss 3.0563, time: 240.56ms\n","iter 1612: loss 1.6866, time: 242.31ms\n","iter 1613: loss 1.9764, time: 241.53ms\n","iter 1614: loss 1.9303, time: 241.45ms\n","iter 1615: loss 1.8729, time: 247.28ms\n","iter 1616: loss 2.6574, time: 242.01ms\n","iter 1617: loss nan, time: 241.45ms\n","iter 1618: loss 2.9232, time: 241.77ms\n","iter 1619: loss 2.0725, time: 241.23ms\n","iter 1620: loss 1.9133, time: 242.57ms\n","iter 1621: loss 2.4719, time: 241.49ms\n","iter 1622: loss 3.8445, time: 241.91ms\n","iter 1623: loss 3.7031, time: 242.26ms\n","iter 1624: loss 2.0814, time: 241.56ms\n","iter 1625: loss 2.2568, time: 242.43ms\n","iter 1626: loss 2.8466, time: 242.16ms\n","iter 1627: loss 1.7705, time: 242.42ms\n","iter 1628: loss 2.5262, time: 243.72ms\n","iter 1629: loss 1.2396, time: 243.73ms\n","iter 1630: loss 1.3255, time: 243.64ms\n","iter 1631: loss 3.2635, time: 244.96ms\n","iter 1632: loss nan, time: 242.36ms\n","iter 1633: loss 1.8863, time: 243.66ms\n","iter 1634: loss 2.6276, time: 243.17ms\n","iter 1635: loss 2.1611, time: 243.16ms\n","iter 1636: loss 1.8272, time: 242.14ms\n","iter 1637: loss 3.0798, time: 243.74ms\n","iter 1638: loss 2.5023, time: 242.34ms\n","iter 1639: loss 1.8225, time: 242.52ms\n","iter 1640: loss 2.4741, time: 242.77ms\n","iter 1641: loss 3.0474, time: 242.60ms\n","iter 1642: loss 1.4994, time: 243.09ms\n","iter 1643: loss 2.3745, time: 243.13ms\n","iter 1644: loss 2.9777, time: 242.89ms\n","iter 1645: loss 2.8368, time: 243.52ms\n","iter 1646: loss 2.4939, time: 243.70ms\n","iter 1647: loss 1.3782, time: 247.65ms\n","iter 1648: loss 2.7901, time: 242.68ms\n","iter 1649: loss 2.2263, time: 248.74ms\n","iter 1650: loss 2.1859, time: 250.16ms\n","iter 1651: loss 2.1136, time: 228.81ms\n","iter 1652: loss 7.0105, time: 249.96ms\n","iter 1653: loss 1.9090, time: 245.05ms\n","iter 1654: loss 1.8800, time: 244.99ms\n","iter 1655: loss 1.3476, time: 243.58ms\n","iter 1656: loss nan, time: 246.82ms\n","iter 1657: loss 1.5572, time: 243.06ms\n","iter 1658: loss 1.8395, time: 243.36ms\n","iter 1659: loss 1.4627, time: 242.88ms\n","iter 1660: loss nan, time: 244.08ms\n","iter 1661: loss 3.3409, time: 244.12ms\n","iter 1662: loss 2.5167, time: 243.36ms\n","iter 1663: loss nan, time: 245.95ms\n","iter 1664: loss 2.7487, time: 241.49ms\n","iter 1665: loss 0.9441, time: 242.15ms\n","iter 1666: loss 2.0378, time: 245.93ms\n","iter 1667: loss 2.1926, time: 242.80ms\n","iter 1668: loss 2.1147, time: 242.42ms\n","iter 1669: loss 2.0048, time: 242.84ms\n","iter 1670: loss 2.8760, time: 242.81ms\n","iter 1671: loss 1.9285, time: 242.64ms\n","iter 1672: loss 2.3195, time: 243.46ms\n","iter 1673: loss 1.8839, time: 243.07ms\n","iter 1674: loss 2.0298, time: 244.79ms\n","iter 1675: loss 1.9235, time: 242.52ms\n","iter 1676: loss 1.7337, time: 243.03ms\n","iter 1677: loss 3.3131, time: 242.93ms\n","iter 1678: loss 2.6058, time: 243.37ms\n","iter 1679: loss 2.9725, time: 243.66ms\n","iter 1680: loss 1.3020, time: 242.71ms\n","iter 1681: loss nan, time: 241.82ms\n","iter 1682: loss 1.7009, time: 242.45ms\n","iter 1683: loss 1.1260, time: 242.10ms\n","iter 1684: loss 1.6063, time: 242.87ms\n","iter 1685: loss 3.2579, time: 242.65ms\n","iter 1686: loss 4.2967, time: 242.19ms\n","iter 1687: loss 2.2325, time: 242.42ms\n","iter 1688: loss 2.5169, time: 241.37ms\n","iter 1689: loss nan, time: 242.46ms\n","iter 1690: loss 1.9249, time: 242.88ms\n","iter 1691: loss 1.4466, time: 242.27ms\n","iter 1692: loss 1.8996, time: 241.92ms\n","iter 1693: loss 1.7280, time: 242.57ms\n","iter 1694: loss nan, time: 242.83ms\n","iter 1695: loss 1.0101, time: 244.43ms\n","iter 1696: loss 2.3220, time: 241.16ms\n","iter 1697: loss 2.1108, time: 242.82ms\n","iter 1698: loss 2.2709, time: 242.57ms\n","iter 1699: loss 2.1827, time: 242.19ms\n","iter 1700: loss nan, time: 242.24ms\n","iter 1701: loss 2.7398, time: 241.91ms\n","iter 1702: loss 1.1251, time: 242.14ms\n","iter 1703: loss 2.7152, time: 242.82ms\n","iter 1704: loss 1.9501, time: 246.10ms\n","iter 1705: loss nan, time: 242.33ms\n","iter 1706: loss 2.4729, time: 243.10ms\n","iter 1707: loss 2.5359, time: 243.03ms\n","iter 1708: loss 3.8593, time: 242.28ms\n","iter 1709: loss 2.6598, time: 242.26ms\n","iter 1710: loss 0.8169, time: 248.82ms\n","iter 1711: loss 3.0574, time: 243.29ms\n","iter 1712: loss 2.0414, time: 242.07ms\n","iter 1713: loss 1.4373, time: 241.93ms\n","iter 1714: loss 1.7384, time: 242.00ms\n","iter 1715: loss nan, time: 242.15ms\n","iter 1716: loss 3.3416, time: 241.49ms\n","iter 1717: loss 2.1384, time: 242.57ms\n","iter 1718: loss 2.6065, time: 241.47ms\n","iter 1719: loss nan, time: 241.85ms\n","iter 1720: loss nan, time: 241.11ms\n","iter 1721: loss 1.2454, time: 240.86ms\n","iter 1722: loss 1.7622, time: 240.38ms\n","iter 1723: loss 2.3051, time: 241.98ms\n","iter 1724: loss nan, time: 241.30ms\n","iter 1725: loss nan, time: 241.83ms\n","iter 1726: loss 2.7954, time: 242.01ms\n","iter 1727: loss 2.7333, time: 244.03ms\n","iter 1728: loss 2.7748, time: 242.01ms\n","iter 1729: loss 1.3390, time: 241.11ms\n","iter 1730: loss nan, time: 241.64ms\n","iter 1731: loss 3.0131, time: 241.05ms\n","iter 1732: loss 2.0711, time: 242.29ms\n","iter 1733: loss 3.0691, time: 242.36ms\n","iter 1734: loss nan, time: 241.23ms\n","iter 1735: loss 2.3087, time: 242.39ms\n","iter 1736: loss 3.4942, time: 241.68ms\n","iter 1737: loss 0.8346, time: 242.04ms\n","iter 1738: loss 2.9581, time: 241.95ms\n","iter 1739: loss 1.1739, time: 241.81ms\n","iter 1740: loss 3.0572, time: 242.34ms\n","iter 1741: loss 2.1416, time: 242.43ms\n","iter 1742: loss 2.5529, time: 220.22ms\n","iter 1743: loss 2.3608, time: 241.70ms\n","iter 1744: loss 2.8308, time: 241.69ms\n","iter 1745: loss 1.5442, time: 242.18ms\n","iter 1746: loss 2.4199, time: 242.15ms\n","iter 1747: loss 2.2635, time: 242.16ms\n","iter 1748: loss 1.7841, time: 241.97ms\n","iter 1749: loss 1.1418, time: 241.41ms\n","iter 1750: loss 3.0325, time: 242.27ms\n","iter 1751: loss 3.5098, time: 242.76ms\n","iter 1752: loss 1.6883, time: 242.59ms\n","iter 1753: loss 2.1935, time: 241.84ms\n","iter 1754: loss 0.8665, time: 243.60ms\n","iter 1755: loss 1.8544, time: 242.20ms\n","iter 1756: loss 2.0740, time: 242.72ms\n","iter 1757: loss nan, time: 242.10ms\n","iter 1758: loss 2.6516, time: 243.96ms\n","iter 1759: loss nan, time: 245.18ms\n","iter 1760: loss 1.6924, time: 242.24ms\n","iter 1761: loss 2.2754, time: 242.42ms\n","iter 1762: loss 3.5512, time: 242.84ms\n","iter 1763: loss 1.6724, time: 243.99ms\n","iter 1764: loss 0.8938, time: 242.90ms\n","iter 1765: loss 1.8430, time: 242.76ms\n","iter 1766: loss 2.8880, time: 243.86ms\n","iter 1767: loss 2.2018, time: 245.69ms\n","iter 1768: loss 1.5879, time: 248.54ms\n","iter 1769: loss 2.1629, time: 242.63ms\n","iter 1770: loss 2.9339, time: 242.38ms\n","iter 1771: loss 1.9060, time: 237.83ms\n","iter 1772: loss 1.7029, time: 242.21ms\n","iter 1773: loss nan, time: 242.70ms\n","iter 1774: loss 1.9004, time: 242.32ms\n","iter 1775: loss 3.3238, time: 242.46ms\n","iter 1776: loss nan, time: 242.29ms\n","iter 1777: loss 1.9989, time: 241.63ms\n","iter 1778: loss 0.6299, time: 241.30ms\n","iter 1779: loss 1.4303, time: 242.20ms\n","iter 1780: loss 2.0312, time: 241.95ms\n","iter 1781: loss 3.3093, time: 241.49ms\n","iter 1782: loss 1.9435, time: 241.87ms\n","iter 1783: loss 2.0439, time: 241.29ms\n","iter 1784: loss 1.5074, time: 241.86ms\n","iter 1785: loss 2.2020, time: 242.06ms\n","iter 1786: loss 1.8577, time: 241.51ms\n","iter 1787: loss 1.5002, time: 242.14ms\n","iter 1788: loss 2.5642, time: 242.64ms\n","iter 1789: loss 2.2069, time: 242.38ms\n","iter 1790: loss 1.2784, time: 242.28ms\n","iter 1791: loss nan, time: 243.83ms\n","iter 1792: loss nan, time: 241.82ms\n","iter 1793: loss nan, time: 241.48ms\n","iter 1794: loss 2.1184, time: 244.31ms\n","iter 1795: loss 3.0980, time: 243.38ms\n","iter 1796: loss 4.0446, time: 242.93ms\n","iter 1797: loss 2.6976, time: 242.41ms\n","iter 1798: loss 1.5326, time: 242.05ms\n","iter 1799: loss 1.4711, time: 242.75ms\n","iter 1800: loss 1.3421, time: 242.42ms\n","iter 1801: loss 2.5048, time: 241.97ms\n","iter 1802: loss 2.7512, time: 243.32ms\n","iter 1803: loss 3.9838, time: 242.99ms\n","iter 1804: loss 1.3910, time: 242.87ms\n","iter 1805: loss 2.1707, time: 242.71ms\n","iter 1806: loss 1.8245, time: 242.68ms\n","iter 1807: loss 1.8884, time: 243.04ms\n","iter 1808: loss 2.5325, time: 243.48ms\n","iter 1809: loss 3.2759, time: 242.97ms\n","iter 1810: loss 2.0244, time: 242.95ms\n","iter 1811: loss 3.0956, time: 245.63ms\n","iter 1812: loss 4.0362, time: 242.34ms\n","iter 1813: loss 3.2226, time: 242.72ms\n","iter 1814: loss 1.6316, time: 242.01ms\n","iter 1815: loss 2.8610, time: 241.95ms\n","iter 1816: loss nan, time: 242.80ms\n","iter 1817: loss 2.0876, time: 242.45ms\n","iter 1818: loss 2.6760, time: 242.44ms\n","iter 1819: loss 1.6139, time: 243.06ms\n","iter 1820: loss nan, time: 242.89ms\n","iter 1821: loss 1.0454, time: 244.02ms\n","iter 1822: loss 3.7413, time: 243.16ms\n","iter 1823: loss 2.8106, time: 245.74ms\n","iter 1824: loss 2.1132, time: 242.75ms\n","iter 1825: loss 0.9480, time: 243.85ms\n","iter 1826: loss 2.1740, time: 243.24ms\n","iter 1827: loss 3.2566, time: 243.41ms\n","iter 1828: loss 2.1210, time: 243.40ms\n","iter 1829: loss 1.8480, time: 242.85ms\n","iter 1830: loss 1.8267, time: 243.47ms\n","iter 1831: loss 2.6437, time: 242.74ms\n","iter 1832: loss nan, time: 242.23ms\n","iter 1833: loss 1.7523, time: 242.94ms\n","iter 1834: loss 2.3853, time: 242.72ms\n","iter 1835: loss 2.4219, time: 243.02ms\n","iter 1836: loss 2.5387, time: 242.54ms\n","iter 1837: loss 2.0417, time: 248.62ms\n","iter 1838: loss 2.1252, time: 243.75ms\n","iter 1839: loss nan, time: 242.32ms\n","iter 1840: loss 3.1178, time: 242.18ms\n","iter 1841: loss 3.2475, time: 242.76ms\n","iter 1842: loss 2.1772, time: 241.95ms\n","iter 1843: loss 2.7826, time: 242.87ms\n","iter 1844: loss 1.7414, time: 242.24ms\n","iter 1845: loss nan, time: 243.05ms\n","iter 1846: loss 1.9660, time: 241.09ms\n","iter 1847: loss nan, time: 241.84ms\n","iter 1848: loss 2.0066, time: 241.27ms\n","iter 1849: loss 2.0991, time: 240.83ms\n","iter 1850: loss 2.6844, time: 241.33ms\n","iter 1851: loss 2.7739, time: 241.37ms\n","iter 1852: loss nan, time: 241.18ms\n","iter 1853: loss 3.4814, time: 241.25ms\n","iter 1854: loss 2.9961, time: 240.17ms\n","iter 1855: loss 2.6349, time: 244.44ms\n","iter 1856: loss 1.9748, time: 246.96ms\n","iter 1857: loss 1.9072, time: 241.52ms\n","iter 1858: loss 3.6240, time: 241.92ms\n","iter 1859: loss 1.2021, time: 242.01ms\n","iter 1860: loss 1.5897, time: 241.13ms\n","iter 1861: loss nan, time: 241.68ms\n","iter 1862: loss nan, time: 242.08ms\n","iter 1863: loss 1.3936, time: 241.15ms\n","iter 1864: loss nan, time: 241.23ms\n","iter 1865: loss 1.1607, time: 241.55ms\n","iter 1866: loss 3.6953, time: 240.68ms\n","iter 1867: loss 2.5245, time: 244.23ms\n","iter 1868: loss 2.2095, time: 241.65ms\n","iter 1869: loss 1.9719, time: 243.01ms\n","iter 1870: loss 1.6138, time: 242.71ms\n","iter 1871: loss 3.0088, time: 242.33ms\n","iter 1872: loss 2.2056, time: 242.88ms\n","iter 1873: loss 2.4893, time: 243.05ms\n","iter 1874: loss 1.7740, time: 241.45ms\n","iter 1875: loss 3.4805, time: 243.20ms\n","iter 1876: loss 2.1361, time: 245.93ms\n","iter 1877: loss 2.5640, time: 241.95ms\n","iter 1878: loss nan, time: 243.41ms\n","iter 1879: loss 3.5727, time: 246.18ms\n","iter 1880: loss 3.0046, time: 247.95ms\n","iter 1881: loss 2.2156, time: 244.38ms\n","iter 1882: loss 1.6138, time: 247.26ms\n","iter 1883: loss 1.8638, time: 241.63ms\n","iter 1884: loss 2.5368, time: 241.89ms\n","iter 1885: loss 1.4983, time: 241.03ms\n","iter 1886: loss 1.9912, time: 241.01ms\n","iter 1887: loss 2.9265, time: 244.57ms\n","iter 1888: loss 2.2591, time: 241.74ms\n","iter 1889: loss 2.3018, time: 242.67ms\n","iter 1890: loss 3.3280, time: 243.17ms\n","iter 1891: loss nan, time: 243.97ms\n","iter 1892: loss 1.6946, time: 242.93ms\n","iter 1893: loss 3.2694, time: 242.98ms\n","iter 1894: loss 2.2445, time: 243.42ms\n","iter 1895: loss nan, time: 243.37ms\n","iter 1896: loss 2.4784, time: 244.26ms\n","iter 1897: loss nan, time: 242.69ms\n","iter 1898: loss 3.0081, time: 242.89ms\n","iter 1899: loss nan, time: 243.86ms\n","iter 1900: loss 2.7975, time: 243.07ms\n","iter 1901: loss 3.4650, time: 243.17ms\n","iter 1902: loss nan, time: 242.74ms\n","iter 1903: loss 2.1201, time: 242.56ms\n","iter 1904: loss 2.1957, time: 242.30ms\n","iter 1905: loss 1.5927, time: 243.86ms\n","iter 1906: loss 1.4742, time: 242.65ms\n","iter 1907: loss 1.5097, time: 248.86ms\n","iter 1908: loss 3.7265, time: 249.03ms\n","iter 1909: loss 3.1373, time: 245.79ms\n","iter 1910: loss 1.8835, time: 248.33ms\n","iter 1911: loss 1.2682, time: 242.68ms\n","iter 1912: loss 2.7697, time: 241.80ms\n","iter 1913: loss 0.9751, time: 242.39ms\n","iter 1914: loss 2.9216, time: 241.65ms\n","iter 1915: loss 2.3708, time: 241.08ms\n","iter 1916: loss 2.5677, time: 242.43ms\n","iter 1917: loss 1.8181, time: 247.58ms\n","iter 1918: loss 3.6109, time: 240.93ms\n","iter 1919: loss 1.6295, time: 243.90ms\n","iter 1920: loss 2.5609, time: 240.99ms\n","iter 1921: loss 2.2114, time: 241.76ms\n","iter 1922: loss 3.6917, time: 242.60ms\n","iter 1923: loss 1.5127, time: 242.75ms\n","iter 1924: loss nan, time: 242.87ms\n","iter 1925: loss 1.6340, time: 242.60ms\n","iter 1926: loss 1.6206, time: 244.52ms\n","iter 1927: loss 1.8127, time: 244.10ms\n","iter 1928: loss 2.0852, time: 246.55ms\n","iter 1929: loss 1.4247, time: 243.27ms\n","iter 1930: loss 2.5629, time: 242.48ms\n","iter 1931: loss nan, time: 242.37ms\n","iter 1932: loss 3.1395, time: 242.78ms\n","iter 1933: loss 3.1655, time: 244.40ms\n","iter 1934: loss nan, time: 242.40ms\n","iter 1935: loss 2.4526, time: 242.31ms\n","iter 1936: loss 2.1799, time: 242.12ms\n","iter 1937: loss 1.2210, time: 243.11ms\n","iter 1938: loss 1.6855, time: 242.78ms\n","iter 1939: loss 2.9548, time: 241.96ms\n","iter 1940: loss 1.8593, time: 240.84ms\n","iter 1941: loss 2.1213, time: 242.09ms\n","iter 1942: loss nan, time: 241.38ms\n","iter 1943: loss 2.9536, time: 242.77ms\n","iter 1944: loss 1.3636, time: 241.19ms\n","iter 1945: loss 2.8852, time: 240.86ms\n","iter 1946: loss nan, time: 242.18ms\n","iter 1947: loss 2.3760, time: 241.86ms\n","iter 1948: loss 2.2862, time: 241.74ms\n","iter 1949: loss 2.4114, time: 241.28ms\n","iter 1950: loss 2.4761, time: 241.53ms\n","iter 1951: loss 2.1105, time: 244.88ms\n","iter 1952: loss 2.9768, time: 241.46ms\n","iter 1953: loss 2.5232, time: 241.59ms\n","iter 1954: loss 3.2159, time: 242.22ms\n","iter 1955: loss 2.3820, time: 241.62ms\n","iter 1956: loss 2.2312, time: 241.95ms\n","iter 1957: loss 2.8781, time: 240.81ms\n","iter 1958: loss nan, time: 242.39ms\n","iter 1959: loss 1.5978, time: 241.92ms\n","iter 1960: loss nan, time: 241.40ms\n","iter 1961: loss 0.6605, time: 241.16ms\n","iter 1962: loss 2.3710, time: 241.09ms\n","iter 1963: loss 2.6910, time: 241.23ms\n","iter 1964: loss 2.3659, time: 241.54ms\n","iter 1965: loss 1.8935, time: 241.59ms\n","iter 1966: loss nan, time: 241.58ms\n","iter 1967: loss nan, time: 241.20ms\n","iter 1968: loss nan, time: 241.87ms\n","iter 1969: loss nan, time: 242.52ms\n","iter 1970: loss 2.1915, time: 241.94ms\n","iter 1971: loss nan, time: 242.15ms\n","iter 1972: loss 2.3714, time: 241.19ms\n","iter 1973: loss 1.0197, time: 243.03ms\n","iter 1974: loss 2.4449, time: 241.79ms\n","iter 1975: loss 3.1107, time: 241.78ms\n","iter 1976: loss 3.0512, time: 242.82ms\n","iter 1977: loss 1.4982, time: 241.47ms\n","iter 1978: loss 1.7315, time: 216.92ms\n","iter 1979: loss 2.2331, time: 241.92ms\n","iter 1980: loss 1.5559, time: 243.85ms\n","iter 1981: loss 2.3879, time: 242.76ms\n","iter 1982: loss 2.1339, time: 242.40ms\n","iter 1983: loss 4.1933, time: 245.81ms\n","iter 1984: loss 1.4229, time: 241.80ms\n","iter 1985: loss 2.7420, time: 242.94ms\n","iter 1986: loss 1.7746, time: 243.97ms\n","iter 1987: loss 1.8741, time: 242.91ms\n","iter 1988: loss nan, time: 242.66ms\n","iter 1989: loss 2.8897, time: 242.40ms\n","iter 1990: loss 1.6780, time: 242.86ms\n","iter 1991: loss 2.2277, time: 242.87ms\n","iter 1992: loss 2.0586, time: 242.22ms\n","iter 1993: loss 2.6340, time: 243.21ms\n","iter 1994: loss 2.7517, time: 242.37ms\n","iter 1995: loss 1.3447, time: 243.53ms\n","iter 1996: loss 1.8989, time: 241.69ms\n","iter 1997: loss 1.3618, time: 241.31ms\n","iter 1998: loss 0.6671, time: 242.79ms\n","iter 1999: loss 1.6484, time: 241.24ms\n","iter 2000: loss 2.3559, time: 242.62ms\n","iter 2001: loss nan, time: 241.26ms\n","iter 2002: loss 4.1533, time: 241.32ms\n","iter 2003: loss 1.9369, time: 241.41ms\n","iter 2004: loss 3.0238, time: 241.62ms\n","iter 2005: loss 2.4792, time: 241.44ms\n","iter 2006: loss 2.2999, time: 242.40ms\n","iter 2007: loss 2.3860, time: 241.87ms\n","iter 2008: loss 2.5430, time: 241.96ms\n","iter 2009: loss 2.4571, time: 241.69ms\n","iter 2010: loss 2.1821, time: 241.52ms\n","iter 2011: loss 0.7430, time: 240.46ms\n","iter 2012: loss 1.6973, time: 243.62ms\n","iter 2013: loss 2.2039, time: 244.83ms\n","iter 2014: loss 0.8851, time: 243.51ms\n","iter 2015: loss 3.0977, time: 245.79ms\n","iter 2016: loss 1.8352, time: 254.10ms\n","iter 2017: loss 2.3258, time: 242.01ms\n","iter 2018: loss 1.5582, time: 244.08ms\n","iter 2019: loss 2.1492, time: 245.13ms\n","iter 2020: loss 2.5801, time: 243.50ms\n","iter 2021: loss nan, time: 242.15ms\n","iter 2022: loss 1.4252, time: 243.49ms\n","iter 2023: loss 2.0259, time: 242.79ms\n","iter 2024: loss nan, time: 243.56ms\n","iter 2025: loss nan, time: 242.02ms\n","iter 2026: loss 1.4099, time: 242.95ms\n","iter 2027: loss nan, time: 242.13ms\n","iter 2028: loss 1.9883, time: 242.42ms\n","iter 2029: loss nan, time: 242.62ms\n","iter 2030: loss 2.9772, time: 241.55ms\n","iter 2031: loss 1.7676, time: 241.83ms\n","iter 2032: loss 1.6548, time: 242.48ms\n","iter 2033: loss nan, time: 241.67ms\n","iter 2034: loss 2.1351, time: 242.16ms\n","iter 2035: loss 1.0479, time: 243.61ms\n","iter 2036: loss 2.1868, time: 221.14ms\n","iter 2037: loss 1.8022, time: 244.71ms\n","iter 2038: loss 1.8212, time: 241.86ms\n","iter 2039: loss 2.6002, time: 242.38ms\n","iter 2040: loss nan, time: 241.62ms\n","iter 2041: loss 2.1816, time: 242.11ms\n","iter 2042: loss 1.5998, time: 241.68ms\n","iter 2043: loss 2.4342, time: 244.17ms\n","iter 2044: loss 1.8311, time: 242.30ms\n","iter 2045: loss nan, time: 244.29ms\n","iter 2046: loss 3.0991, time: 242.19ms\n","iter 2047: loss 1.9938, time: 245.22ms\n","iter 2048: loss 2.2892, time: 248.25ms\n","iter 2049: loss nan, time: 242.66ms\n","iter 2050: loss 2.0169, time: 243.46ms\n","iter 2051: loss 1.7301, time: 243.24ms\n","iter 2052: loss 1.2701, time: 243.03ms\n","iter 2053: loss 2.5838, time: 243.19ms\n","iter 2054: loss 2.5386, time: 241.98ms\n","iter 2055: loss nan, time: 242.27ms\n","iter 2056: loss 2.6240, time: 242.00ms\n","iter 2057: loss 2.4002, time: 241.61ms\n","iter 2058: loss 2.2483, time: 241.90ms\n","iter 2059: loss nan, time: 241.82ms\n","iter 2060: loss 2.5220, time: 241.23ms\n","iter 2061: loss nan, time: 241.78ms\n","iter 2062: loss 2.3320, time: 247.47ms\n","iter 2063: loss 1.6728, time: 241.89ms\n","iter 2064: loss 2.5767, time: 241.08ms\n","iter 2065: loss 1.7807, time: 241.88ms\n","iter 2066: loss 2.7693, time: 241.54ms\n","iter 2067: loss 2.8534, time: 242.49ms\n","iter 2068: loss 1.2752, time: 242.46ms\n","iter 2069: loss 1.5098, time: 241.62ms\n","iter 2070: loss 2.3706, time: 242.24ms\n","iter 2071: loss 3.1255, time: 242.95ms\n","iter 2072: loss 1.6721, time: 241.88ms\n","iter 2073: loss 2.1799, time: 243.39ms\n","iter 2074: loss 3.3232, time: 242.84ms\n","iter 2075: loss 1.6246, time: 242.24ms\n","iter 2076: loss nan, time: 242.75ms\n","iter 2077: loss 1.5058, time: 241.74ms\n","iter 2078: loss 1.6026, time: 241.78ms\n","iter 2079: loss 2.1530, time: 243.41ms\n","iter 2080: loss 0.9542, time: 240.90ms\n","iter 2081: loss 1.7367, time: 242.35ms\n","iter 2082: loss 1.9926, time: 241.76ms\n","iter 2083: loss 3.1284, time: 242.51ms\n","iter 2084: loss 1.6448, time: 241.96ms\n","iter 2085: loss 2.2715, time: 249.06ms\n","iter 2086: loss 2.2105, time: 242.19ms\n","iter 2087: loss 3.1356, time: 241.27ms\n","iter 2088: loss 2.5177, time: 240.72ms\n","iter 2089: loss 3.9811, time: 242.13ms\n","iter 2090: loss 2.0699, time: 242.21ms\n","iter 2091: loss nan, time: 241.53ms\n","iter 2092: loss 2.7875, time: 241.97ms\n","iter 2093: loss 1.3568, time: 243.42ms\n","iter 2094: loss 2.1458, time: 245.76ms\n","iter 2095: loss nan, time: 241.64ms\n","iter 2096: loss 2.0251, time: 241.50ms\n","iter 2097: loss nan, time: 242.21ms\n","iter 2098: loss 2.2981, time: 241.84ms\n","iter 2099: loss 2.2360, time: 242.04ms\n","iter 2100: loss 2.0444, time: 241.47ms\n","iter 2101: loss 3.1435, time: 241.63ms\n","iter 2102: loss 2.0315, time: 246.46ms\n","iter 2103: loss 3.0872, time: 245.05ms\n","iter 2104: loss 1.7796, time: 245.60ms\n","iter 2105: loss 2.5807, time: 243.15ms\n","iter 2106: loss 2.2135, time: 242.60ms\n","iter 2107: loss 2.6658, time: 242.66ms\n","iter 2108: loss 1.5952, time: 244.11ms\n","iter 2109: loss nan, time: 241.58ms\n","iter 2110: loss 2.2585, time: 241.20ms\n","iter 2111: loss 2.2205, time: 244.99ms\n","iter 2112: loss 1.3756, time: 240.80ms\n","iter 2113: loss 2.3178, time: 240.64ms\n","iter 2114: loss 2.4339, time: 241.42ms\n","iter 2115: loss 2.2819, time: 241.55ms\n","iter 2116: loss nan, time: 241.07ms\n","iter 2117: loss 1.6569, time: 240.92ms\n","iter 2118: loss 2.2418, time: 242.20ms\n","iter 2119: loss 2.1753, time: 243.15ms\n","iter 2120: loss 1.7964, time: 241.09ms\n","iter 2121: loss 2.0720, time: 241.87ms\n","iter 2122: loss 1.9878, time: 240.73ms\n","iter 2123: loss nan, time: 241.76ms\n","iter 2124: loss 1.1672, time: 241.73ms\n","iter 2125: loss nan, time: 241.10ms\n","iter 2126: loss 0.8799, time: 241.20ms\n","iter 2127: loss 2.2857, time: 242.19ms\n","iter 2128: loss 2.4399, time: 241.06ms\n","iter 2129: loss nan, time: 240.76ms\n","iter 2130: loss 0.8801, time: 240.83ms\n","iter 2131: loss 2.3877, time: 241.63ms\n","iter 2132: loss 2.9254, time: 241.35ms\n","iter 2133: loss 1.4048, time: 242.87ms\n","iter 2134: loss 2.0952, time: 242.11ms\n","iter 2135: loss 2.2579, time: 241.46ms\n","iter 2136: loss 2.0492, time: 242.82ms\n","iter 2137: loss 2.9915, time: 242.06ms\n","iter 2138: loss 1.4445, time: 242.46ms\n","iter 2139: loss 2.0772, time: 241.48ms\n","iter 2140: loss 2.5114, time: 243.74ms\n","iter 2141: loss 2.5513, time: 242.19ms\n","iter 2142: loss 2.0300, time: 241.85ms\n","iter 2143: loss 2.2603, time: 244.94ms\n","iter 2144: loss 1.6095, time: 241.48ms\n","iter 2145: loss nan, time: 242.82ms\n","iter 2146: loss 2.5388, time: 243.27ms\n","iter 2147: loss nan, time: 241.94ms\n","iter 2148: loss 1.7853, time: 242.32ms\n","iter 2149: loss 1.5713, time: 241.91ms\n","iter 2150: loss 2.2055, time: 244.51ms\n","iter 2151: loss 2.6638, time: 242.03ms\n","iter 2152: loss 2.9296, time: 242.62ms\n","iter 2153: loss 1.3112, time: 242.69ms\n","iter 2154: loss 2.3400, time: 243.70ms\n","iter 2155: loss 1.5245, time: 242.89ms\n","iter 2156: loss 1.7689, time: 245.08ms\n","iter 2157: loss 3.9691, time: 242.76ms\n","iter 2158: loss nan, time: 245.10ms\n","iter 2159: loss nan, time: 244.67ms\n","iter 2160: loss nan, time: 244.39ms\n","iter 2161: loss 2.2313, time: 242.45ms\n","iter 2162: loss 2.3080, time: 243.78ms\n","iter 2163: loss 2.9496, time: 244.92ms\n","iter 2164: loss 2.6242, time: 246.03ms\n","iter 2165: loss nan, time: 242.89ms\n","iter 2166: loss 1.5815, time: 241.50ms\n","iter 2167: loss 1.6585, time: 241.63ms\n","iter 2168: loss 2.8316, time: 241.84ms\n","iter 2169: loss 2.0838, time: 242.19ms\n","iter 2170: loss 3.0388, time: 240.97ms\n","iter 2171: loss 2.1010, time: 242.12ms\n","iter 2172: loss 1.6669, time: 241.46ms\n","iter 2173: loss 2.2015, time: 241.44ms\n","iter 2174: loss 2.0355, time: 242.03ms\n","iter 2175: loss 2.5594, time: 242.83ms\n","iter 2176: loss 0.9285, time: 240.98ms\n","iter 2177: loss 2.4165, time: 241.84ms\n","iter 2178: loss 1.4937, time: 241.11ms\n","iter 2179: loss 2.4387, time: 242.61ms\n","iter 2180: loss 3.0232, time: 241.92ms\n","iter 2181: loss 1.8091, time: 243.50ms\n","iter 2182: loss 1.9173, time: 242.80ms\n","iter 2183: loss 2.7155, time: 241.58ms\n","iter 2184: loss 2.8554, time: 242.09ms\n","iter 2185: loss 2.4045, time: 242.13ms\n","iter 2186: loss 0.9683, time: 242.72ms\n","iter 2187: loss 2.4005, time: 242.03ms\n","iter 2188: loss 2.3306, time: 241.61ms\n","iter 2189: loss 1.5704, time: 243.76ms\n","iter 2190: loss 1.0588, time: 243.24ms\n","iter 2191: loss nan, time: 242.19ms\n","iter 2192: loss 1.8129, time: 242.68ms\n","iter 2193: loss 1.2597, time: 241.84ms\n","iter 2194: loss 1.8573, time: 242.25ms\n","iter 2195: loss 2.7910, time: 241.67ms\n","iter 2196: loss 1.3369, time: 241.68ms\n","iter 2197: loss 1.5543, time: 241.59ms\n","iter 2198: loss 4.8314, time: 241.61ms\n","iter 2199: loss 2.5657, time: 242.71ms\n","iter 2200: loss 1.3893, time: 242.67ms\n","iter 2201: loss 2.0260, time: 242.71ms\n","iter 2202: loss 1.8083, time: 241.85ms\n","iter 2203: loss 1.8569, time: 242.04ms\n","iter 2204: loss 1.3276, time: 242.03ms\n","iter 2205: loss nan, time: 241.64ms\n","iter 2206: loss 2.7130, time: 243.06ms\n","iter 2207: loss 1.6980, time: 245.58ms\n","iter 2208: loss 2.2035, time: 241.27ms\n","iter 2209: loss 1.6068, time: 241.06ms\n","iter 2210: loss 2.8361, time: 241.40ms\n","iter 2211: loss 3.0508, time: 240.81ms\n","iter 2212: loss 2.7982, time: 240.90ms\n","iter 2213: loss 2.1100, time: 241.18ms\n","iter 2214: loss 2.0308, time: 240.93ms\n","iter 2215: loss 2.0998, time: 242.37ms\n","iter 2216: loss 1.6290, time: 242.82ms\n","iter 2217: loss 1.5946, time: 242.55ms\n","iter 2218: loss 2.4280, time: 243.01ms\n","iter 2219: loss 2.8741, time: 242.86ms\n","iter 2220: loss 1.7055, time: 243.24ms\n","iter 2221: loss 2.1104, time: 243.17ms\n","iter 2222: loss 1.8251, time: 242.18ms\n","iter 2223: loss nan, time: 241.58ms\n","iter 2224: loss 1.8702, time: 240.65ms\n","iter 2225: loss 1.1682, time: 240.31ms\n","iter 2226: loss 2.1875, time: 242.18ms\n","iter 2227: loss 2.0541, time: 240.21ms\n","iter 2228: loss 2.1444, time: 240.81ms\n","iter 2229: loss 2.2115, time: 241.57ms\n","iter 2230: loss 1.6988, time: 241.31ms\n","iter 2231: loss 2.0417, time: 241.57ms\n","iter 2232: loss 2.1477, time: 241.33ms\n","iter 2233: loss 2.0981, time: 241.59ms\n","iter 2234: loss nan, time: 242.01ms\n","iter 2235: loss 1.7042, time: 240.68ms\n","iter 2236: loss 1.0863, time: 240.85ms\n","iter 2237: loss 1.8170, time: 240.74ms\n","iter 2238: loss 1.5685, time: 240.46ms\n","iter 2239: loss 1.5873, time: 243.17ms\n","iter 2240: loss 1.6542, time: 241.83ms\n","iter 2241: loss 2.3372, time: 241.56ms\n","iter 2242: loss 2.1724, time: 241.91ms\n","iter 2243: loss 2.8379, time: 241.31ms\n","iter 2244: loss 1.4609, time: 242.38ms\n","iter 2245: loss 1.6621, time: 242.02ms\n","iter 2246: loss 2.8637, time: 242.20ms\n","iter 2247: loss 2.3109, time: 248.82ms\n","iter 2248: loss 2.6392, time: 241.85ms\n","iter 2249: loss 2.4362, time: 242.17ms\n","iter 2250: loss 2.0074, time: 241.55ms\n","iter 2251: loss 1.5329, time: 241.87ms\n","iter 2252: loss 2.2812, time: 241.26ms\n","iter 2253: loss 1.8746, time: 241.89ms\n","iter 2254: loss nan, time: 241.53ms\n","iter 2255: loss nan, time: 242.16ms\n","iter 2256: loss 1.5064, time: 241.80ms\n","iter 2257: loss 1.7018, time: 242.10ms\n","iter 2258: loss 2.9002, time: 242.53ms\n","iter 2259: loss 2.1502, time: 241.90ms\n","iter 2260: loss 1.5648, time: 242.22ms\n","iter 2261: loss 1.1238, time: 242.60ms\n","iter 2262: loss 1.5871, time: 241.65ms\n","iter 2263: loss 3.6866, time: 242.73ms\n","iter 2264: loss nan, time: 242.26ms\n","iter 2265: loss 2.3146, time: 242.44ms\n","iter 2266: loss 3.4655, time: 243.14ms\n","iter 2267: loss 2.3380, time: 242.65ms\n","iter 2268: loss 2.0185, time: 242.04ms\n","iter 2269: loss 1.7439, time: 242.70ms\n","iter 2270: loss 3.7596, time: 243.06ms\n","iter 2271: loss 2.4108, time: 245.53ms\n","iter 2272: loss 1.1069, time: 241.81ms\n","iter 2273: loss 2.3406, time: 245.11ms\n","iter 2274: loss 2.3813, time: 245.71ms\n","iter 2275: loss 3.0262, time: 243.79ms\n","iter 2276: loss 2.4343, time: 242.74ms\n","iter 2277: loss 1.7226, time: 244.56ms\n","iter 2278: loss nan, time: 244.28ms\n","iter 2279: loss nan, time: 241.77ms\n","iter 2280: loss 2.9960, time: 242.30ms\n","iter 2281: loss 1.8190, time: 241.59ms\n","iter 2282: loss 2.2101, time: 241.85ms\n","iter 2283: loss 2.0646, time: 241.45ms\n","iter 2284: loss 3.1554, time: 241.88ms\n","iter 2285: loss 2.9239, time: 241.53ms\n","iter 2286: loss 1.6449, time: 243.05ms\n","iter 2287: loss 1.8662, time: 242.13ms\n","iter 2288: loss 0.9578, time: 241.96ms\n","iter 2289: loss 1.9375, time: 242.40ms\n","iter 2290: loss 2.1517, time: 242.19ms\n","iter 2291: loss 2.7770, time: 241.67ms\n","iter 2292: loss 1.5949, time: 241.82ms\n","iter 2293: loss 1.7593, time: 241.03ms\n","iter 2294: loss 2.0047, time: 241.20ms\n","iter 2295: loss 3.1945, time: 241.78ms\n","iter 2296: loss 2.6568, time: 243.05ms\n","iter 2297: loss 3.2910, time: 241.22ms\n","iter 2298: loss 2.0866, time: 241.71ms\n","iter 2299: loss nan, time: 242.18ms\n","iter 2300: loss 1.7604, time: 241.31ms\n","iter 2301: loss 1.3317, time: 242.56ms\n","iter 2302: loss nan, time: 241.10ms\n","iter 2303: loss 2.0003, time: 244.74ms\n","iter 2304: loss 2.7415, time: 241.37ms\n","iter 2305: loss 2.1462, time: 241.08ms\n","iter 2306: loss 2.0360, time: 241.25ms\n","iter 2307: loss 2.6549, time: 241.21ms\n","iter 2308: loss 2.5725, time: 240.76ms\n","iter 2309: loss 1.7208, time: 244.82ms\n","iter 2310: loss nan, time: 240.55ms\n","iter 2311: loss 2.2845, time: 241.12ms\n","iter 2312: loss 1.5879, time: 249.32ms\n","iter 2313: loss 2.7754, time: 241.59ms\n","iter 2314: loss 1.7119, time: 241.60ms\n","iter 2315: loss 2.2518, time: 242.61ms\n","iter 2316: loss 2.1507, time: 242.48ms\n","iter 2317: loss 1.1752, time: 242.07ms\n","iter 2318: loss 1.0752, time: 242.51ms\n","iter 2319: loss 2.0901, time: 241.98ms\n","iter 2320: loss 2.7807, time: 245.22ms\n","iter 2321: loss 1.9314, time: 247.24ms\n","iter 2322: loss 2.5992, time: 242.81ms\n","iter 2323: loss 1.8835, time: 246.90ms\n","iter 2324: loss 2.4814, time: 243.92ms\n","iter 2325: loss nan, time: 244.03ms\n","iter 2326: loss 1.9810, time: 242.95ms\n","iter 2327: loss nan, time: 243.06ms\n","iter 2328: loss 2.5347, time: 243.94ms\n","iter 2329: loss 2.6469, time: 243.79ms\n","iter 2330: loss nan, time: 243.25ms\n","iter 2331: loss 3.1717, time: 242.72ms\n","iter 2332: loss 1.5344, time: 243.97ms\n","iter 2333: loss 2.1102, time: 243.80ms\n","iter 2334: loss 1.6880, time: 250.06ms\n","iter 2335: loss 1.9204, time: 247.32ms\n","iter 2336: loss 3.1570, time: 222.65ms\n","iter 2337: loss 2.0687, time: 242.48ms\n","iter 2338: loss 3.0366, time: 241.97ms\n","iter 2339: loss 1.9645, time: 242.43ms\n","iter 2340: loss 1.8439, time: 242.25ms\n","iter 2341: loss 1.6961, time: 241.73ms\n","iter 2342: loss nan, time: 242.37ms\n","iter 2343: loss 1.3269, time: 241.44ms\n","iter 2344: loss 1.2381, time: 242.82ms\n","iter 2345: loss 2.4388, time: 242.68ms\n","iter 2346: loss 2.0764, time: 241.41ms\n","iter 2347: loss 1.9364, time: 241.53ms\n","iter 2348: loss 2.2468, time: 242.64ms\n","iter 2349: loss 3.1541, time: 241.84ms\n","iter 2350: loss 2.2309, time: 242.21ms\n","iter 2351: loss 3.4787, time: 240.74ms\n","iter 2352: loss 1.6857, time: 241.25ms\n","iter 2353: loss 0.6098, time: 241.35ms\n","iter 2354: loss 1.5051, time: 242.58ms\n","iter 2355: loss 2.3349, time: 241.00ms\n","iter 2356: loss 1.7411, time: 241.53ms\n","iter 2357: loss 2.3223, time: 240.83ms\n","iter 2358: loss 0.9732, time: 241.25ms\n","iter 2359: loss 2.1550, time: 246.90ms\n","iter 2360: loss 4.8729, time: 240.54ms\n","iter 2361: loss 1.9423, time: 241.23ms\n","iter 2362: loss 2.5010, time: 241.63ms\n","iter 2363: loss 1.4782, time: 240.68ms\n","iter 2364: loss nan, time: 242.27ms\n","iter 2365: loss 1.7005, time: 243.57ms\n","iter 2366: loss 1.1409, time: 241.29ms\n","iter 2367: loss 2.5911, time: 245.04ms\n","iter 2368: loss 1.4251, time: 241.53ms\n","iter 2369: loss 1.1030, time: 243.73ms\n","iter 2370: loss 2.3628, time: 242.50ms\n","iter 2371: loss nan, time: 241.92ms\n","iter 2372: loss 3.9657, time: 242.62ms\n","iter 2373: loss 1.4209, time: 242.35ms\n","iter 2374: loss 1.8217, time: 243.21ms\n","iter 2375: loss 3.1912, time: 241.67ms\n","iter 2376: loss 3.0950, time: 242.13ms\n","iter 2377: loss 3.4113, time: 247.81ms\n","iter 2378: loss 2.2208, time: 242.76ms\n","iter 2379: loss 2.1852, time: 244.08ms\n","iter 2380: loss 2.4415, time: 242.18ms\n","iter 2381: loss 1.3822, time: 242.71ms\n","iter 2382: loss 2.4034, time: 242.58ms\n","iter 2383: loss 2.2248, time: 243.26ms\n","iter 2384: loss nan, time: 242.22ms\n","iter 2385: loss 1.1483, time: 243.08ms\n","iter 2386: loss 1.3111, time: 243.01ms\n","iter 2387: loss 1.3660, time: 245.99ms\n","iter 2388: loss 1.5897, time: 248.98ms\n","iter 2389: loss 1.3882, time: 242.55ms\n","iter 2390: loss 2.7240, time: 242.39ms\n","iter 2391: loss 2.4533, time: 244.59ms\n","iter 2392: loss 2.0999, time: 241.75ms\n","iter 2393: loss 2.3027, time: 241.66ms\n","iter 2394: loss 2.0199, time: 243.98ms\n","iter 2395: loss 1.4567, time: 242.86ms\n","iter 2396: loss 1.3713, time: 240.97ms\n","iter 2397: loss 1.2234, time: 241.39ms\n","iter 2398: loss 1.6428, time: 241.55ms\n","iter 2399: loss 1.6231, time: 244.31ms\n","iter 2400: loss 4.2339, time: 241.43ms\n","iter 2401: loss 1.8570, time: 240.87ms\n","iter 2402: loss 1.7099, time: 242.33ms\n","iter 2403: loss 1.6517, time: 242.43ms\n","iter 2404: loss nan, time: 241.73ms\n","iter 2405: loss 2.6117, time: 242.65ms\n","iter 2406: loss nan, time: 242.93ms\n","iter 2407: loss 1.6037, time: 242.87ms\n","iter 2408: loss 2.5424, time: 242.19ms\n","iter 2409: loss 1.2269, time: 241.93ms\n","iter 2410: loss 1.9993, time: 244.37ms\n","iter 2411: loss 1.3202, time: 242.27ms\n","iter 2412: loss 1.8096, time: 242.58ms\n","iter 2413: loss nan, time: 240.94ms\n","iter 2414: loss 1.5561, time: 243.03ms\n","iter 2415: loss 1.3155, time: 242.72ms\n","iter 2416: loss 1.8918, time: 242.49ms\n","iter 2417: loss 1.5704, time: 241.92ms\n","iter 2418: loss 1.8541, time: 241.70ms\n","iter 2419: loss 1.6008, time: 242.42ms\n","iter 2420: loss 1.6352, time: 242.62ms\n","iter 2421: loss 2.4569, time: 241.48ms\n","iter 2422: loss 4.9306, time: 242.70ms\n","iter 2423: loss 2.2738, time: 241.44ms\n","iter 2424: loss nan, time: 242.38ms\n","iter 2425: loss nan, time: 241.93ms\n","iter 2426: loss 2.5045, time: 241.93ms\n","iter 2427: loss 3.1570, time: 243.82ms\n","iter 2428: loss 2.0310, time: 242.58ms\n","iter 2429: loss 2.1758, time: 242.95ms\n","iter 2430: loss 2.0897, time: 241.49ms\n","iter 2431: loss 2.1460, time: 244.17ms\n","iter 2432: loss 2.6041, time: 240.89ms\n","iter 2433: loss 2.2423, time: 243.22ms\n","iter 2434: loss 1.5205, time: 243.48ms\n","iter 2435: loss nan, time: 242.43ms\n","iter 2436: loss 1.0504, time: 243.78ms\n","iter 2437: loss 2.3765, time: 244.51ms\n","iter 2438: loss 3.0613, time: 242.82ms\n","iter 2439: loss 1.2004, time: 244.26ms\n","iter 2440: loss 2.4360, time: 243.12ms\n","iter 2441: loss 1.4330, time: 244.37ms\n","iter 2442: loss 3.2778, time: 246.38ms\n","iter 2443: loss 3.0250, time: 244.03ms\n","iter 2444: loss 2.0422, time: 244.06ms\n","iter 2445: loss 2.2430, time: 242.69ms\n","iter 2446: loss 1.2628, time: 243.88ms\n","iter 2447: loss 1.6001, time: 244.84ms\n","iter 2448: loss 2.3056, time: 244.16ms\n","iter 2449: loss 2.3463, time: 242.66ms\n","iter 2450: loss 1.7977, time: 241.67ms\n","iter 2451: loss 1.7813, time: 242.50ms\n","iter 2452: loss 1.5206, time: 241.63ms\n","iter 2453: loss 1.3434, time: 242.24ms\n","iter 2454: loss 0.5936, time: 242.53ms\n","iter 2455: loss 3.1812, time: 242.02ms\n","iter 2456: loss 1.1261, time: 241.92ms\n","iter 2457: loss nan, time: 241.35ms\n","iter 2458: loss 2.1261, time: 240.78ms\n","iter 2459: loss 2.0632, time: 240.83ms\n","iter 2460: loss 1.5887, time: 241.42ms\n","iter 2461: loss 2.4230, time: 242.49ms\n","iter 2462: loss 3.1412, time: 241.85ms\n","iter 2463: loss 2.1153, time: 243.55ms\n","iter 2464: loss 1.7468, time: 240.85ms\n","iter 2465: loss 1.3851, time: 242.00ms\n","iter 2466: loss 1.7509, time: 242.38ms\n","iter 2467: loss nan, time: 241.50ms\n","iter 2468: loss 2.7351, time: 241.93ms\n","iter 2469: loss 1.7147, time: 241.26ms\n","iter 2470: loss nan, time: 242.25ms\n","iter 2471: loss 1.2279, time: 241.34ms\n","iter 2472: loss 2.3650, time: 241.72ms\n","iter 2473: loss 2.0464, time: 241.46ms\n","iter 2474: loss 1.8866, time: 241.52ms\n","iter 2475: loss 2.7910, time: 242.19ms\n","iter 2476: loss 1.5291, time: 242.28ms\n","iter 2477: loss 2.4798, time: 241.57ms\n","iter 2478: loss 2.3204, time: 242.17ms\n","iter 2479: loss 1.9356, time: 242.88ms\n","iter 2480: loss nan, time: 242.11ms\n","iter 2481: loss 2.0812, time: 242.55ms\n","iter 2482: loss 2.2843, time: 242.17ms\n","iter 2483: loss 2.6544, time: 242.93ms\n","iter 2484: loss 2.2598, time: 242.30ms\n","iter 2485: loss 2.3607, time: 242.59ms\n","iter 2486: loss 2.2385, time: 241.82ms\n","iter 2487: loss 3.7814, time: 242.10ms\n","iter 2488: loss 2.3194, time: 242.82ms\n","iter 2489: loss 2.1858, time: 242.67ms\n","iter 2490: loss nan, time: 243.79ms\n","iter 2491: loss 2.4877, time: 242.51ms\n","iter 2492: loss 2.7667, time: 243.03ms\n","iter 2493: loss nan, time: 243.31ms\n","iter 2494: loss 3.3164, time: 243.86ms\n","iter 2495: loss 2.4684, time: 246.94ms\n","iter 2496: loss 2.1736, time: 242.43ms\n","iter 2497: loss 1.5443, time: 243.85ms\n","iter 2498: loss 3.4838, time: 244.07ms\n","iter 2499: loss 1.9010, time: 247.92ms\n","iter 2500: loss 2.1107, time: 243.29ms\n","iter 2501: loss 1.3252, time: 247.52ms\n","iter 2502: loss nan, time: 244.08ms\n","iter 2503: loss 3.0004, time: 244.13ms\n","iter 2504: loss 2.0599, time: 243.32ms\n","iter 2505: loss 2.5393, time: 242.39ms\n","iter 2506: loss 2.4978, time: 242.22ms\n","iter 2507: loss 2.0320, time: 241.52ms\n","iter 2508: loss 2.3237, time: 241.90ms\n","iter 2509: loss 3.2131, time: 242.14ms\n","iter 2510: loss 1.4830, time: 241.42ms\n","iter 2511: loss 2.1401, time: 241.64ms\n","iter 2512: loss 2.6562, time: 241.65ms\n","iter 2513: loss 1.7195, time: 241.61ms\n","iter 2514: loss 1.5259, time: 242.02ms\n","iter 2515: loss 2.0932, time: 242.10ms\n","iter 2516: loss 3.4147, time: 242.33ms\n","iter 2517: loss 1.2583, time: 241.23ms\n","iter 2518: loss 2.6298, time: 242.48ms\n","iter 2519: loss 1.4696, time: 243.87ms\n","iter 2520: loss 2.3475, time: 242.41ms\n","iter 2521: loss 2.5036, time: 241.85ms\n","iter 2522: loss 2.2405, time: 241.69ms\n","iter 2523: loss 2.4970, time: 242.40ms\n","iter 2524: loss 2.0907, time: 241.52ms\n","iter 2525: loss 2.2081, time: 242.29ms\n","iter 2526: loss 1.1661, time: 242.28ms\n","iter 2527: loss 1.7266, time: 244.38ms\n","iter 2528: loss 2.4623, time: 240.76ms\n","iter 2529: loss 2.1929, time: 242.71ms\n","iter 2530: loss 2.0528, time: 242.10ms\n","iter 2531: loss 2.7657, time: 242.27ms\n","iter 2532: loss 2.3846, time: 241.13ms\n","iter 2533: loss 2.7411, time: 241.36ms\n","iter 2534: loss 1.8780, time: 242.32ms\n","iter 2535: loss nan, time: 242.61ms\n","iter 2536: loss 3.8310, time: 242.13ms\n","iter 2537: loss nan, time: 242.49ms\n","iter 2538: loss 2.2259, time: 242.16ms\n","iter 2539: loss 2.4712, time: 242.31ms\n","iter 2540: loss nan, time: 241.60ms\n","iter 2541: loss 3.0762, time: 240.27ms\n","iter 2542: loss 2.1121, time: 241.38ms\n","iter 2543: loss 1.4200, time: 241.09ms\n","iter 2544: loss 1.6552, time: 242.24ms\n","iter 2545: loss 3.0596, time: 241.47ms\n","iter 2546: loss 2.6215, time: 242.54ms\n","iter 2547: loss 1.6151, time: 242.18ms\n","iter 2548: loss 4.1681, time: 241.36ms\n","iter 2549: loss 2.5798, time: 241.11ms\n","iter 2550: loss 2.3281, time: 242.11ms\n","iter 2551: loss 1.5348, time: 242.39ms\n","iter 2552: loss nan, time: 242.12ms\n","iter 2553: loss 2.5737, time: 241.71ms\n","iter 2554: loss 1.2548, time: 241.08ms\n","iter 2555: loss 1.8269, time: 241.90ms\n","iter 2556: loss 2.2443, time: 241.67ms\n","iter 2557: loss 2.4122, time: 242.23ms\n","iter 2558: loss nan, time: 241.43ms\n","iter 2559: loss 1.4838, time: 244.58ms\n","iter 2560: loss nan, time: 242.21ms\n","iter 2561: loss 1.3814, time: 246.40ms\n","iter 2562: loss 1.0085, time: 243.46ms\n","iter 2563: loss 2.7354, time: 241.31ms\n","iter 2564: loss nan, time: 241.74ms\n","iter 2565: loss 2.2954, time: 241.16ms\n","iter 2566: loss nan, time: 240.87ms\n","iter 2567: loss 2.4233, time: 242.08ms\n","iter 2568: loss 3.6278, time: 240.95ms\n","iter 2569: loss 2.2802, time: 241.31ms\n","iter 2570: loss 4.4526, time: 241.32ms\n","iter 2571: loss 2.3852, time: 241.72ms\n","iter 2572: loss 2.8810, time: 241.81ms\n","iter 2573: loss 4.6242, time: 241.14ms\n","iter 2574: loss 2.3796, time: 240.85ms\n","iter 2575: loss 4.1474, time: 241.89ms\n","iter 2576: loss 4.6942, time: 241.16ms\n","iter 2577: loss 2.7165, time: 241.67ms\n","iter 2578: loss 1.9342, time: 240.48ms\n","iter 2579: loss 3.1818, time: 241.25ms\n","iter 2580: loss 1.7437, time: 241.73ms\n","iter 2581: loss 1.2564, time: 241.33ms\n","iter 2582: loss 1.2845, time: 241.20ms\n","iter 2583: loss 1.9856, time: 241.59ms\n","iter 2584: loss 1.0605, time: 242.52ms\n","iter 2585: loss 2.0738, time: 243.11ms\n","iter 2586: loss 5.0291, time: 241.22ms\n","iter 2587: loss nan, time: 242.13ms\n","iter 2588: loss nan, time: 242.89ms\n","iter 2589: loss 2.4780, time: 242.01ms\n","iter 2590: loss nan, time: 240.92ms\n","iter 2591: loss 3.1243, time: 244.71ms\n","iter 2592: loss 1.6970, time: 248.62ms\n","iter 2593: loss 3.1796, time: 241.58ms\n","iter 2594: loss 1.7207, time: 242.55ms\n","iter 2595: loss 3.3522, time: 240.99ms\n","iter 2596: loss nan, time: 241.63ms\n","iter 2597: loss 2.1162, time: 242.32ms\n","iter 2598: loss 2.1552, time: 240.63ms\n","iter 2599: loss 2.0266, time: 240.69ms\n","iter 2600: loss 2.2893, time: 241.32ms\n","iter 2601: loss 1.9056, time: 219.80ms\n","iter 2602: loss 1.4829, time: 242.79ms\n","iter 2603: loss 2.2307, time: 243.54ms\n","iter 2604: loss 1.6299, time: 246.58ms\n","iter 2605: loss nan, time: 242.94ms\n","iter 2606: loss 2.4019, time: 243.20ms\n","iter 2607: loss 1.9227, time: 242.71ms\n","iter 2608: loss 1.1322, time: 242.60ms\n","iter 2609: loss 1.9639, time: 242.60ms\n","iter 2610: loss 1.3661, time: 241.35ms\n","iter 2611: loss 2.7130, time: 242.49ms\n","iter 2612: loss 1.3632, time: 243.35ms\n","iter 2613: loss 2.2272, time: 249.11ms\n","iter 2614: loss 1.6711, time: 246.32ms\n","iter 2615: loss 1.5941, time: 244.08ms\n","iter 2616: loss nan, time: 244.31ms\n","iter 2617: loss 0.9123, time: 247.85ms\n","iter 2618: loss 2.4410, time: 244.51ms\n","iter 2619: loss 2.5681, time: 241.61ms\n","iter 2620: loss 1.4921, time: 241.78ms\n","iter 2621: loss 1.0931, time: 241.62ms\n","iter 2622: loss 2.4724, time: 242.30ms\n","iter 2623: loss 2.8252, time: 243.75ms\n","iter 2624: loss 1.7956, time: 240.77ms\n","iter 2625: loss 1.7902, time: 242.22ms\n","iter 2626: loss 2.1167, time: 241.72ms\n","iter 2627: loss 3.7979, time: 242.15ms\n","iter 2628: loss 1.4354, time: 219.24ms\n","iter 2629: loss 2.0719, time: 241.16ms\n","iter 2630: loss 2.6170, time: 242.18ms\n","iter 2631: loss 1.9626, time: 242.01ms\n","iter 2632: loss 3.1480, time: 241.97ms\n","iter 2633: loss 1.3944, time: 243.73ms\n","iter 2634: loss nan, time: 241.88ms\n","iter 2635: loss nan, time: 241.99ms\n","iter 2636: loss 2.6470, time: 242.05ms\n","iter 2637: loss 2.1726, time: 241.60ms\n","iter 2638: loss 2.3920, time: 242.05ms\n","iter 2639: loss 0.6994, time: 241.76ms\n","iter 2640: loss 2.5530, time: 240.80ms\n","iter 2641: loss 2.7654, time: 241.58ms\n","iter 2642: loss nan, time: 241.99ms\n","iter 2643: loss 2.6491, time: 242.47ms\n","iter 2644: loss 2.2730, time: 241.85ms\n","iter 2645: loss 1.3067, time: 242.04ms\n","iter 2646: loss 2.8046, time: 241.64ms\n","iter 2647: loss 1.6679, time: 242.86ms\n","iter 2648: loss nan, time: 241.20ms\n","iter 2649: loss nan, time: 241.58ms\n","iter 2650: loss 2.2885, time: 242.17ms\n","iter 2651: loss 3.2066, time: 241.98ms\n","iter 2652: loss 1.7519, time: 240.81ms\n","iter 2653: loss 1.2826, time: 241.45ms\n","iter 2654: loss 2.2727, time: 241.09ms\n","iter 2655: loss 2.3382, time: 244.07ms\n","iter 2656: loss 2.7405, time: 240.22ms\n","iter 2657: loss 1.8217, time: 240.74ms\n","iter 2658: loss 1.5080, time: 242.43ms\n","iter 2659: loss 2.1684, time: 242.37ms\n","iter 2660: loss 1.5349, time: 244.50ms\n","iter 2661: loss 2.1946, time: 243.28ms\n","iter 2662: loss nan, time: 244.53ms\n","iter 2663: loss nan, time: 244.51ms\n","iter 2664: loss 1.3497, time: 243.73ms\n","iter 2665: loss 1.9990, time: 242.70ms\n","iter 2666: loss 2.2252, time: 243.21ms\n","iter 2667: loss 2.4506, time: 243.85ms\n","iter 2668: loss 1.3950, time: 243.90ms\n","iter 2669: loss 1.5126, time: 242.67ms\n","iter 2670: loss 2.0738, time: 244.22ms\n","iter 2671: loss 1.5218, time: 242.91ms\n","iter 2672: loss 3.7421, time: 243.54ms\n","iter 2673: loss 2.1848, time: 243.44ms\n","iter 2674: loss 1.3688, time: 244.16ms\n","iter 2675: loss 4.4831, time: 244.29ms\n","iter 2676: loss 3.3345, time: 242.30ms\n","iter 2677: loss 1.7695, time: 243.01ms\n","iter 2678: loss nan, time: 242.54ms\n","iter 2679: loss 0.8207, time: 243.12ms\n","iter 2680: loss 2.7447, time: 243.11ms\n","iter 2681: loss 1.5492, time: 241.89ms\n","iter 2682: loss 2.1721, time: 241.90ms\n","iter 2683: loss 1.6393, time: 241.68ms\n","iter 2684: loss 2.2189, time: 242.52ms\n","iter 2685: loss 1.3782, time: 241.61ms\n","iter 2686: loss 2.6634, time: 242.58ms\n","iter 2687: loss 1.8412, time: 244.51ms\n","iter 2688: loss 1.5472, time: 242.57ms\n","iter 2689: loss 2.3080, time: 242.75ms\n","iter 2690: loss 2.3100, time: 241.41ms\n","iter 2691: loss 1.2406, time: 241.55ms\n","iter 2692: loss 1.7053, time: 242.81ms\n","iter 2693: loss 2.3228, time: 242.79ms\n","iter 2694: loss 1.4676, time: 242.10ms\n","iter 2695: loss nan, time: 242.54ms\n","iter 2696: loss 1.6643, time: 242.58ms\n","iter 2697: loss 1.9609, time: 241.96ms\n","iter 2698: loss 1.1274, time: 242.02ms\n","iter 2699: loss 2.1407, time: 242.67ms\n","iter 2700: loss 1.3808, time: 242.67ms\n","iter 2701: loss 3.4229, time: 241.85ms\n","iter 2702: loss 2.2447, time: 241.53ms\n","iter 2703: loss 2.5805, time: 240.99ms\n","iter 2704: loss nan, time: 241.77ms\n","iter 2705: loss 1.9772, time: 242.85ms\n","iter 2706: loss 2.7445, time: 243.09ms\n","iter 2707: loss 2.4006, time: 242.21ms\n","iter 2708: loss 2.0397, time: 242.79ms\n","iter 2709: loss 1.8208, time: 243.26ms\n","iter 2710: loss 1.5283, time: 241.92ms\n","iter 2711: loss 1.7149, time: 248.86ms\n","iter 2712: loss 1.7169, time: 241.50ms\n","iter 2713: loss 2.2160, time: 242.81ms\n","iter 2714: loss 3.6186, time: 241.91ms\n","iter 2715: loss 1.7224, time: 242.33ms\n","iter 2716: loss 2.6183, time: 243.60ms\n","iter 2717: loss 1.4900, time: 244.03ms\n","iter 2718: loss 2.1766, time: 243.83ms\n","iter 2719: loss 1.1606, time: 245.72ms\n","iter 2720: loss 2.3782, time: 241.82ms\n","iter 2721: loss 2.9211, time: 244.61ms\n","iter 2722: loss 5.2293, time: 241.62ms\n","iter 2723: loss 1.5304, time: 241.54ms\n","iter 2724: loss 3.3834, time: 241.53ms\n","iter 2725: loss nan, time: 242.34ms\n","iter 2726: loss 2.6356, time: 242.63ms\n","iter 2727: loss 2.4528, time: 241.84ms\n","iter 2728: loss 2.6797, time: 243.88ms\n","iter 2729: loss 2.3355, time: 244.55ms\n","iter 2730: loss 3.0830, time: 243.54ms\n","iter 2731: loss nan, time: 242.27ms\n","iter 2732: loss 2.2911, time: 241.13ms\n","iter 2733: loss 1.8028, time: 241.67ms\n","iter 2734: loss 2.3061, time: 241.37ms\n","iter 2735: loss 1.4723, time: 241.98ms\n","iter 2736: loss 1.8446, time: 241.82ms\n","iter 2737: loss 1.6886, time: 241.34ms\n","iter 2738: loss 2.4245, time: 242.02ms\n","iter 2739: loss 2.3165, time: 241.56ms\n","iter 2740: loss 3.0358, time: 241.38ms\n","iter 2741: loss nan, time: 242.22ms\n","iter 2742: loss nan, time: 243.40ms\n","iter 2743: loss 2.7429, time: 242.29ms\n","iter 2744: loss 2.2714, time: 241.34ms\n","iter 2745: loss 2.5862, time: 242.22ms\n","iter 2746: loss 2.7208, time: 242.45ms\n","iter 2747: loss 1.5401, time: 241.77ms\n","iter 2748: loss 2.5319, time: 242.18ms\n","iter 2749: loss 1.8159, time: 242.13ms\n","iter 2750: loss 2.1709, time: 242.61ms\n","iter 2751: loss 2.7366, time: 245.22ms\n","iter 2752: loss 3.7509, time: 241.89ms\n","iter 2753: loss 1.4320, time: 242.17ms\n","iter 2754: loss 3.5764, time: 242.30ms\n","iter 2755: loss 1.9263, time: 243.00ms\n","iter 2756: loss 2.9438, time: 241.50ms\n","iter 2757: loss 0.8802, time: 241.84ms\n","iter 2758: loss 2.9997, time: 242.12ms\n","iter 2759: loss 1.3067, time: 241.07ms\n","iter 2760: loss 2.8955, time: 241.66ms\n","iter 2761: loss 2.2026, time: 241.28ms\n","iter 2762: loss 1.5385, time: 242.18ms\n","iter 2763: loss 1.8605, time: 243.88ms\n","iter 2764: loss 1.5145, time: 241.35ms\n","iter 2765: loss 1.5898, time: 242.62ms\n","iter 2766: loss 2.7182, time: 241.77ms\n","iter 2767: loss 2.1525, time: 241.87ms\n","iter 2768: loss 2.0500, time: 243.17ms\n","iter 2769: loss 3.3225, time: 243.00ms\n","iter 2770: loss nan, time: 241.79ms\n","iter 2771: loss nan, time: 241.37ms\n","iter 2772: loss 2.8120, time: 243.01ms\n","iter 2773: loss 2.2199, time: 244.11ms\n","iter 2774: loss 2.3022, time: 242.07ms\n","iter 2775: loss 1.5993, time: 241.38ms\n","iter 2776: loss 2.5572, time: 245.91ms\n","iter 2777: loss 1.5658, time: 242.08ms\n","iter 2778: loss 1.3013, time: 242.24ms\n","iter 2779: loss 2.3883, time: 242.47ms\n","iter 2780: loss 1.6461, time: 241.17ms\n","iter 2781: loss 2.9090, time: 242.03ms\n","iter 2782: loss nan, time: 243.34ms\n","iter 2783: loss nan, time: 245.54ms\n","iter 2784: loss 1.9208, time: 241.69ms\n","iter 2785: loss 2.1926, time: 243.53ms\n","iter 2786: loss 3.4221, time: 242.72ms\n","iter 2787: loss 3.3729, time: 243.70ms\n","iter 2788: loss 2.4009, time: 242.39ms\n","iter 2789: loss 2.1805, time: 242.23ms\n","iter 2790: loss 1.0833, time: 241.45ms\n","iter 2791: loss 2.1866, time: 242.66ms\n","iter 2792: loss 2.4468, time: 241.22ms\n","iter 2793: loss 3.6842, time: 241.45ms\n","iter 2794: loss 1.1802, time: 241.90ms\n","iter 2795: loss 2.5806, time: 242.11ms\n","iter 2796: loss 1.6445, time: 242.18ms\n","iter 2797: loss 1.6582, time: 242.49ms\n","iter 2798: loss 2.8538, time: 241.94ms\n","iter 2799: loss 1.9567, time: 243.15ms\n","iter 2800: loss 2.9540, time: 241.77ms\n","iter 2801: loss 1.9256, time: 241.98ms\n","iter 2802: loss 2.2233, time: 242.00ms\n","iter 2803: loss 1.0497, time: 242.40ms\n","iter 2804: loss nan, time: 242.13ms\n","iter 2805: loss 1.0504, time: 242.61ms\n","iter 2806: loss 2.3930, time: 243.02ms\n","iter 2807: loss 1.6824, time: 242.42ms\n","iter 2808: loss 1.5005, time: 243.19ms\n","iter 2809: loss 1.4381, time: 242.38ms\n","iter 2810: loss nan, time: 241.55ms\n","iter 2811: loss 1.6612, time: 241.75ms\n","iter 2812: loss 2.9375, time: 241.90ms\n","iter 2813: loss 2.0003, time: 241.33ms\n","iter 2814: loss 1.1320, time: 241.78ms\n","iter 2815: loss 2.6466, time: 243.76ms\n","iter 2816: loss 2.9030, time: 241.79ms\n","iter 2817: loss 2.4207, time: 240.78ms\n","iter 2818: loss 3.0662, time: 241.59ms\n","iter 2819: loss 2.2990, time: 248.40ms\n","iter 2820: loss 2.0420, time: 241.93ms\n","iter 2821: loss nan, time: 241.24ms\n","iter 2822: loss nan, time: 241.29ms\n","iter 2823: loss 1.9566, time: 241.80ms\n","iter 2824: loss 5.2201, time: 242.09ms\n","iter 2825: loss 1.3546, time: 241.10ms\n","iter 2826: loss 2.2430, time: 241.24ms\n","iter 2827: loss 2.1733, time: 215.87ms\n","iter 2828: loss 1.4696, time: 242.46ms\n","iter 2829: loss 1.5024, time: 242.10ms\n","iter 2830: loss 2.0575, time: 243.80ms\n","iter 2831: loss 2.0074, time: 241.78ms\n","iter 2832: loss 1.1722, time: 242.38ms\n","iter 2833: loss 2.0323, time: 245.98ms\n","iter 2834: loss 2.6716, time: 244.61ms\n","iter 2835: loss 2.0503, time: 243.78ms\n","iter 2836: loss 2.6591, time: 243.48ms\n","iter 2837: loss 1.5859, time: 241.76ms\n","iter 2838: loss 2.4077, time: 243.95ms\n","iter 2839: loss 3.2713, time: 245.03ms\n","iter 2840: loss 2.6040, time: 244.12ms\n","iter 2841: loss 1.5756, time: 244.64ms\n","iter 2842: loss nan, time: 244.03ms\n","iter 2843: loss 1.9217, time: 242.90ms\n","iter 2844: loss nan, time: 242.60ms\n","iter 2845: loss 2.2417, time: 243.46ms\n","iter 2846: loss 2.6594, time: 241.23ms\n","iter 2847: loss 2.3290, time: 244.25ms\n","iter 2848: loss 2.3261, time: 240.95ms\n","iter 2849: loss 0.8738, time: 242.05ms\n","iter 2850: loss 2.9439, time: 242.15ms\n","iter 2851: loss 1.8271, time: 241.97ms\n","iter 2852: loss 1.9898, time: 242.86ms\n","iter 2853: loss 2.4570, time: 242.64ms\n","iter 2854: loss 1.7110, time: 241.20ms\n","iter 2855: loss 2.8884, time: 241.91ms\n","iter 2856: loss 1.4435, time: 242.52ms\n","iter 2857: loss 2.2928, time: 242.09ms\n","iter 2858: loss 2.0200, time: 241.57ms\n","iter 2859: loss 1.1320, time: 241.58ms\n","iter 2860: loss 2.4447, time: 242.71ms\n","iter 2861: loss 2.0164, time: 242.39ms\n","iter 2862: loss 2.5680, time: 241.34ms\n","iter 2863: loss 2.4903, time: 241.83ms\n","iter 2864: loss nan, time: 241.94ms\n","iter 2865: loss 1.8847, time: 242.66ms\n","iter 2866: loss 5.3962, time: 241.82ms\n","iter 2867: loss 2.3755, time: 242.28ms\n","iter 2868: loss 5.0132, time: 242.21ms\n","iter 2869: loss 1.7765, time: 241.58ms\n","iter 2870: loss 2.5574, time: 241.27ms\n","iter 2871: loss 2.0531, time: 240.98ms\n","iter 2872: loss 2.5850, time: 240.89ms\n","iter 2873: loss 2.0587, time: 241.69ms\n","iter 2874: loss 2.3142, time: 241.68ms\n","iter 2875: loss 3.3210, time: 241.01ms\n","iter 2876: loss 1.7667, time: 241.30ms\n","iter 2877: loss 1.7701, time: 242.23ms\n","iter 2878: loss 1.7704, time: 242.51ms\n","iter 2879: loss 1.8730, time: 243.26ms\n","iter 2880: loss nan, time: 240.97ms\n","iter 2881: loss 1.1264, time: 241.96ms\n","iter 2882: loss 2.2018, time: 241.64ms\n","iter 2883: loss 1.9173, time: 240.97ms\n","iter 2884: loss 1.9311, time: 240.74ms\n","iter 2885: loss 2.3757, time: 243.07ms\n","iter 2886: loss 2.3277, time: 242.53ms\n","iter 2887: loss 3.4275, time: 242.15ms\n","iter 2888: loss 1.6729, time: 241.59ms\n","iter 2889: loss 1.0480, time: 242.12ms\n","iter 2890: loss 2.7153, time: 242.34ms\n","iter 2891: loss 2.1229, time: 242.16ms\n","iter 2892: loss 2.5936, time: 242.73ms\n","iter 2893: loss nan, time: 243.33ms\n","iter 2894: loss 2.8620, time: 243.94ms\n","iter 2895: loss 1.6025, time: 242.78ms\n","iter 2896: loss nan, time: 243.11ms\n","iter 2897: loss 3.1845, time: 243.15ms\n","iter 2898: loss 2.2380, time: 245.12ms\n","iter 2899: loss 2.4692, time: 242.71ms\n","iter 2900: loss 3.7851, time: 242.49ms\n","iter 2901: loss 1.1320, time: 243.20ms\n","iter 2902: loss 1.9355, time: 242.91ms\n","iter 2903: loss 3.7460, time: 242.65ms\n","iter 2904: loss 3.8519, time: 242.27ms\n","iter 2905: loss 2.0478, time: 242.99ms\n","iter 2906: loss 1.3391, time: 242.84ms\n","iter 2907: loss 0.9468, time: 242.57ms\n","iter 2908: loss 3.2756, time: 241.72ms\n","iter 2909: loss 2.8079, time: 242.25ms\n","iter 2910: loss 2.4772, time: 242.73ms\n","iter 2911: loss 5.2585, time: 245.95ms\n","iter 2912: loss 1.6712, time: 241.36ms\n","iter 2913: loss 1.7300, time: 242.45ms\n","iter 2914: loss 2.2136, time: 241.65ms\n","iter 2915: loss 1.3323, time: 242.18ms\n","iter 2916: loss 2.9739, time: 240.70ms\n","iter 2917: loss 2.0895, time: 240.06ms\n","iter 2918: loss 1.7211, time: 240.59ms\n","iter 2919: loss 2.1283, time: 242.03ms\n","iter 2920: loss 1.7605, time: 240.48ms\n","iter 2921: loss 1.9684, time: 242.66ms\n","iter 2922: loss 1.9250, time: 241.94ms\n","iter 2923: loss 0.8324, time: 243.08ms\n","iter 2924: loss 1.2996, time: 241.20ms\n","iter 2925: loss 2.3604, time: 241.05ms\n","iter 2926: loss 2.0332, time: 242.36ms\n","iter 2927: loss 3.6301, time: 242.69ms\n","iter 2928: loss 1.4437, time: 241.72ms\n","iter 2929: loss nan, time: 241.88ms\n","iter 2930: loss nan, time: 241.74ms\n","iter 2931: loss 1.8219, time: 243.02ms\n","iter 2932: loss 1.8792, time: 241.67ms\n","iter 2933: loss 2.0116, time: 241.68ms\n","iter 2934: loss 3.0359, time: 242.32ms\n","iter 2935: loss 2.0306, time: 242.14ms\n","iter 2936: loss 4.6170, time: 242.08ms\n","iter 2937: loss 2.6585, time: 241.68ms\n","iter 2938: loss 2.1155, time: 242.58ms\n","iter 2939: loss 1.7535, time: 242.20ms\n","iter 2940: loss 2.4403, time: 242.82ms\n","iter 2941: loss 1.4322, time: 242.96ms\n","iter 2942: loss 2.5259, time: 242.17ms\n","iter 2943: loss 2.7799, time: 248.86ms\n","iter 2944: loss 1.5081, time: 246.42ms\n","iter 2945: loss nan, time: 241.79ms\n","iter 2946: loss nan, time: 242.44ms\n","iter 2947: loss 1.5043, time: 242.29ms\n","iter 2948: loss 1.8433, time: 242.81ms\n","iter 2949: loss nan, time: 245.26ms\n","iter 2950: loss 2.2184, time: 242.75ms\n","iter 2951: loss 2.5599, time: 244.11ms\n","iter 2952: loss 3.7233, time: 243.76ms\n","iter 2953: loss 1.7358, time: 249.28ms\n","iter 2954: loss nan, time: 248.69ms\n","iter 2955: loss 1.7418, time: 242.19ms\n","iter 2956: loss nan, time: 243.86ms\n","iter 2957: loss 2.0497, time: 243.11ms\n","iter 2958: loss 1.8293, time: 243.74ms\n","iter 2959: loss 2.0161, time: 243.33ms\n","iter 2960: loss 2.0876, time: 242.46ms\n","iter 2961: loss 3.3609, time: 241.95ms\n","iter 2962: loss 2.4599, time: 241.80ms\n","iter 2963: loss 2.4989, time: 233.00ms\n","iter 2964: loss 1.7971, time: 242.41ms\n","iter 2965: loss 1.5856, time: 243.99ms\n","iter 2966: loss 2.2161, time: 241.96ms\n","iter 2967: loss 2.5442, time: 242.67ms\n","iter 2968: loss 1.0726, time: 242.19ms\n","iter 2969: loss 2.2531, time: 242.31ms\n","iter 2970: loss 2.4557, time: 242.10ms\n","iter 2971: loss 2.0436, time: 241.18ms\n","iter 2972: loss 1.5623, time: 241.96ms\n","iter 2973: loss 2.0679, time: 242.18ms\n","iter 2974: loss nan, time: 242.24ms\n","iter 2975: loss 2.8496, time: 244.79ms\n","iter 2976: loss 1.6202, time: 241.75ms\n","iter 2977: loss 2.0861, time: 241.87ms\n","iter 2978: loss 1.7856, time: 241.99ms\n","iter 2979: loss 1.4472, time: 241.82ms\n","iter 2980: loss 0.8136, time: 241.33ms\n","iter 2981: loss 2.4985, time: 242.34ms\n","iter 2982: loss 1.7868, time: 241.02ms\n","iter 2983: loss 1.9922, time: 241.26ms\n","iter 2984: loss nan, time: 241.68ms\n","iter 2985: loss 2.4837, time: 242.37ms\n","iter 2986: loss 2.4868, time: 241.71ms\n","iter 2987: loss 1.7922, time: 241.14ms\n","iter 2988: loss 1.3596, time: 241.61ms\n","iter 2989: loss 1.8138, time: 241.52ms\n","iter 2990: loss 1.2697, time: 241.75ms\n","iter 2991: loss 2.6246, time: 241.73ms\n","iter 2992: loss 1.6034, time: 240.69ms\n","iter 2993: loss 1.1242, time: 241.93ms\n","iter 2994: loss 1.9310, time: 241.63ms\n","iter 2995: loss 2.5510, time: 239.55ms\n","iter 2996: loss 1.4069, time: 242.31ms\n","iter 2997: loss nan, time: 241.25ms\n","iter 2998: loss 2.2891, time: 242.65ms\n","iter 2999: loss 2.3338, time: 241.97ms\n","iter 3000: loss 1.4181, time: 243.54ms\n","iter 3001: loss 2.1133, time: 249.68ms\n","iter 3002: loss 2.4674, time: 248.34ms\n","iter 3003: loss 2.0301, time: 243.04ms\n","iter 3004: loss 2.2264, time: 242.80ms\n","iter 3005: loss 2.2398, time: 242.71ms\n","iter 3006: loss 1.5573, time: 243.46ms\n","iter 3007: loss 3.2144, time: 244.34ms\n","iter 3008: loss 2.1712, time: 241.70ms\n","iter 3009: loss 1.8270, time: 242.60ms\n","iter 3010: loss 2.4927, time: 243.65ms\n","iter 3011: loss 2.4332, time: 243.32ms\n","iter 3012: loss 1.0283, time: 243.21ms\n","iter 3013: loss 1.0618, time: 243.21ms\n","iter 3014: loss 2.5277, time: 242.91ms\n","iter 3015: loss 2.7169, time: 243.06ms\n","iter 3016: loss nan, time: 242.57ms\n","iter 3017: loss 3.3986, time: 242.84ms\n","iter 3018: loss 1.7379, time: 243.72ms\n","iter 3019: loss 3.2164, time: 243.48ms\n","iter 3020: loss 2.8348, time: 243.48ms\n","iter 3021: loss 2.6666, time: 242.55ms\n","iter 3022: loss 1.7205, time: 243.16ms\n","iter 3023: loss 1.0392, time: 241.19ms\n","iter 3024: loss nan, time: 242.15ms\n","iter 3025: loss 2.3733, time: 242.01ms\n","iter 3026: loss 2.0403, time: 242.23ms\n","iter 3027: loss 1.7612, time: 242.28ms\n","iter 3028: loss 1.5941, time: 241.05ms\n","iter 3029: loss 3.2672, time: 241.74ms\n","iter 3030: loss 0.6844, time: 241.91ms\n","iter 3031: loss 2.4590, time: 241.46ms\n","iter 3032: loss 2.1893, time: 241.85ms\n","iter 3033: loss 1.8743, time: 243.05ms\n","iter 3034: loss 1.8729, time: 242.24ms\n","iter 3035: loss 2.0162, time: 242.26ms\n"]}]},{"cell_type":"markdown","source":["## Evaluating and generating titles after the Fine-Tuning\n","\n","Firstly: computation of ROUGE Scores"],"metadata":{"id":"80Hh9JxsXQPL"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","\n","def postprocess_text(text_sequence):   ##, labels):\n","\n","    \"\"\"\n","    Post-processing to prepare inputs to the ROGUE functions\n","    \"\"\"\n","\n","    text_sequence = [a.strip() for a in text_sequence]\n","\n","    # ROUGE expects a newline after each sentence\n","    text_sequence = [\"\\n\".join(nltk.sent_tokenize(a)) for a in text_sequence]\n","\n","    return text_sequence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3nk59yvBlPs","executionInfo":{"status":"ok","timestamp":1686388243428,"user_tz":-120,"elapsed":2918,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"7902b5b8-f709-42a2-86eb-48cebe7b1b7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["!mkdir out"],"metadata":{"id":"UXPNP2QdyZpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import torch\n","\n","model_ft = torch.load(\"/content/gdrive/My Drive/Thesis/Models/lit-llama-lora-finetuned.pth\")\n","torch.save(model_ft, \"/content/out/lit-llama-adapter-finetuned.pth\")\n"],"metadata":{"id":"AyPX0AqKXPuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sys.path.append(\"/content/lit-llama\")\n","\n","\n","import lightning as L\n","import torch\n","from generate import generate\n","from lit_llama import Tokenizer\n","from lit_llama.adapter import LLaMA\n","from lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\n","from scripts.prepare_alpaca import generate_prompt"],"metadata":{"id":"m1Lh_DNB8VGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["adapter_path =\"/content/out/lit-llama-adapter-finetuned.pth\"\n","pretrained_path = \"/content/checkpoints/lit-llama/7B/lit-llama.pth\"\n","tokenizer_path = \"/content/checkpoints/lit-llama/tokenizer.model\"\n","\n","\n","fabric = L.Fabric(devices=1)\n","dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n"],"metadata":{"id":"0iDfLRpU4A_p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following function runs slowly, but sadly is needed to call for every generation ...."],"metadata":{"id":"V3txi9vMMyM2"}},{"cell_type":"code","source":["def gen_title(pretrained_path, adapter_path, sample, max_new_tokens, top_k, temperature):\n","\n","  \"\"\" Expects sample as set up above / like in the .pt data \"\"\"\n","\n","  with lazy_load(pretrained_path) as pretrained_checkpoint, lazy_load(adapter_path) as adapter_checkpoint:\n","      name = llama_model_lookup(pretrained_checkpoint)\n","\n","      with EmptyInitOnDevice(\n","              device=fabric.device, dtype=dtype, quantization_mode = \"llm.int8\"\n","      ):\n","      #   quantization\n","          model = LLaMA.from_name(name)\n","\n","\n","      # 1. Load the pretrained weights\n","      model.load_state_dict(pretrained_checkpoint, strict=False)\n","      # 2. Load the fine-tuned adapter weights\n","      model.load_state_dict(adapter_checkpoint, strict=False)\n","\n","\n","  model.eval()\n","  model = fabric.setup_module(model)\n","\n","  tokenizer = Tokenizer(tokenizer_path)\n","\n","  prompt = generate_prompt(sample)\n","  encoded = tokenizer.encode(prompt, bos=True, eos=False, device=model.device)\n","  prompt_length = encoded.size(0)\n","\n","  y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n","\n","  output = tokenizer.decode(y)\n","  output = output.split(\"### Response:\")[1].strip()\n","\n","  return output\n"],"metadata":{"id":"s1WQqwMy6_MF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## rouge with  jusst 1000 samples\n","\n","test_set_ = test_set[:1000]\n"],"metadata":{"id":"MKbnW_OGLO4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## looping through eval/test samples\n","\n","gen_titles_eval = []\n","\n","for i in range(len(test_set_)):\n","  sample_ = test_set_[i]\n","  print(i/len(test_set_))\n","  gen_title_new = gen_title(pretrained_path, adapter_path, sample_, 30, 200, 0.2)\n","  gen_titles_eval.append(gen_title_new)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grNgND_08xCS","outputId":"a40017b1-c348-47a1-e675-c041ce904f9c","executionInfo":{"status":"ok","timestamp":1686391815336,"user_tz":-120,"elapsed":675632,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.006666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.013333333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.02\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.02666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.03333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.04\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.04666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.05333333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.06\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.06666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.07333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.08\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.08666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.09333333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.1\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.10666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.11333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.12\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.12666666666666668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.13333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.14\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.14666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.15333333333333332\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.16\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.16666666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.17333333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.18\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.18666666666666668\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.19333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.2\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.20666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.21333333333333335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.22\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.22666666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.23333333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.24\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.24666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.25333333333333335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.26\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.26666666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.2733333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.28\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.2866666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.29333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.3\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.30666666666666664\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.31333333333333335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.32\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.32666666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.3333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.34\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.3466666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.35333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.36\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.36666666666666664\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.37333333333333335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.38\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.38666666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.3933333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.4\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.4066666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.41333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.42\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.4266666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.43333333333333335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.44\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.44666666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.4533333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.46\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.4666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.47333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.48\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.4866666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.49333333333333335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5066666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5133333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.52\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5266666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.54\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5466666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5533333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.56\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5733333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.58\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5866666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.5933333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6066666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6133333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.62\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6266666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.64\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6466666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6533333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.66\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6666666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6733333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.68\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6866666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.6933333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7066666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7133333333333334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.72\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7266666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7333333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.74\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7466666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7533333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.76\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7666666666666667\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7733333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.78\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7866666666666666\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.7933333333333333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["0.8\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8066666666666666\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8133333333333334\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.82\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8266666666666667\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8333333333333334\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.84\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8466666666666667\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8533333333333334\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.86\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8666666666666667\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8733333333333333\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.88\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8866666666666667\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.8933333333333333\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9066666666666666\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9133333333333333\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.92\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9266666666666666\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9333333333333333\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.94\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9466666666666667\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9533333333333334\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.96\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9666666666666667\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9733333333333334\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.98\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9866666666666667\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]},{"output_type":"stream","name":"stdout","text":["0.9933333333333333\n"]},{"output_type":"stream","name":"stderr","text":["INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"]}]},{"cell_type":"code","source":["## examining the generated titles in evaluation set!\n","\n","\n","for i in range(10):\n","  print(f\"True title: {test_set[i]['output']}\")\n","  print(f\"Generated title: {gen_titles_eval[i]}\")\n","  print(\"---------------------------------\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1G7Eu62aqesN","executionInfo":{"status":"ok","timestamp":1686391987785,"user_tz":-120,"elapsed":505,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"65f05126-f4a7-44fc-9cdd-a135161594a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True title: Kew Gardens 3BR house in cul-de-sac\n","Generated title: 3 Bed House with garden, 20 min from Central London\n","---------------------------------\n","True title: Room with a view  zone 1  Central Bankside\n","Generated title: Separate bedroom with shared bathroom\n","---------------------------------\n","True title: Quiet Comfortable Room in Fulham\n","Generated title: Double Room in a Quiet House by the River\n","---------------------------------\n","True title: Beautiful, Luxurious Art Deco  +private bathroom\n","Generated title: Quiet, comfortable room in Hackney\n","---------------------------------\n","True title: Cosy Double studio in Zone 2 Hammersmith (3)\n","Generated title: Stunning studio flat in Hammersmith\n","---------------------------------\n","True title: ✦Bright Top Floor Sunset View Modern Chelsea apt✦\n","Generated title: Sunset view 1bed in Chelsea\n","---------------------------------\n","True title: West London-W7, Hanwell (Area Ealing) Room\n","Generated title: Double Room in Central West London\n","---------------------------------\n","True title: Double Room 2 Miles from Wimbledon in Morden SM4\n","Generated title: Double Room with en-suite shower room.\n","---------------------------------\n","True title: Luxury Self contained Studio Apt.\n","Generated title: Bushy Park Luxury Studio Apartment\n","---------------------------------\n","True title: Charming Flat in Notting Hill\n","Generated title: Entire, stylish one-bedroom flat\n","---------------------------------\n"]}]},{"cell_type":"code","source":["titles_eval_true = [a[\"output\"] for a in test_set_]"],"metadata":{"id":"W4aXRdwu8kCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9h2158ZxYWlp"},"outputs":[],"source":["processed_preds = postprocess_text(gen_titles_eval)\n","processed_labels = postprocess_text(titles_eval_true)\n"]},{"cell_type":"code","source":["!pip install evaluate\n","!pip install rouge_score"],"metadata":{"id":"AuleVdVKHxmC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686392025875,"user_tz":-120,"elapsed":7063,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"387d70a6-82ed-4291-f554-716e1756bd3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: evaluate\n","Successfully installed evaluate-0.4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.22.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.65.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=04abdd9c0f77f508cf2e6f19d16a4c88d1e60aa36f1f035eaed198708d528555\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}]},{"cell_type":"code","source":["#processed_preds_df = pd.read_csv(\"/content/gdrive/My Drive/Thesis/loss_data/preds_rouge.csv\")\n","#processed_preds = processed_preds_df.preds\n","\n"],"metadata":{"id":"jXNTRSSBBqK6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBzpnVWqZ5kV","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["55a0edaf0f8e4a7c980dcb62c4a4dda0","400cab11aa2a46e3b6459a4bc4add75e","cc2e405b03ec466fb674a648c8dfca7f","70121987f3ca45f59a908f1340b66b09","c0dfc46c79334098ab6c3dfadf067906","aa0e6266bac64762b6caabf977c35330","959ba813a8364be7a2afa07bb0f8b634","a125ec9aa7644db496897d36134ac53e","4a9b7627788747479b4d3cdd2d86aed7","3d6898939b344b4abd78eb3a65ff03ef","6b2df51a588041219a0412446c99e9f4"]},"executionInfo":{"status":"ok","timestamp":1686392032431,"user_tz":-120,"elapsed":3343,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"}},"outputId":"04201c37-f2d6-4a0d-c3ff-83df59495e94"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a0edaf0f8e4a7c980dcb62c4a4dda0"}},"metadata":{}}],"source":["import evaluate\n","rouge_score = evaluate.load(\"rouge\")\n","\n","rouge_score.add_batch(predictions=processed_preds, references=processed_labels)\n","\n","result = rouge_score.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":530,"status":"ok","timestamp":1686392047000,"user":{"displayName":"Nico Schwarzer","userId":"00583317607943344686"},"user_tz":-120},"id":"XewTLqqdJsE_","outputId":"69ab530a-d83d-4de1-87e3-e9d206ec16da"},"outputs":[{"output_type":"stream","name":"stdout","text":["The ROUGE metrics on the evaluation data AFTER LORA-PEFT are:\n","     F1 - ROUGE-1: 0.2524.\n","     F1 - ROUGE-2: 0.0882.\n","     F1 - ROUGE-L: 0.2423.\n"]}],"source":["print(\"The ROUGE metrics on the evaluation data AFTER LORA-PEFT are:\")\n","print(f'     F1 - ROUGE-1: {np.round(result[\"rouge1\"],4)}.')\n","print(f'     F1 - ROUGE-2: {np.round(result[\"rouge2\"],4)}.')\n","print(f'     F1 - ROUGE-L: {np.round(result[\"rougeL\"],4)}.')"]},{"cell_type":"markdown","metadata":{"id":"TTQuJ35-L5M1"},"source":["### Saving generated titles\n","\n","Because this process is extremely slow (even with large A 100 GPU) I shall only generate titles for a subsample!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15nuiM88P4yo"},"outputs":[],"source":["\n","sub_df = airbnb_london_filtered_images.sample(n = 1000)[[\"id\", \"description\", \"name\"]]\n"]},{"cell_type":"code","source":["sub_df[\"gen_title\"] = \"\""],"metadata":{"id":"j8gVDXX7zckD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub_df.index = list(range(sub_df.shape[0]))"],"metadata":{"id":"WGNOdcOpIh4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## setting up new dict\n","\n","data_dict_sub_df = []\n","\n","for i in range(sub_df.shape[0]):\n","\n","  data_dict_sub_df.append({\n","      \"instruction\": \"Summarize the following description into a short, appealing title for an AirBnB listing.\",\n","      \"input\":   sub_df.description[i],\n","      \"output\": sub_df.name[i]\n","  })\n","\n","\n","max_seq_length = 256\n","mask_inputs = True\n","gen_set = [prepare_sample(sample, tokenizer, max_seq_length, mask_inputs) for sample in data_dict_sub_df]\n"],"metadata":{"id":"y2FgkoQ2XsxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_ids_ = list(sub_df.id)"],"metadata":{"id":"TcV7kr3xn7CQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gen_titles_llama\n","\n","\n","gen_titles = []\n","list_ids = []\n","\n","for i in range(len(gen_set)):\n","  sample_ = gen_set[i]\n","  print(i/len(gen_set))\n","  gen_title_new = gen_title(pretrained_path, adapter_path, sample_, 30, 200, 0.2)\n","  gen_titles.append(gen_title_new)\n","  list_ids.append(list_ids_[i])\n","\n","  if i % 100 == 0:\n","    sub_df_lora = pd.DataFrame({\"id\": list_ids, \"gen_titles\":gen_titles})\n","    sub_df_lora.to_csv(\"/content/gdrive/My Drive/Thesis/loss_data/gen_titles_llama.csv\", index = False)\n","\n","sub_df_lora = pd.DataFrame({\"id\": list_ids, \"gen_titles\":gen_titles})\n","sub_df_lora.to_csv(\"/content/gdrive/My Drive/Thesis/loss_data/gen_titles_llama.csv\", index = False)\n"],"metadata":{"id":"ouArq4DzZXF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # since this code in running overnight in Colab and no resources shall be wasted:\n","\n","from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"g_cvfJg5leDw"},"execution_count":null,"outputs":[]}]}